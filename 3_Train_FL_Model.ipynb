{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNIC/Z5Rf0m+LOe2oSvPRNQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# !pip install flwr\n","# !pip install monai\n","# !pip install kornia\n","# !pip install nibabel\n","# !pip install scikit-image"],"metadata":{"id":"dqewC_Ykix6I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##README\n","\n","This notebook is for training FL model with bench and GS transformed data for model performance comparison. We will train and save the models in this notebook to later use for model performance comparison and some of the model stealing attack that are targetting the trained model parameters or gradients.\n","\n","If you don't want to run this, we have provided pre-trained models in the github repository under ```pretrained_models``` folder.\n","\n","If you want to run this notebook please make sure you have done following:\n","1. You have run the ```1_Load_and_preprocess_Data``` and ```2_GS_Transform_data``` notebooks and save the original and transformed data in your repository.\n","2. Run the pip install commands at the above cell.\n","3. Modify the load and save paths according to your folder hierarchy in your workspace."],"metadata":{"id":"qCj35eBzirNr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBuQ0_6-iic3"},"outputs":[],"source":["1from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Imports and Functions"],"metadata":{"id":"Zw2_eHyWirF7"}},{"cell_type":"code","source":["import os\n","import sys\n","import gc\n","import pickle\n","import logging\n","from typing import Tuple, List, Dict, Union, Optional\n","from collections import defaultdict\n","from dataclasses import dataclass\n","import threading\n","import time\n","\n","# Environment configuration for GRPC (used by Flower)\n","os.environ[\"GRPC_MAX_RECEIVE_MESSAGE_LENGTH\"] = \"4000000000\"\n","os.environ[\"GRPC_MAX_SEND_MESSAGE_LENGTH\"] = \"4000000000\"\n","os.environ[\"GRPC_DEFAULT_COMPRESSION_ALGORITHM\"] = \"gzip\"\n","\n","# Configure logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","# ----------------------------\n","# PyTorch and Federated Learning\n","# ----------------------------\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision\n","from torchvision import transforms as T\n","from torch.amp import autocast, GradScaler # For mixed precision training\n","# Flower for federated learning coordination\n","import flwr as fl\n","\n","# ----------------------------\n","# Medical Imaging and Inversion Attack Utilities\n","# ----------------------------\n","# MONAI: for medical image processing and transformations\n","import monai\n","\n","# Kornia: for differentiable computer vision operations (e.g., edge detection)\n","import kornia\n","import kornia.enhance\n","import kornia.losses\n","# nibabel: for reading/writing medical image formats (e.g., NIfTI)\n","import nibabel as nib\n","\n","# ----------------------------\n","# Data Processing, Evaluation, and Visualization\n","# ----------------------------\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_curve, auc\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","# scikit-image: for image quality metrics (e.g., SSIM)\n","from skimage.metrics import structural_similarity as ssim\n"],"metadata":{"id":"gqkGAFxcitMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Medical Classifier\n","class BrainMRIClassifier(nn.Module):\n","    def __init__(self):\n","        super(BrainMRIClassifier, self).__init__()\n","        self.features = nn.Sequential(\n","            # First block\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(32),\n","            # Second block\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(64),\n","            # Third block\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(128),\n","            # Fourth block\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256),\n","            # Fifth block\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256),\n","        )\n","        # With input size 299x299 and 5 pooling layers, spatial dimensions ~9x9\n","        self.flat_features = 256 * 9 * 9\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Dropout(0.5),\n","            nn.Linear(self.flat_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, 4)  # 4 classes\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n"],"metadata":{"id":"DsVgYolEitJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Training Loop Definition\n","\n","def train_classifier_model(model, train_loader, device, epochs=1):\n","    model.train()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","    total_samples = len(train_loader.dataset)\n","\n","    for epoch in range(epochs):\n","        epoch_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_size = images.size(0)\n","            epoch_loss += loss.item() * batch_size\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        avg_loss = epoch_loss / total\n","        accuracy = correct / total\n","        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}\")\n","\n","    return avg_loss, accuracy\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            batch_size = images.size(0)\n","            total_loss += loss.item() * batch_size\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / total\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n"],"metadata":{"id":"lxZkQzTWitGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Federated Setup\n","import flwr as fl\n","\n","class FLClient(fl.client.NumPyClient):\n","    def __init__(self, model, train_loader, test_loader, device, epochs=1):\n","        self.model = model\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","        self.device = device\n","        self.epochs = epochs\n","\n","    # In FLClient class\n","    def get_parameters(self):\n","        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n","\n","    def set_parameters(self, parameters):\n","        # Load parameters into model.\n","        params_dict = dict(zip(self.model.state_dict().keys(), parameters))\n","        state_dict = {k: torch.tensor(v) for k, v in params_dict.items()}\n","        self.model.load_state_dict(state_dict, strict=True)\n","\n","    def fit(self, parameters, config):\n","        self.set_parameters(parameters)\n","        print(\"Starting local training...\")\n","        train_loss, train_acc = train_classifier_model(self.model, self.train_loader, self.device, epochs=self.epochs)\n","        return self.get_parameters(), len(self.train_loader.dataset), {\"loss\": train_loss, \"accuracy\": train_acc}\n","\n","    def evaluate(self, parameters, config):\n","        self.set_parameters(parameters)\n","        eval_loss, eval_acc = evaluate_model(self.model, self.test_loader, self.device)\n","        return float(eval_loss), len(self.test_loader.dataset), {\"accuracy\": eval_acc}\n"],"metadata":{"id":"d-hXLPc_jAcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FederatedLearningSimulator:\n","    def __init__(self, model, client_loaders, test_loader, device, num_rounds=5, epochs_per_round=2):\n","        self.model = model\n","        self.client_loaders = client_loaders\n","        self.test_loader = test_loader\n","        self.device = device\n","        self.num_rounds = num_rounds\n","        self.epochs_per_round = epochs_per_round\n","        self.global_parameters = None\n","        self.client_models = []  # Store client models for inversion attacks\n","\n","    def initialize_global_model(self):\n","        global_model = self.model().to(self.device)\n","        self.global_parameters = [val.cpu().numpy() for _, val in global_model.state_dict().items()]\n","\n","    def run_federated_learning(self):\n","        self.initialize_global_model()\n","        self.client_models = []  # Reset client models\n","\n","        for round_num in range(self.num_rounds):\n","            print(f\"\\n===== Round {round_num+1}/{self.num_rounds} =====\")\n","            client_parameters = []\n","            client_weights = []\n","            round_client_models = []  # Store client models for this round\n","\n","            # Train on each client\n","            for client_idx, loader in enumerate(self.client_loaders):\n","                print(f\"\\nTraining client {client_idx+1}:\")\n","\n","                # Create new model instance for this client\n","                client_model = self.model().to(self.device)\n","\n","                # Load global parameters\n","                params_dict = dict(zip(client_model.state_dict().keys(), self.global_parameters))\n","                state_dict = {k: torch.tensor(v) for k, v in params_dict.items()}\n","                client_model.load_state_dict(state_dict, strict=True)\n","\n","                # Train the client model with per-epoch validation\n","                for epoch in range(self.epochs_per_round):\n","                    train_loss, train_acc = train_classifier_model(client_model, loader, self.device, epochs=1)\n","                    val_loss, val_acc = evaluate_model(client_model, self.test_loader, self.device)\n","                    print(f\"  Epoch {epoch+1}/{self.epochs_per_round}: Train acc={train_acc:.4f}, Val acc={val_acc:.4f}\")\n","\n","                # Save client model (only from final round)\n","                if round_num == self.num_rounds - 1:\n","                    # Create a copy to avoid reference issues\n","                    client_copy = self.model().to(self.device)\n","                    client_copy.load_state_dict(client_model.state_dict())\n","                    round_client_models.append((client_idx, client_copy))\n","\n","                # Collect client's updated parameters\n","                params = [val.cpu().numpy() for _, val in client_model.state_dict().items()]\n","                client_parameters.append(params)\n","                client_weights.append(len(loader.dataset))\n","\n","            # Save client models from final round\n","            if round_num == self.num_rounds - 1:\n","                self.client_models = round_client_models\n","\n","            # Perform weighted averaging to update global parameters\n","            self.global_parameters = self.federated_averaging(client_parameters, client_weights)\n","\n","            # Evaluate global model after aggregation\n","            global_model = self.model().to(self.device)\n","            params_dict = dict(zip(global_model.state_dict().keys(), self.global_parameters))\n","            state_dict = {k: torch.tensor(v) for k, v in params_dict.items()}\n","            global_model.load_state_dict(state_dict, strict=True)\n","\n","            eval_loss, eval_acc = evaluate_model(global_model, self.test_loader, self.device)\n","            print(f\"\\n>> Round {round_num+1} complete: Global model accuracy = {eval_acc:.4f}\")\n","\n","        return self.global_parameters\n","\n","    def federated_averaging(self, client_parameters, client_weights):\n","        total_weight = sum(client_weights)\n","        weighted_params = []\n","\n","        for i in range(len(client_parameters[0])):\n","            weighted_sum = sum(client_params[i] * weight for client_params, weight\n","                              in zip(client_parameters, client_weights))\n","            weighted_params.append(weighted_sum / total_weight)\n","\n","        return weighted_params\n",""],"metadata":{"id":"IDfC19V6jAZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def client_fn(cid: str) -> fl.client.NumPyClient:\n","    # Convert client id (string) to integer index\n","    index = int(cid)\n","    # Create a new instance of the classifier for this client\n","    model = BrainMRIClassifier().to(device)\n","    # Use the corresponding client DataLoader and common test_loader as validation\n","    return FLClient(model, client_loaders[index], test_loader, device, epochs=2)\n","\n","## Data Loading Functions\n","def load_data(images_path, labels_path):\n","    with open(images_path, 'rb') as f:\n","        images = pickle.load(f)\n","    with open(labels_path, 'rb') as f:\n","        labels = pickle.load(f)\n","    return images, labels\n","\n","def prepare_dataset(images, labels):\n","    # Convert images to NumPy array and then to a float tensor.\n","    images = torch.tensor(np.array(images)).float()\n","    # If images are grayscale and shaped (N, H, W), add channel dimension to get (N, 1, H, W)\n","    if images.ndim == 3:\n","        images = images.unsqueeze(1)\n","    # Convert labels to tensor (assuming integer encoding for CrossEntropyLoss)\n","    labels = torch.tensor(np.array(labels)).long()\n","    return torch.utils.data.TensorDataset(images, labels)\n","\n","def save_model_states(global_model, client_models, key, save_dir=\"saved_models\"):\n","    \"\"\"\n","    Saves the global model state and each client model state to disk.\n","\n","    Parameters:\n","      - global_model: the global model instance.\n","      - client_models: a list of tuples (client_idx, client_model) from fl_simulator.client_models.\n","      - key: the current root key (e.g., 'bench', 'mask_0', etc.).\n","      - save_dir: directory where models will be saved.\n","\n","    Returns:\n","      A dictionary with paths to the saved global model and client models.\n","    \"\"\"\n","    # Ensure the save directory exists\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Save global model state\n","    global_model_state = global_model.state_dict()\n","    global_model_path = os.path.join(save_dir, f\"{key}_global_model.pt\")\n","    torch.save(global_model_state, global_model_path)\n","\n","    # Save each client model state and record paths in a dictionary\n","    client_paths = {}\n","    for client_idx, client_model in client_models:\n","        client_model_path = os.path.join(save_dir, f\"{key}_client_{client_idx+1}_model.pt\")\n","        torch.save(client_model.state_dict(), client_model_path)\n","        client_paths[f\"client_{client_idx+1}\"] = client_model_path\n","\n","    return {\n","        \"global\": global_model_path,\n","        \"clients\": client_paths\n","    }"],"metadata":{"id":"pZ6ieHFbjAW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#####################################################\n","# Helper Function to Evaluate a Given Model on Test\n","#####################################################\n","def compute_metrics(model, loader, device):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","    cm = confusion_matrix(all_labels, all_preds)\n","    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n","    report = classification_report(all_labels, all_preds, output_dict=True)\n","    return {\n","        \"accuracy\": acc,\n","        \"f1_score\": f1,\n","        \"per_class_accuracy\": per_class_acc,\n","        \"classification_report\": report,\n","        \"confusion_matrix\": cm\n","    }\n","\n","# Annotate bars with values\n","def autolabel(rects):\n","    for rect in rects:\n","        height = rect.get_height()\n","        ax.annotate(f'{height:.2f}',\n","                    xy=(rect.get_x() + rect.get_width() / 2, height),\n","                    xytext=(0, 3),  # offset text by 3 points vertically\n","                    textcoords=\"offset points\",\n","                    ha='center', va='bottom')\n","\n","def compute_confidence_metrics(model, loader, device):\n","    \"\"\"\n","    Computes the mean and standard deviation of the model's confidence (i.e. max softmax probability)\n","    per true class over the test set.\n","\n","    Returns:\n","      mean_conf: dict mapping class label (int) to mean confidence.\n","      std_conf: dict mapping class label (int) to standard deviation.\n","    \"\"\"\n","    model.eval()\n","    confidences = {}\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            probs = torch.softmax(outputs, dim=1)\n","            max_probs, preds = torch.max(probs, dim=1)\n","            for i, label in enumerate(labels):\n","                lbl = int(label)\n","                if lbl not in confidences:\n","                    confidences[lbl] = []\n","                confidences[lbl].append(max_probs[i].item())\n","    mean_conf = {lbl: np.mean(vals) for lbl, vals in confidences.items()}\n","    std_conf = {lbl: np.std(vals) for lbl, vals in confidences.items()}\n","    return mean_conf, std_conf\n"],"metadata":{"id":"i87-DfBjjAUe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main Execution Block"],"metadata":{"id":"FbS9-LiVk-n-"}},{"cell_type":"code","source":["#@title Modify DATA_PATHS here!!\n","MASKED_DATA_PATHS = {\n","    \"bench\": {\n","        \"clients\": [\n","            {\n","                \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client1_images.pickle\",  ## <-Replace\n","                \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client1_labels.pickle\",  ## <-Replace\n","            },\n","            {\n","                \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client2_images.pickle\",  ## <-Replace\n","                \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client2_labels.pickle\",  ## <-Replace\n","            },\n","            {\n","                \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client3_images.pickle\",  ## <-Replace\n","                \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client3_labels.pickle\",  ## <-Replace\n","            }\n","        ],\n","        \"test\": {\n","            \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client_test_images.pickle\",  ## <-Replace\n","            \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client_test_labels.pickle\"  ## <-Replace\n","        }\n","    },\n","    \"mask_20\": {\n","        \"clients\": [\n","            {\n","                \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client1_images_gs20p.pickle\",  ## <-Replace\n","                \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client1_labels.pickle\",  ## <-Replace\n","            },\n","            {\n","                \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client2_images_gs20p.pickle\",  ## <-Replace\n","                \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client2_labels.pickle\",  ## <-Replace\n","            },\n","            {\n","                \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client3_images_gs20p.pickle\",  ## <-Replace\n","                \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client3_labels.pickle\",  ## <-Replace\n","            }\n","        ],\n","        \"test\": {\n","            \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client_test_images_gs20p.pickle\",  ## <-Replace\n","            \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client_test_labels.pickle\"  ## <-Replace\n","        }\n","    }#,\n","    ## You can add more data variant here if you want to test different masking settings with GS\n","    ## i.e.:\n","    # \"mask_50\": {\n","    #     \"clients\": [\n","    #         {\n","    #             \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client1_images_gs50p.pickle\",  ## <-Replace\n","    #             \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client1_labels.pickle\",  ## <-Replace\n","    #         },\n","    #         {\n","    #             \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client2_images_gs50p.pickle\",  ## <-Replace\n","    #             \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client2_labels.pickle\",  ## <-Replace\n","    #         },\n","    #         {\n","    #             \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client3_images_gs50p.pickle\",  ## <-Replace\n","    #             \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client3_labels.pickle\",  ## <-Replace\n","    #         }\n","    #     ],\n","    #     \"test\": {\n","    #         \"images\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/gs/client_test_images_gs50p.pickle\",  ## <-Replace\n","    #         \"labels\": \"/content/drive/MyDrive/Spring 25/github_brainfl/data/bench/client_test_labels.pickle\"  ## <-Replace\n","    #     }\n","    # }\n","}\n","\n"],"metadata":{"cellView":"form","id":"h3stSIb_jASR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#####################################################\n","# Loop Over Masking Settings and Run Simulation\n","#####################################################\n","# Dictionary to store evaluation metrics for each setting\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","results = {}\n","\n","for mask_name, paths in MASKED_DATA_PATHS.items():\n","    print(f\"\\n=== Evaluating for Masking Setting: {mask_name} ===\\n\")\n","\n","    # 1. Load Test Data\n","    test_images, test_labels = load_data(paths[\"test\"][\"images\"], paths[\"test\"][\"labels\"])\n","    test_dataset = prepare_dataset(test_images, test_labels)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    # 1. Load Client Data for 3 Clients\n","    client_datasets = []\n","    for client_info in paths[\"clients\"]:\n","        imgs, labs = load_data(client_info[\"images\"], client_info[\"labels\"])\n","        ds = prepare_dataset(imgs, labs)\n","        client_datasets.append(ds)\n","    client_loaders = [DataLoader(ds, batch_size=32, shuffle=True) for ds in client_datasets]\n","\n","    # 2. Create and run the federated learning simulation\n","    fl_simulator = FederatedLearningSimulator(\n","        model=BrainMRIClassifier,\n","        client_loaders=client_loaders,\n","        test_loader=test_loader,\n","        device=device,\n","        num_rounds=10,       # Adjustable number of rounds\n","        epochs_per_round=5   # Adjustable epochs per round\n","    )\n","\n","    print(\"Starting federated learning simulation...\")\n","    global_parameters = fl_simulator.run_federated_learning()\n","    print(\"Federated training simulation completed.\")\n","\n","    # Evaluate the final global model\n","    global_model = BrainMRIClassifier().to(device)\n","    params_dict = dict(zip(global_model.state_dict().keys(), global_parameters))\n","    state_dict = {k: torch.tensor(v) for k, v in params_dict.items()}\n","    global_model.load_state_dict(state_dict, strict=True)\n","    global_metrics = compute_metrics(global_model, test_loader, device)\n","\n","    # Compute model confidence metrics (mean and std per class)\n","    global_mean_conf, global_std_conf = compute_confidence_metrics(global_model, test_loader, device)\n","\n","    # Evaluate each client model saved from the final round\n","    client_metrics = {}\n","    for client_idx, client_model in fl_simulator.client_models:\n","        metrics = compute_metrics(client_model, test_loader, device)\n","        client_metrics[f\"client_{client_idx+1}\"] = metrics\n","\n","    # Save metrics for this masking setting\n","    results[mask_name] = {\n","        \"global\": global_metrics,\n","        \"clients\": client_metrics,\n","        \"confidence\": {\"mean\": global_mean_conf, \"std\": global_std_conf}\n","    }\n","\n","    # Save the models for this masking setting and record the file paths in the results\n","    model_save_info = save_model_states(global_model, fl_simulator.client_models, mask_name)\n","    results[mask_name][\"model_paths\"] = model_save_info\n","\n","    # 4. Clear memory: delete models, loaders, and force garbage collection\n","    del global_model, fl_simulator, client_datasets, client_loaders, test_dataset, test_loader\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    time.sleep(1)  # optional pause between iterations\n","\n","# Optionally, save the results dictionary (with evaluation metrics and model paths) to disk\n","with open(\"federated_results.pkl\", \"wb\") as f:\n","    pickle.dump(results, f)\n"],"metadata":{"id":"zmVwNvV2jAPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualize Model Performance w/ Heatmaps\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","# Figure 1: F1 Score Heatmap\n","# ---------------------------------\n","# Get dataset variants and class labels (assumed to be numeric strings)\n","dataset_variants = list(results.keys())\n","example_report = results[dataset_variants[0]][\"global\"][\"classification_report\"]\n","# Filter keys that are digits (i.e. class labels) and sort them\n","class_labels = sorted([k for k in example_report.keys() if k.isdigit()], key=lambda x: int(x))\n","class_labels_int = [int(x) for x in class_labels]\n","\n","# Build a 2D array: rows: dataset variant, columns: per-class F1 scores\n","f1_matrix = np.zeros((len(dataset_variants), len(class_labels_int)))\n","for i, variant in enumerate(dataset_variants):\n","    report = results[variant][\"global\"][\"classification_report\"]\n","    for j, cls in enumerate(class_labels):\n","        f1_matrix[i, j] = report[cls][\"f1-score\"]\n","\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(f1_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n","            xticklabels=class_labels_int, yticklabels=dataset_variants)\n","plt.xlabel(\"Class Label\")\n","plt.ylabel(\"Dataset Variant\")\n","plt.title(\"Global Model Per-Class F1 Scores\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Figure 2: Model Confidence per Class (Facet Grid)\n","# ---------------------------------\n","# Prepare data: For each dataset variant, we already have the confidence metrics in results.\n","# For each class, we want to collect the mean and std for each variant.\n","n_classes = len(class_labels_int)\n","# For each class, create lists for means and stds across dataset variants.\n","confidence_data = {cls: {\"mean\": [], \"std\": []} for cls in class_labels_int}\n","for variant in dataset_variants:\n","    conf = results[variant][\"confidence\"][\"mean\"]\n","    conf_std = results[variant][\"confidence\"][\"std\"]\n","    for cls in class_labels_int:\n","        # Append mean confidence and std (use np.nan if not present)\n","        confidence_data[cls][\"mean\"].append(conf.get(cls, np.nan))\n","        confidence_data[cls][\"std\"].append(conf_std.get(cls, np.nan))\n","\n","# Create a subplot for each class\n","fig, axs = plt.subplots(1, n_classes, figsize=(5 * n_classes, 5), sharey=True)\n","if n_classes == 1:\n","    axs = [axs]  # Ensure axs is iterable when only one subplot\n","\n","x = np.arange(len(dataset_variants))\n","for i, cls in enumerate(class_labels_int):\n","    ax = axs[i]\n","    means = np.array(confidence_data[cls][\"mean\"])\n","    stds = np.array(confidence_data[cls][\"std\"])\n","    ax.errorbar(x, means, yerr=stds, fmt='o-', capsize=5)\n","    ax.set_xticks(x)\n","    ax.set_xticklabels(dataset_variants, rotation=45, ha=\"right\")\n","    ax.set_title(f\"Class {cls}\")\n","    ax.set_xlabel(\"Dataset Variant\")\n","    if i == 0:\n","        ax.set_ylabel(\"Mean Confidence (Softmax Probability)\")\n","    ax.grid(True, linestyle='--', alpha=0.5)\n","\n","fig.suptitle(\"Global Model Confidence per Class Across Dataset Variants\", fontsize=16)\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()\n"],"metadata":{"id":"KgrORsDejAM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Se3fVVtWitEc"},"execution_count":null,"outputs":[]}]}