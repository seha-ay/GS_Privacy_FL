{"cells":[{"cell_type":"markdown","source":["##README\n","\n","## Caution!\n","This notebook will take significantly long time to complete and generate inverted images **(20-30 hours with GPU)**!\n","\n","You are welcome to run this if you are okay for the execution times. We have used following system for this notebook:\n","\n","- Google Console Platform - Vertex AI Jupyter Notebook\n","  - Machine type: n1-standard-16 (16 vCPUs, 60 GB RAM)\n","  - GPU: NVIDIA Tesla P100 x 1\n","  - Environment: TensorFlow Enterprise 2.16 (IntelÂ® MKL-DNN/MKL)\n","  - CUDA Version 12\n","\n","\n","## Alternative\n","We have already run and obtain inverted image samples. This can be found at the ```inverted_images``` folder under ```/main/model_inversion/blakcbox/inverted_images``` path in the github page."],"metadata":{"id":"v3fvQ8hjx6gz"},"id":"v3fvQ8hjx6gz"},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"pAzWhrtAx7Zo"},"id":"pAzWhrtAx7Zo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports and Functions"],"metadata":{"id":"7K0SASfKzBbF"},"id":"7K0SASfKzBbF"},{"cell_type":"code","execution_count":null,"id":"7cf26470-91bb-44a2-8e8e-c87048e1cfe4","metadata":{"tags":[],"id":"7cf26470-91bb-44a2-8e8e-c87048e1cfe4"},"outputs":[],"source":["import os\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.utils as vutils\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n","import torch.nn.functional as F\n","from functools import partial\n","import traceback\n","\n","class BrainMRIClassifier(nn.Module):\n","    \"\"\"CNN classifier for brain MRI images with 4 output classes\"\"\"\n","    def __init__(self):\n","        super(BrainMRIClassifier, self).__init__()\n","        self.features = nn.Sequential(\n","            # First block\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(32),\n","            # Second block\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(64),\n","            # Third block\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(128),\n","            # Fourth block\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256),\n","            # Fifth block\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256),\n","        )\n","        # With input size 299x299 and 5 pooling layers, spatial dimensions ~9x9\n","        self.flat_features = 256 * 9 * 9\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Dropout(0.5),\n","            nn.Linear(self.flat_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, 4)  # 4 classes\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","    def get_feature_maps(self, x):\n","        # Helper method to extract intermediate feature maps for inversion guidance\n","        feature_maps = []\n","\n","        # Extract feature maps from each block\n","        x1 = self.features[0:4](x)  # First block\n","        feature_maps.append(x1)\n","\n","        x2 = self.features[4:8](x1)  # Second block\n","        feature_maps.append(x2)\n","\n","        x3 = self.features[8:12](x2)  # Third block\n","        feature_maps.append(x3)\n","\n","        x4 = self.features[12:16](x3)  # Fourth block\n","        feature_maps.append(x4)\n","\n","        x5 = self.features[16:20](x4)  # Fifth block\n","        feature_maps.append(x5)\n","\n","        return feature_maps\n","\n","\n","\n","\n","# -------------------------------------------------------------\n","# 1. Model Loading and Feature Extraction\n","# -------------------------------------------------------------\n","def load_model(model_path, model_class=None):\n","    \"\"\"\n","    Loads a pre-trained model from the given file path.\n","    If the loaded object is a state dictionary, a model_class must be provided\n","    to instantiate the model and load the state.\n","    \"\"\"\n","    checkpoint = torch.load(model_path)\n","    # Check if checkpoint is a state dict (OrderedDict) and not a full model\n","    if isinstance(checkpoint, dict) and not hasattr(checkpoint, 'eval'):\n","        if model_class is None:\n","            raise ValueError(\"The checkpoint is a state_dict but no model_class was provided. \"\n","                             \"Please supply the model_class argument.\")\n","        model = model_class()  # Instantiate your model architecture\n","        model.load_state_dict(checkpoint)\n","    else:\n","        model = checkpoint\n","    model.eval()  # set to evaluation mode\n","    return model\n","\n","\n","def extract_model_layers(model):\n","    \"\"\"\n","    Analyzes a model and extracts information about its layers\n","    to help determine optimal feature extraction points.\n","    Enhanced to handle EnsembleModel.\n","    \"\"\"\n","    layers_info = []\n","\n","    # Special handling for EnsembleModel\n","    if isinstance(model, EnsembleModel):\n","        # Try to extract layers from the first wrapped model\n","        if model.models and len(model.models) > 0:\n","            return extract_model_layers(model.models[0])\n","        return []  # Empty list if no models\n","\n","    # Standard layer extraction\n","    for name, module in model.named_modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            layers_info.append({\n","                'name': name,\n","                'type': type(module).__name__,\n","                'params': sum(p.numel() for p in module.parameters())\n","            })\n","    return layers_info\n","\n","def get_optimal_hook_layers(model):\n","    \"\"\"\n","    Automatically determines optimal layers for feature extraction\n","    based on model architecture. Fixed to handle custom models like EnsembleModel.\n","    \"\"\"\n","    layers_info = extract_model_layers(model)\n","\n","    # Check if we found any layers\n","    if not layers_info:\n","        # For custom models like EnsembleModel, return a default layer name\n","        # that will be handled specially in the main function\n","        return [\"default_hook\"]\n","\n","    if len(layers_info) <= 2:\n","        return [layers_info[-1]['name']]\n","\n","    # For ResNet-like models, target specific blocks\n","    all_names = [layer['name'] for layer in layers_info]\n","    candidates = []\n","\n","    # Look for common layer naming patterns\n","    for name in all_names:\n","        if any(pattern in name for pattern in ['layer2', 'layer3', 'layer4', 'block', 'features']):\n","            if 'conv' in name.lower() and 'weight' not in name:\n","                candidates.append(name)\n","\n","    # Select a diverse set of layers (early, middle, late)\n","    if len(candidates) >= 3:\n","        idx1 = len(candidates) // 4\n","        idx2 = len(candidates) // 2\n","        idx3 = (3 * len(candidates)) // 4\n","        return [candidates[idx1], candidates[idx2], candidates[idx3]]\n","    elif len(candidates) > 0:\n","        return candidates\n","    else:\n","        # Fallback to selecting first, middle and last conv layers\n","        conv_layers = [l['name'] for l in layers_info if 'Conv' in l['type']]\n","        if len(conv_layers) >= 3:\n","            return [conv_layers[0], conv_layers[len(conv_layers)//2], conv_layers[-1]]\n","        elif len(conv_layers) > 0:\n","            return [conv_layers[-1]]\n","        else:\n","            # Final fallback - use the last layer of any type\n","            return [layers_info[-1]['name']] if layers_info else [\"default_hook\"]\n","\n","\n","# -------------------------------------------------------------\n","# 2. Enhanced Regularization Functions with Self-tuning\n","# -------------------------------------------------------------\n","def total_variation_loss(img, tv_weight):\n","    \"\"\"\n","    Enhanced total variation loss with improved normalization.\n","    \"\"\"\n","    batch_size = img.size()[0]\n","    h_x = img.size()[2]\n","    w_x = img.size()[3]\n","    count_h = (h_x - 1) * w_x\n","    count_w = h_x * (w_x - 1)\n","\n","    # Use higher-order differences for better smoothness\n","    h_tv = torch.pow(img[:, :, 1:, :] - img[:, :, :h_x-1, :], 2).sum()\n","    w_tv = torch.pow(img[:, :, :, 1:] - img[:, :, :, :w_x-1], 2).sum()\n","\n","    # Add second-order differences for capturing textures better\n","    if h_x > 2:\n","        h_tv2 = torch.pow(img[:, :, 2:, :] - 2*img[:, :, 1:-1, :] + img[:, :, :-2, :], 2).sum()\n","        h_tv = h_tv + 0.5 * h_tv2\n","\n","    if w_x > 2:\n","        w_tv2 = torch.pow(img[:, :, :, 2:] - 2*img[:, :, :, 1:-1] + img[:, :, :, :-2], 2).sum()\n","        w_tv = w_tv + 0.5 * w_tv2\n","\n","    return tv_weight * (h_tv / count_h + w_tv / count_w) / batch_size\n","\n","def color_distribution_loss(img, color_weight):\n","    \"\"\"\n","    Enhanced color loss that encourages natural color distributions.\n","    Penalizes both gray images and unnatural color distributions.\n","    For grayscale images, returns zero loss.\n","    \"\"\"\n","    # If the image has less than 3 channels, bypass color loss.\n","    if img.shape[1] < 3:\n","        return torch.tensor(0.0, device=img.device)\n","\n","    # Calculate mean and std across spatial dimensions for each channel\n","    mean_rgb = torch.mean(img, dim=[2, 3])\n","    std_rgb = torch.std(img, dim=[2, 3])\n","\n","    # Split channels\n","    mr, mg, mb = torch.split(mean_rgb, 1, dim=1)\n","    sr, sg, sb = torch.split(std_rgb, 1, dim=1)\n","\n","    # 1. Channel diversity loss - penalize when channels are too similar (gray image)\n","    diversity_loss = -torch.mean(torch.abs(mr - mg) + torch.abs(mr - mb) + torch.abs(mg - mb))\n","\n","    # 2. Natural distribution loss - RGB channels typically have correlations\n","    # Encourage typical RGB relationships: G typically higher than R and B\n","    natural_mean_loss = torch.mean(torch.relu(mr - mg)) + torch.mean(torch.relu(mb - mg))\n","\n","    # 3. Natural variance loss - encourage reasonable variance in each channel\n","    target_std = torch.tensor([0.2, 0.2, 0.2], device=img.device).view(1, 3)\n","    variance_loss = F.mse_loss(std_rgb, target_std)\n","\n","    return color_weight * (diversity_loss + 0.5 * natural_mean_loss + variance_loss)\n","\n","\n","def perceptual_smoothness_loss(img, smooth_weight):\n","    \"\"\"\n","    Multi-scale perceptual smoothness that better preserves edges.\n","    Fixed to handle padding correctly.\n","    \"\"\"\n","    loss = 0.0\n","\n","    # Multiple kernel sizes capture different levels of detail\n","    for kernel_size in [3, 5, 7]:\n","        # Create Gaussian-like kernel (approximation)\n","        sigma = kernel_size / 3\n","        grid_x = torch.arange(kernel_size, device=img.device) - (kernel_size - 1) / 2\n","        grid_y = grid_x.view(-1, 1)\n","        kernel_2d = torch.exp(-(grid_x.pow(2) + grid_y.pow(2)) / (2 * sigma**2))\n","        kernel_2d = kernel_2d / kernel_2d.sum()\n","\n","        # Expand to 4D kernel\n","        channels = img.shape[1]\n","        kernel = kernel_2d.expand(channels, 1, kernel_size, kernel_size)\n","\n","        # Apply smoothing\n","        padding = (kernel_size - 1) // 2\n","        smoothed = F.conv2d(img, kernel, padding=padding, groups=channels)\n","\n","        # Compute image gradients using finite differences\n","        grad_x = torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1])\n","        grad_y = torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :])\n","\n","        # Manually handle padding instead of using F.pad with 'replicate'\n","        # For grad_x: pad the last column by repeating the last valid column\n","        last_col = grad_x[:, :, :, -1:]\n","        grad_x = torch.cat([grad_x, last_col], dim=3)\n","\n","        # For grad_y: pad the last row by repeating the last valid row\n","        last_row = grad_y[:, :, -1:, :]\n","        grad_y = torch.cat([grad_y, last_row], dim=2)\n","\n","        # Compute edge-preserving weights: lower weight near edges, higher in smooth regions\n","        edge_weights = torch.exp(-50 * (grad_x.pow(2) + grad_y.pow(2)))\n","\n","        # Calculate weighted difference\n","        weighted_diff = edge_weights * (img - smoothed).pow(2)\n","        loss += weighted_diff.mean()\n","\n","    return smooth_weight * (loss / 3)  # Average across scales\n","\n","def naturalness_prior_loss(img, natural_weight):\n","    \"\"\"\n","    Naturalness prior encouraging realistic image statistics.\n","    Based on natural image priors in the gradient domain.\n","    \"\"\"\n","    # Gradient in x and y directions\n","    grad_x = img[:, :, :, 1:] - img[:, :, :, :-1]\n","    grad_y = img[:, :, 1:, :] - img[:, :, :-1, :]\n","\n","    # Natural images follow a heavy-tailed distribution in gradient domain\n","    # We can approximate this with a combination of L1 and log penalties\n","    l1_grad = torch.mean(torch.abs(grad_x)) + torch.mean(torch.abs(grad_y))\n","\n","    # Log penalty encourages sparse but strong gradients (edges)\n","    eps = 1e-5\n","    log_grad = torch.mean(torch.log(torch.abs(grad_x) + eps)) + torch.mean(torch.log(torch.abs(grad_y) + eps))\n","\n","    return natural_weight * (l1_grad - 0.1 * log_grad)\n","\n","def compute_fft_loss(img, fft_weight):\n","    \"\"\"\n","    Spectral loss operating in the frequency domain to encourage\n","    natural frequency distributions found in real images.\n","    Fixed to handle cuFFT size requirements.\n","    \"\"\"\n","    # Convert to grayscale for frequency analysis\n","    if img.shape[1] >= 3:\n","        # RGB to grayscale\n","        gray = 0.299 * img[:, 0:1] + 0.587 * img[:, 1:2] + 0.114 * img[:, 2:3]\n","    else:\n","        # Already grayscale\n","        gray = img\n","\n","    # Ensure dimensions are compatible with cuFFT (powers of 2, 3, 5, 7)\n","    # A simple approach is to pad to the next power of 2\n","    h, w = gray.shape[2], gray.shape[3]\n","    padded_h = 2**int(np.ceil(np.log2(h)))\n","    padded_w = 2**int(np.ceil(np.log2(w)))\n","\n","    if h != padded_h or w != padded_w:\n","        # Pad to power of 2 dimensions\n","        padding_h = padded_h - h\n","        padding_w = padded_w - w\n","        pad_h1, pad_h2 = padding_h // 2, padding_h - (padding_h // 2)\n","        pad_w1, pad_w2 = padding_w // 2, padding_w - (padding_w // 2)\n","\n","        # Use zero padding\n","        gray = F.pad(gray, (pad_w1, pad_w2, pad_h1, pad_h2), mode='constant', value=0)\n","\n","    try:\n","        # Compute 2D FFT\n","        fft = torch.fft.fft2(gray)\n","        fft_mag = torch.abs(fft)\n","\n","        # Shift to center low frequencies\n","        fft_mag = torch.fft.fftshift(fft_mag)\n","\n","        # Create a reference power spectrum that follows 1/f distribution\n","        h, w = fft_mag.shape[-2:]\n","        cy, cx = h // 2, w // 2\n","        y_grid, x_grid = torch.meshgrid(torch.arange(h, device=img.device),\n","                                        torch.arange(w, device=img.device),\n","                                        indexing='ij')\n","        y_grid = y_grid - cy\n","        x_grid = x_grid - cx\n","        dist = torch.sqrt(x_grid.pow(2) + y_grid.pow(2)) + 1e-5\n","        target_spectrum = 1 / dist\n","\n","        # Normalize target and actual spectrum\n","        target_spectrum = target_spectrum / target_spectrum.sum()\n","        actual_spectrum = fft_mag / (fft_mag.sum() + 1e-8)\n","\n","        # Compute KL divergence as a measure of distribution difference\n","        eps = 1e-8\n","        kl_div = target_spectrum * torch.log((target_spectrum + eps) / (actual_spectrum + eps))\n","\n","        return fft_weight * kl_div.sum()\n","\n","    except RuntimeError:\n","        # Fallback if FFT still fails: return a small constant loss\n","        print(\"Warning: FFT computation failed, using fallback loss\")\n","        return fft_weight * torch.tensor(0.1, device=img.device)\n","\n","# -------------------------------------------------------------\n","# 3. Advanced Initialization Strategies\n","# -------------------------------------------------------------\n","def get_initial_image(strategy='mixed', size=(1, 3, 224, 224), device='cpu', target_class=None, channels=None):\n","    \"\"\"\n","    Enhanced initialization strategies for faster convergence.\n","    Accepts a 'channels' parameter. If not provided, it defaults to size[1].\n","    \"\"\"\n","    if channels is None:\n","        channels = size[1]\n","\n","    if strategy == 'mean':\n","        if channels == 1:\n","            mean = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","        else:\n","            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n","        img = mean.repeat(size[0], 1, size[2], size[3])\n","    elif strategy == 'gaussian':\n","        if channels == 1:\n","            mean = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","            std = torch.tensor([0.25]).view(1, 1, 1, 1).to(device)\n","        else:\n","            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n","            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n","        img = torch.randn(size, device=device) * std + mean\n","    elif strategy == 'pca':\n","        img = torch.zeros(size, device=device)\n","        for c in range(channels):\n","            freq_representation = torch.zeros((size[2], size[3]), dtype=torch.complex64, device=device)\n","            r_max = min(size[2], size[3]) // 8\n","            cy, cx = size[2] // 2, size[3] // 2\n","            for ky in range(size[2]):\n","                for kx in range(size[3]):\n","                    y_rel = (ky - cy) / r_max\n","                    x_rel = (kx - cx) / r_max\n","                    dist = torch.sqrt(y_rel**2 + x_rel**2)\n","                    if dist < 1.0:\n","                        phase = torch.rand(1, device=device) * 2 * np.pi\n","                        amplitude = torch.exp(-3.0 * dist)\n","                        freq_representation[ky, kx] = amplitude * torch.exp(1j * phase)\n","            channel_data = torch.fft.ifft2(freq_representation).real\n","            channel_data = (channel_data - channel_data.min()) / (channel_data.max() - channel_data.min() + 1e-8)\n","            img[0, c] = channel_data\n","    elif strategy == 'mixed':\n","        if channels == 1:\n","            mean = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","        else:\n","            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n","        base = mean.repeat(size[0], 1, size[2], size[3])\n","        freq_img = torch.zeros(size, device=device)\n","        for octave in range(3):\n","            scale_factor = 2 ** octave\n","            noise_size = (size[2] // scale_factor, size[3] // scale_factor)\n","            noise = torch.randn((1, channels, *noise_size), device=device)\n","            noise = F.interpolate(noise, size=(size[2], size[3]), mode='bilinear', align_corners=False)\n","            freq_img += noise * (0.5 ** octave)\n","        freq_img = (freq_img - freq_img.min()) / (freq_img.max() - freq_img.min() + 1e-8) * 0.2\n","        img = base + freq_img\n","    elif strategy == 'class_prior' and target_class is not None:\n","        hue_shift = (target_class % 10) / 10.0\n","        if channels == 1:\n","            base_color = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","        else:\n","            r = 0.5 + 0.4 * torch.cos(2 * np.pi * hue_shift)\n","            g = 0.5 + 0.4 * torch.cos(2 * np.pi * (hue_shift + 1/3))\n","            b = 0.5 + 0.4 * torch.cos(2 * np.pi * (hue_shift + 2/3))\n","            base_color = torch.tensor([r, g, b]).view(1, 3, 1, 1).to(device)\n","        img = base_color.repeat(size[0], 1, size[2], size[3])\n","        noise = torch.randn((1, channels, size[2]//4, size[3]//4), device=device)\n","        noise = F.interpolate(noise, size=(size[2], size[3]), mode='bilinear', align_corners=False)\n","        noise = (noise - noise.min()) / (noise.max() - noise.min() + 1e-8) * 0.15\n","        img = img + noise\n","    else:\n","        img = torch.randn(size, device=device)\n","\n","    img = torch.clamp(img, 0, 1)\n","    img.requires_grad = True\n","    return img\n","\n","# -------------------------------------------------------------\n","# 4. Improved Feature Matching with Adaptive Statistics\n","# -------------------------------------------------------------\n","class FeatureHook:\n","    \"\"\"\n","    Advanced feature hook with statistics tracking capabilities.\n","    \"\"\"\n","    def __init__(self, layer_name, adaptive_stats=True):\n","        self.layer_name = layer_name\n","        self.features = None\n","        self.adaptive_stats = adaptive_stats\n","        self.running_mean = None\n","        self.running_std = None\n","        self.momentum = 0.9\n","\n","    def hook_fn(self, module, input, output):\n","        self.features = output\n","\n","        # Update running statistics for adaptive matching\n","        if self.adaptive_stats:\n","            current_mean = output.mean(dim=[0, 2, 3]).detach()\n","            current_std = output.std(dim=[0, 2, 3]).detach()\n","\n","            if self.running_mean is None:\n","                self.running_mean = current_mean\n","                self.running_std = current_std\n","            else:\n","                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * current_mean\n","                self.running_std = self.momentum * self.running_std + (1 - self.momentum) * current_std\n","\n","    def get_target_stats(self):\n","        \"\"\"\n","        Returns target statistics for feature matching.\n","        Adapts to the evolving feature distribution during optimization.\n","        \"\"\"\n","        if self.adaptive_stats and self.running_mean is not None:\n","            return {\n","                'mean': self.running_mean,\n","                'std': self.running_std\n","            }\n","        else:\n","            # Fallback to reasonable defaults\n","            if self.features is not None:\n","                num_channels = self.features.shape[1]\n","                device = self.features.device\n","                return {\n","                    'mean': torch.zeros(num_channels, device=device),\n","                    'std': torch.ones(num_channels, device=device)\n","                }\n","            return None\n","\n","    def compute_feature_loss(self, target_stats=None):\n","        \"\"\"\n","        Compute feature distribution matching loss.\n","        \"\"\"\n","        if self.features is None:\n","            return torch.tensor(0.0, device='cpu')\n","\n","        if target_stats is None:\n","            target_stats = self.get_target_stats()\n","            if target_stats is None:\n","                return torch.tensor(0.0, device=self.features.device)\n","\n","        current_mean = self.features.mean(dim=[0, 2, 3])\n","        current_std = self.features.std(dim=[0, 2, 3])\n","\n","        # Mean and std matching with additional correlation structure\n","        mean_loss = F.mse_loss(current_mean, target_stats['mean'])\n","        std_loss = F.mse_loss(current_std, target_stats['std'])\n","\n","        # Optionally add correlation structure matching\n","        # (omitted for simplicity but could be added here)\n","\n","        return mean_loss + std_loss\n","\n","# -------------------------------------------------------------\n","# 5. Advanced Multi-scale and Multi-resolution Optimization\n","# -------------------------------------------------------------\n","def progressive_model_inversion_attack(\n","        model,\n","        target_class,\n","        scales=None,\n","        iterations_per_scale=500,\n","        lr_initial=0.1,\n","        lr_final=0.001,\n","        auto_schedule_hyperparams=True,\n","        regularization_weights={\n","            'tv': 5e-4,\n","            'l2': 1e-4,\n","            'color': 5e-5,\n","            'smooth': 1e-4,\n","            'natural': 2e-4,\n","            'fft': 1e-5,\n","            'feature': 5e-3\n","        },\n","        init_strategy='mixed',\n","        hook_layers=None,\n","        verbose=False,\n","        log_interval=50,\n","        init_img=None):\n","\n","    import sys  # for dynamic printing\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    # Get model information\n","    model_info = get_model_info(model)\n","    input_channels = model_info[\"input_channels\"] or 1\n","\n","    # For BrainMRIClassifier, we need input size 299x299\n","    if scales is None:\n","        scales = [(1, input_channels, 75, 75),\n","                  (1, input_channels, 150, 150),\n","                  (1, input_channels, 299, 299)]\n","\n","    # Force final scale to be 299x299\n","    if scales[-1][2:] != (299, 299):\n","        scales[-1] = (1, input_channels, 299, 299)\n","\n","    print(f\"Using scales: {scales} with {input_channels} input channels\")\n","\n","    # Get optimal hook layers if not specified\n","    if hook_layers is None:\n","        hook_layers = get_optimal_hook_layers(model)\n","\n","    # Initialize feature hooks\n","    hooks = []\n","    feature_extractors = {}\n","\n","    # Special handling for ensemble model\n","    if \"default_hook\" in hook_layers:\n","        print(\"Using default hooks for custom model\")\n","    else:\n","        for layer_name in hook_layers:\n","            for name, module in model.named_modules():\n","                if layer_name in name:\n","                    extractor = FeatureHook(layer_name)\n","                    feature_extractors[layer_name] = extractor\n","                    hooks.append(module.register_forward_hook(extractor.hook_fn))\n","                    break\n","\n","    # Initialize with the coarsest scale or provided init_img\n","    if init_img is None:\n","        current_img = get_initial_image(init_strategy, size=scales[0], device=device,\n","                                       target_class=target_class, channels=input_channels)\n","    else:\n","        current_img = init_img.to(device)\n","\n","    current_img.requires_grad = True\n","\n","    # Training losses for visualization\n","    losses_history = {\n","        'total': [], 'ce': [], 'tv': [], 'l2': [], 'color': [],\n","        'smooth': [], 'natural': [], 'fft': [], 'feature': []\n","    }\n","\n","    # Cross-entropy loss function\n","    ce_loss_fn = nn.CrossEntropyLoss()\n","    target = torch.tensor([target_class], device=device)\n","\n","    # Track best image and score\n","    best_img = current_img.clone().detach()\n","    best_score = float('inf')\n","\n","    # Process each scale\n","    for scale_idx, scale in enumerate(scales):\n","        # Print phase information on its own line.\n","        print(f\"\\nOptimizing at scale {scale[2]}x{scale[3]}\")\n","\n","        # Resize image if needed\n","        if current_img.shape != scale:\n","            with torch.no_grad():\n","                resized_img = F.interpolate(current_img.detach(), size=scale[2:],\n","                                           mode='bilinear', align_corners=False)\n","            current_img = resized_img.clone()\n","            current_img.requires_grad = True\n","\n","        # Determine iterations for this scale\n","        scale_factor = scale[2] / scales[0][2]\n","        iterations = int(iterations_per_scale * min(scale_factor, 2.0))\n","\n","        # Initialize optimizer and scheduler\n","        lr = lr_initial * (lr_final / lr_initial) ** (scale_idx / len(scales))\n","        optimizer = optim.AdamW([current_img], lr=lr, weight_decay=0.01)\n","        scheduler = OneCycleLR(optimizer, max_lr=lr, total_steps=iterations,\n","                             pct_start=0.3, anneal_strategy='cos')\n","\n","        # Define scheduler for hyperparameters\n","        if auto_schedule_hyperparams:\n","            def schedule_weight(base_weight, iteration, iterations, warmup=0.2):\n","                if iteration < iterations * warmup:\n","                    factor = iteration / (iterations * warmup)\n","                else:\n","                    factor = 1.0\n","                return base_weight * factor\n","        else:\n","            schedule_weight = lambda w, i, t, **kwargs: w\n","\n","        # Optimization loop for this scale\n","        for iteration in range(iterations):\n","            optimizer.zero_grad()\n","\n","            # Forward pass - ALWAYS resize to 299x299 for model output\n","            resized_img = F.interpolate(current_img, size=(299, 299),\n","                                       mode='bilinear', align_corners=False)\n","            output = model(resized_img)\n","\n","            # Calculate losses\n","            ce_loss = ce_loss_fn(output, target)\n","            tv_loss = total_variation_loss(current_img,\n","                                         schedule_weight(regularization_weights['tv'], iteration, iterations))\n","            l2_loss = schedule_weight(regularization_weights['l2'], iteration, iterations) * torch.norm(current_img, 2)\n","            color_loss = color_distribution_loss(current_img,\n","                                               schedule_weight(regularization_weights['color'], iteration, iterations))\n","            smooth_loss = perceptual_smoothness_loss(current_img,\n","                                                   schedule_weight(regularization_weights['smooth'], iteration, iterations))\n","            natural_loss = naturalness_prior_loss(current_img,\n","                                                schedule_weight(regularization_weights['natural'], iteration, iterations, warmup=0.4))\n","            fft_loss = compute_fft_loss(current_img,\n","                                       schedule_weight(regularization_weights['fft'], iteration, iterations, warmup=0.5))\n","\n","            # Feature matching losses\n","            feature_loss = 0.0\n","            if feature_extractors:\n","                feature_w = schedule_weight(regularization_weights['feature'] * scale_factor,\n","                                         iteration, iterations, warmup=0.3)\n","                for layer_name, extractor in feature_extractors.items():\n","                    feature_loss += extractor.compute_feature_loss() * feature_w\n","\n","            # Combine all losses\n","            total_loss = ce_loss + tv_loss + l2_loss + color_loss + smooth_loss + natural_loss + fft_loss + feature_loss\n","\n","            # Record losses occasionally for visualization\n","            if iteration % log_interval == 0 or iteration == iterations - 1:\n","                losses_history['total'].append(total_loss.item())\n","                losses_history['ce'].append(ce_loss.item())\n","                losses_history['tv'].append(tv_loss.item())\n","                losses_history['l2'].append(l2_loss.item())\n","                losses_history['color'].append(color_loss.item())\n","                losses_history['smooth'].append(smooth_loss.item())\n","                losses_history['natural'].append(natural_loss.item())\n","                losses_history['fft'].append(fft_loss.item())\n","                losses_history['feature'].append(feature_loss.item())\n","\n","            # Backward pass and update\n","            total_loss.backward()\n","            torch.nn.utils.clip_grad_norm_([current_img], max_norm=1.0)\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Ensure image remains in valid range\n","            current_img.data = torch.clamp(current_img.data, 0, 1)\n","\n","            # Save best result\n","            score = ce_loss.item() + 0.1 * (tv_loss.item() + smooth_loss.item() + natural_loss.item())\n","            if score < best_score:\n","                best_score = score\n","                best_img = current_img.clone().detach()\n","\n","            # Dynamic update of progress on the same line (if verbose)\n","            if verbose and (iteration % log_interval == 0 or iteration == iterations - 1):\n","                sys.stdout.write(f\"\\rScale {scale[2]}x{scale[3]} Iter {iteration}/{iterations} - Loss: {total_loss.item():.4f}, CE: {ce_loss.item():.4f}\")\n","                sys.stdout.flush()\n","        # Move to next line after finishing iterations for this scale\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","    # Clean up hooks\n","    for hook in hooks:\n","        hook.remove()\n","\n","    # Apply post-processing to the best image\n","    try:\n","        final_img = advanced_post_process(best_img)\n","        print(\"Post-processing completed successfully\")\n","    except Exception as e:\n","        print(f\"Error in post-processing: {str(e)}\")\n","        final_img = best_img  # Use best image without post-processing if there's an error\n","\n","    return final_img\n","\n","\n","def plot_losses(losses_history, filename):\n","    \"\"\"\n","    Create a detailed loss curve plot.\n","    \"\"\"\n","    plt.figure(figsize=(12, 8))\n","    # Main plot with overall loss\n","    plt.subplot(2, 1, 1)\n","    plt.plot(losses_history['total'], 'k-', label='Total Loss')\n","    plt.plot(losses_history['ce'], 'r-', label='CE Loss')\n","    plt.title('Overall Loss Progress')\n","    plt.yscale('log')\n","    plt.legend()\n","\n","    # Subplot with individual regularization terms\n","    plt.subplot(2, 1, 2)\n","    for key in ['tv', 'l2', 'color', 'smooth', 'natural', 'fft', 'feature']:\n","        if losses_history[key]:\n","            plt.plot(losses_history[key], label=f'{key} Loss')\n","    plt.title('Regularization Losses')\n","    plt.yscale('log')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(filename)\n","    plt.close()\n","\n","# -------------------------------------------------------------\n","# 6. Advanced Post-Processing Pipeline\n","# -------------------------------------------------------------\n","def advanced_post_process(img):\n","    \"\"\"\n","    Advanced post-processing pipeline for improved visual quality.\n","    Fixed to handle both grayscale and RGB images.\n","    \"\"\"\n","    # Convert tensor to numpy, handling both color and grayscale images\n","    img_cpu = img.squeeze().cpu()\n","\n","    # Check if grayscale (1 channel) or color (3 channels)\n","    if len(img_cpu.shape) == 2:\n","        # Grayscale image (H, W)\n","        img_np = img_cpu.numpy()\n","        is_grayscale = True\n","    elif len(img_cpu.shape) == 3 and img_cpu.shape[0] == 1:\n","        # Single channel image in format (1, H, W)\n","        img_np = img_cpu.numpy()[0]\n","        is_grayscale = True\n","    else:\n","        # Color image (C, H, W) -> (H, W, C)\n","        img_np = img_cpu.numpy().transpose(1, 2, 0)\n","        is_grayscale = False\n","\n","    # 1. Contrast stretching with percentile-based normalization\n","    p2, p98 = np.percentile(img_np, (2, 98))\n","    img_np = np.clip((img_np - p2) / (p98 - p2 + 1e-8), 0, 1)\n","\n","    # For grayscale images, skip color-specific enhancements\n","    if not is_grayscale:\n","        # 2. Local contrast enhancement (CLAHE-like)\n","        img_np = local_contrast_enhance(img_np)\n","\n","        # 3. Histogram equalization with color preservation\n","        img_np = histogram_equalization_with_color(img_np)\n","\n","        # 5. Color balancing\n","        img_np = color_balance(img_np)\n","    else:\n","        # Apply grayscale-specific enhancements\n","        hist, bins = np.histogram(img_np.flatten(), 256, [0, 1])\n","        cdf = hist.cumsum()\n","        cdf = cdf / (cdf[-1] + 1e-8)  # Normalize\n","        img_np = np.interp(img_np.flatten(), bins[:-1], cdf).reshape(img_np.shape)\n","\n","    # 4. Detail enhancement with edge preservation (works for both color and grayscale)\n","    if is_grayscale:\n","        # For grayscale\n","        img_np = edge_preserving_sharpen_gray(img_np)\n","\n","        # Convert back to tensor (add channel dimension)\n","        img_t = torch.from_numpy(img_np).unsqueeze(0).unsqueeze(0).float()\n","    else:\n","        # For color images\n","        img_np = edge_preserving_sharpen(img_np)\n","\n","        # Convert back to tensor\n","        img_t = torch.from_numpy(img_np.transpose(2, 0, 1)).unsqueeze(0).float()\n","\n","    return img_t\n","\n","def edge_preserving_sharpen_gray(img, sigma=0.5, amount=1.0):\n","    \"\"\"\n","    Apply edge-preserving sharpening to grayscale images.\n","    \"\"\"\n","    # Create a Gaussian kernel for edge detection\n","    kernel_size = max(3, int(2 * sigma) * 2 + 1)\n","    kernel_1d = np.exp(-np.arange(-(kernel_size//2), kernel_size//2 + 1)**2 / (2 * sigma**2))\n","    kernel_1d = kernel_1d / kernel_1d.sum()\n","    kernel_2d = np.outer(kernel_1d, kernel_1d)\n","\n","    # Apply Gaussian blur\n","    blurred = convolve2d(img, kernel_2d, mode='same', boundary='symm')\n","\n","    # Calculate edge mask\n","    gx = convolve2d(img, np.array([[-1, 0, 1]]), mode='same', boundary='symm')\n","    gy = convolve2d(img, np.array([[-1], [0], [1]]), mode='same', boundary='symm')\n","    gradient_mag = np.sqrt(gx**2 + gy**2)\n","\n","    # Normalize and invert to give less weight to edges\n","    edge_mask = 1 - np.clip(gradient_mag / (gradient_mag.max() + 1e-8), 0, 1)**2\n","\n","    # Apply sharpening with edge preservation\n","    high_freq = img - blurred\n","    sharpened = img + amount * high_freq * edge_mask\n","\n","    return np.clip(sharpened, 0, 1)\n","\n","def local_contrast_enhance(img, tile_size=16, clip_limit=3.0):\n","    \"\"\"\n","    Simplified CLAHE-like local contrast enhancement.\n","    \"\"\"\n","    result = np.zeros_like(img)\n","\n","    # Process each channel\n","    for c in range(img.shape[2]):\n","        channel = img[:, :, c]\n","        height, width = channel.shape\n","\n","        # Process each tile\n","        for y in range(0, height, tile_size):\n","            for x in range(0, width, tile_size):\n","                # Get the tile\n","                y_end = min(y + tile_size, height)\n","                x_end = min(x + tile_size, width)\n","                tile = channel[y:y_end, x:x_end]\n","\n","                # Skip empty tiles\n","                if tile.size == 0:\n","                    continue\n","\n","                # Compute histogram\n","                hist, bins = np.histogram(tile.flatten(), 256, [0, 1])\n","\n","                # Clip histogram\n","                if clip_limit > 0:\n","                    clip = clip_limit * tile.size / 256\n","                    hist_sum = 0\n","                    for i in range(len(hist)):\n","                        if hist[i] > clip:\n","                            hist_sum += hist[i] - clip\n","                            hist[i] = clip\n","\n","                    # Redistribute clipped pixels\n","                    redistr = hist_sum / 256\n","                    for i in range(len(hist)):\n","                        hist[i] += redistr\n","\n","                # Calculate CDF\n","                cdf = hist.cumsum()\n","                cdf = cdf / cdf[-1]  # Normalize\n","\n","                # Apply histogram equalization to the tile\n","                tile_result = np.interp(tile.flatten(), bins[:-1], cdf)\n","                result[y:y_end, x:x_end, c] = tile_result.reshape(tile.shape)\n","\n","    return result\n","\n","def histogram_equalization_with_color(img):\n","    \"\"\"\n","    Performs histogram equalization while preserving color relationships.\n","    Works in YCbCr color space to maintain color while enhancing contrast.\n","    \"\"\"\n","    # Convert to YCbCr-like space (simple approximation)\n","    y = 0.299 * img[:,:,0] + 0.587 * img[:,:,1] + 0.114 * img[:,:,2]\n","    cb = -0.1687 * img[:,:,0] - 0.3313 * img[:,:,1] + 0.5 * img[:,:,2] + 0.5\n","    cr = 0.5 * img[:,:,0] - 0.4187 * img[:,:,1] - 0.0813 * img[:,:,2] + 0.5\n","\n","    # Apply histogram equalization to Y channel only\n","    hist, bins = np.histogram(y.flatten(), 256, [0, 1])\n","    cdf = hist.cumsum()\n","    cdf = cdf / cdf[-1]  # Normalize\n","    y_eq = np.interp(y.flatten(), bins[:-1], cdf).reshape(y.shape)\n","\n","    # Convert back to RGB\n","    r = y_eq + 1.402 * (cr - 0.5)\n","    g = y_eq - 0.344136 * (cb - 0.5) - 0.714136 * (cr - 0.5)\n","    b = y_eq + 1.772 * (cb - 0.5)\n","\n","    # Combine and clip\n","    result = np.stack([r, g, b], axis=2)\n","    return np.clip(result, 0, 1)\n","\n","def edge_preserving_sharpen(img, sigma=0.5, amount=1.0):\n","    \"\"\"\n","    Apply edge-preserving sharpening using a bilateral filter approximation.\n","    \"\"\"\n","    # Create a Gaussian kernel for edge detection\n","    kernel_size = max(3, int(2 * sigma) * 2 + 1)\n","    kernel_1d = np.exp(-np.arange(-(kernel_size//2), kernel_size//2 + 1)**2 / (2 * sigma**2))\n","    kernel_1d = kernel_1d / kernel_1d.sum()\n","    kernel_2d = np.outer(kernel_1d, kernel_1d)\n","\n","    # Apply filtering\n","    blurred = np.zeros_like(img)\n","    for c in range(img.shape[2]):\n","        # Apply Gaussian blur for each channel\n","        blurred[:,:,c] = convolve2d(img[:,:,c], kernel_2d, mode='same', boundary='symm')\n","\n","    # Calculate edge mask\n","    edge_mask = np.ones_like(img)\n","    for c in range(img.shape[2]):\n","        # Create edge mask using gradient magnitude\n","        gx = convolve2d(img[:,:,c], np.array([[-1, 0, 1]]), mode='same', boundary='symm')\n","        gy = convolve2d(img[:,:,c], np.array([[-1], [0], [1]]), mode='same', boundary='symm')\n","        gradient_mag = np.sqrt(gx**2 + gy**2)\n","\n","        # Normalize and invert to give less weight to edges\n","        edge_mask[:,:,c] = 1 - np.clip(gradient_mag / gradient_mag.max(), 0, 1)**2\n","\n","    # Apply sharpening with edge preservation\n","    high_freq = img - blurred\n","    sharpened = img + amount * high_freq * edge_mask\n","\n","    return np.clip(sharpened, 0, 1)\n","\n","def convolve2d(img, kernel, mode='same', boundary='symm'):\n","    \"\"\"\n","    Simple 2D convolution implementation to avoid scipy dependency.\n","    \"\"\"\n","    k_h, k_w = kernel.shape\n","    i_h, i_w = img.shape\n","\n","    # Pad the image based on boundary mode\n","    if boundary == 'symm':\n","        pad_h = k_h // 2\n","        pad_w = k_w // 2\n","        padded = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='symmetric')\n","    else:\n","        pad_h = k_h // 2\n","        pad_w = k_w // 2\n","        padded = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n","\n","    # Output array\n","    out = np.zeros_like(img)\n","\n","    # Apply convolution\n","    for i in range(i_h):\n","        for j in range(i_w):\n","            out[i, j] = np.sum(padded[i:i+k_h, j:j+k_w] * kernel)\n","\n","    return out\n","\n","def color_balance(img, clip_percent=1):\n","    \"\"\"\n","    Automatically balance colors by applying separate contrast stretching to each channel.\n","    \"\"\"\n","    result = np.zeros_like(img)\n","\n","    for c in range(img.shape[2]):\n","        channel = img[:,:,c]\n","        # Calculate percentile values\n","        low = np.percentile(channel, clip_percent)\n","        high = np.percentile(channel, 100 - clip_percent)\n","\n","        # Apply contrast stretching\n","        result[:,:,c] = np.clip((channel - low) / (high - low), 0, 1)\n","\n","    return result\n","\n","# -------------------------------------------------------------\n","# 7. Highly Advanced Ensemble Attack with Knowledge Distillation\n","# -------------------------------------------------------------\n","def advanced_ensemble_attack(\n","        models,\n","        target_class,\n","        weights=None,\n","        distill_iterations=500,\n","        scales=None,\n","        verbose=True,\n","        log_interval=10,\n","        **attack_params):\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Move all models to the same device\n","    for i in range(len(models)):\n","        models[i] = models[i].to(device)\n","\n","    # Normalize weights if not provided\n","    if weights is None:\n","        weights = [1.0/len(models)] * len(models)\n","\n","    # Get model info\n","    model_info = get_model_info(models[0])\n","    input_channels = model_info[\"input_channels\"] or 1\n","\n","    # Set scales with 299x299 final size\n","    if scales is None:\n","        scales = [(1, input_channels, 112, 112),\n","                 (1, input_channels, 224, 224),\n","                 (1, input_channels, 299, 299)]\n","\n","    # Force 299x299 final scale\n","    if scales[-1][2:] != (299, 299):\n","        if verbose:\n","            print(f\"Adjusting final scale to (1, {input_channels}, 299, 299)\")\n","        scales[-1] = (1, input_channels, 299, 299)\n","\n","    # Phase 1: Individual model inversions\n","    print(\"\\nPhase 1: Individual model inversions\")\n","    individual_images = []\n","\n","    indiv_attack_params = attack_params.copy()\n","    if 'iterations_per_scale' in indiv_attack_params:\n","        indiv_attack_params['iterations_per_scale'] = indiv_attack_params['iterations_per_scale'] // 2\n","\n","    for i, model in enumerate(models):\n","        print(f\"  Inverting model {i+1}/{len(models)}\")\n","        img = progressive_model_inversion_attack(model, target_class, scales=scales[:2], **indiv_attack_params)\n","        if img is not None:  # Ensure we have a valid image\n","            individual_images.append(img)\n","        else:\n","            print(f\"  WARNING: Model {i+1} produced no image\")\n","            # Create a default random noise image\n","            default_img = torch.rand((1, input_channels, 224, 224), device=device)\n","            individual_images.append(default_img)\n","\n","    # Verify we have at least one valid image\n","    if not individual_images:\n","        print(\"ERROR: No valid images produced. Returning random noise.\")\n","        return torch.rand((1, input_channels, 299, 299), device=device)\n","\n","    # Phase 2: Distillation\n","    print(\"\\nPhase 2: Knowledge distillation from individual reconstructions\")\n","\n","    # Resize all images to the same dimensions before averaging\n","    standard_size = (1, input_channels, 224, 224)  # Use a standard size for distillation\n","    resized_images = []\n","\n","    print(\"  Standardizing image dimensions...\")\n","    for i, img in enumerate(individual_images):\n","        print(f\"  Image {i+1} shape before resize: {img.shape}\")\n","        resized = F.interpolate(img.to(device), size=standard_size[2:], mode='bilinear', align_corners=False)\n","        print(f\"  Image {i+1} shape after resize: {resized.shape}\")\n","        resized_images.append(resized)\n","\n","    # Initialize ensemble seed with weighted average of RESIZED images\n","    ensemble_seed = torch.zeros(standard_size, device=device)\n","    for img, weight in zip(resized_images, weights):\n","        ensemble_seed += weight * img\n","\n","    # Ensure ensemble_seed is valid and on the correct device\n","    ensemble_seed = torch.clamp(ensemble_seed, 0, 1)  # Ensure valid range\n","    ensemble_seed.requires_grad = True\n","\n","    # Run distillation\n","    optimizer = optim.Adam([ensemble_seed], lr=0.01)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=distill_iterations, eta_min=0.001)\n","\n","    # Debug print\n","    print(f\"  Ensemble seed shape: {ensemble_seed.shape}, device: {ensemble_seed.device}\")\n","    print(f\"  Model devices: {[next(m.parameters()).device for m in models]}\")\n","\n","    feature_hook = FeatureHook(\"features\", adaptive_stats=False)\n","    hooks = []\n","\n","    for iter in range(distill_iterations):\n","        optimizer.zero_grad()\n","\n","        # Resize to 299x299 for model\n","        resized_seed = F.interpolate(ensemble_seed, size=(299, 299),\n","                                   mode='bilinear', align_corners=False)\n","\n","        # Ensemble classification loss\n","        ce_loss = 0\n","        for model, weight in zip(models, weights):\n","            output = model(resized_seed)\n","            target = torch.tensor([target_class], device=device)\n","            ce_loss += weight * F.cross_entropy(output, target)\n","\n","        # Feature consistency with individual reconstructions\n","        feature_loss = 0\n","        for i, (model, indiv_img) in enumerate(zip(models, individual_images)):\n","            # Register temporary hooks\n","            for name, module in model.named_modules():\n","                if isinstance(module, nn.Conv2d) and 'features' in name:\n","                    hook = module.register_forward_hook(feature_hook.hook_fn)\n","                    hooks.append(hook)\n","                    break\n","\n","            # Get individual image features\n","            with torch.no_grad():\n","                indiv_img_resized = F.interpolate(indiv_img.to(device), size=(299, 299),\n","                                                mode='bilinear', align_corners=False)\n","                _ = model(indiv_img_resized)\n","                target_features = feature_hook.features.detach() if feature_hook.features is not None else None\n","\n","            # Skip if we couldn't extract features\n","            if target_features is None:\n","                continue\n","\n","            # Get current seed features\n","            _ = model(resized_seed)\n","            current_features = feature_hook.features\n","\n","            # Compute feature consistency loss if we have both feature sets\n","            if current_features is not None and target_features is not None:\n","                feature_loss += weights[i] * F.mse_loss(current_features, target_features)\n","\n","            # Clear hooks\n","            for hook in hooks:\n","                hook.remove()\n","            hooks = []\n","\n","        # Regularization\n","        tv_loss = total_variation_loss(ensemble_seed, 1e-4)\n","\n","        # Total loss\n","        total_loss = ce_loss + 0.5 * feature_loss + tv_loss\n","\n","        # Update\n","        total_loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Ensure valid range\n","        ensemble_seed.data = torch.clamp(ensemble_seed.data, 0, 1)\n","\n","        if verbose and (iter % log_interval == 0 or iter == distill_iterations - 1):\n","            print(f\"  Distillation iter {iter}/{distill_iterations}, \"\n","                  f\"Loss: {total_loss.item():.4f}, CE: {ce_loss.item():.4f}\")\n","\n","    # Phase 3: Final ensemble optimization\n","    print(\"\\nPhase 3: Final ensemble optimization\")\n","\n","    # Create ensemble model wrapper\n","    ensemble_model = EnsembleModel(models, weights).to(device)\n","\n","    # Ensure we have a valid seed image\n","    final_seed = ensemble_seed.detach().to(device)\n","    if torch.isnan(final_seed).any() or torch.isinf(final_seed).any():\n","        print(\"WARNING: NaN/Inf values detected in distilled image. Using random initialization.\")\n","        final_seed = torch.rand((1, input_channels, 224, 224), device=device)\n","\n","    # Final inversion with ensemble model\n","    final_img = progressive_model_inversion_attack(\n","        ensemble_model,\n","        target_class,\n","        scales=scales,\n","        init_img=final_seed,\n","        **attack_params\n","    )\n","\n","    # Final sanity check\n","    if final_img is None or torch.isnan(final_img).any() or torch.isinf(final_img).any():\n","        print(\"ERROR: Invalid final image. Returning the distilled seed instead.\")\n","        return final_seed\n","\n","    return final_img\n","\n","class EnsembleModel(nn.Module):\n","    def __init__(self, models, weights):\n","        super().__init__()\n","        self.models = nn.ModuleList(models)  # Use ModuleList for proper device handling\n","        self.weights = weights\n","\n","        # For feature extraction capability\n","        # Get the first model's features module if available\n","        self.features = None\n","        for model in models:\n","            if hasattr(model, 'features'):\n","                self.features = model.features\n","                break\n","\n","    def forward(self, x):\n","        outputs = []\n","        for model in self.models:\n","            outputs.append(model(x))\n","\n","        # Weighted average of logits\n","        result = torch.zeros_like(outputs[0])\n","        for output, weight in zip(outputs, self.weights):\n","            result += weight * output\n","        return result\n","\n","    # Add direct feature extraction method to simplify hook registration\n","    def get_feature_maps(self, x):\n","        if hasattr(self.models[0], 'get_feature_maps'):\n","            return self.models[0].get_feature_maps(x)\n","        return []  # Return empty list if not available\n","\n","# -------------------------------------------------------------\n","# 8. Main Routine: State-of-the-Art Federated MI Attack Framework\n","# -------------------------------------------------------------\n","def get_model_info(model):\n","    \"\"\"Extract input channels, expected input size, and classes from model architecture\"\"\"\n","    input_channels = None\n","    num_classes = None\n","    expected_input_size = None\n","\n","    # Find first conv layer for input channels\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            input_channels = module.in_channels\n","            break\n","\n","    # Try to determine expected input size from model\n","    if hasattr(model, 'flat_features'):\n","        expected_input_size = model.flat_features\n","    else:\n","        # Default to standard size for BrainMRIClassifier\n","        expected_input_size = 256 * 9 * 9  # As defined in BrainMRIClassifier\n","\n","    # Find output classes from last linear layer\n","    last_linear = None\n","    for module in model.modules():\n","        if isinstance(module, nn.Linear):\n","            last_linear = module\n","    if last_linear:\n","        num_classes = last_linear.out_features\n","\n","    return {\n","        \"input_channels\": input_channels,\n","        \"num_classes\": num_classes,\n","        \"expected_input_size\": expected_input_size\n","    }\n","\n","# Add this function to debug model dimensions:\n","def debug_model_dimensions(model, input_size=(1, 1, 299, 299)):\n","    \"\"\"Print the output shape at each layer to diagnose dimension issues\"\"\"\n","    device = next(model.parameters()).device\n","    x = torch.randn(input_size).to(device)\n","\n","    # Track feature dimensions\n","    print(f\"Input shape: {x.shape}\")\n","\n","    # Features\n","    for i, layer in enumerate(model.features):\n","        x = layer(x)\n","        print(f\"After features[{i}] {type(layer).__name__}: {x.shape}\")\n","\n","    # Check final flattened size\n","    flat_size = x.view(x.size(0), -1).shape[1]\n","    print(f\"Flattened size: {flat_size}, Expected: {model.flat_features}\")\n","\n","    # Try classifier\n","    try:\n","        output = model.classifier(x)\n","        print(f\"Final output shape: {output.shape}\")\n","    except Exception as e:\n","        print(f\"Error in classifier: {str(e)}\")"]},{"cell_type":"markdown","source":["# Modification Needed on below cell!\n","\n","Please modify the model paths (```MODEL_PATHS```) under main function below."],"metadata":{"id":"rh9RP-o5zPi1"},"id":"rh9RP-o5zPi1"},{"cell_type":"code","source":["def main():\n","    import os\n","    import sys\n","\n","    # ----------------------------------------------------------------\n","    # Step 1: Create the base save directory dynamically\n","    # ----------------------------------------------------------------\n","    base_save_path = os.path.join(os.getcwd(), \"inverted_images\", \"bench\")\n","    if not os.path.exists(base_save_path):\n","        os.makedirs(base_save_path)\n","    sys.stdout.write(f\"Base save path set to: {base_save_path}\\n\")\n","    sys.stdout.flush()\n","\n","    # ----------------------------------------------------------------\n","    # Step 2: Set up file paths for pre-trained models\n","    # ----------------------------------------------------------------\n","    MODEL_PATHS = {\n","        'global': '/pretrained_models/global_model.pth', ## <-Replace\n","        'clients': [\n","            '/pretrained_models/client_1_model.pth', ## <-Replace\n","            '/pretrained_models/client_2_model.pth', ## <-Replace\n","            '/pretrained_models/client_3_model.pth'  ## <-Replace\n","        ]\n","    }\n","    client_model_paths = MODEL_PATHS['clients']\n","    global_model_path = MODEL_PATHS['global']\n","\n","    # ----------------------------------------------------------------\n","    # Step 3: Load models with dynamic progress updates\n","    # ----------------------------------------------------------------\n","    verbose = True\n","    log_interval = 10  # Log every 10 iterations\n","\n","    sys.stdout.write(\"Loading client models...\\n\")\n","    sys.stdout.flush()\n","    client_models = []\n","    for i, path in enumerate(client_model_paths):\n","        sys.stdout.write(f\"\\rLoading client model {i+1}/{len(client_model_paths)}\")\n","        sys.stdout.flush()\n","        client_models.append(load_model(path, model_class=BrainMRIClassifier))\n","    sys.stdout.write(\"\\n\")\n","\n","    sys.stdout.write(\"Loading global model...\\n\")\n","    sys.stdout.flush()\n","    global_model = load_model(global_model_path, model_class=BrainMRIClassifier)\n","    sys.stdout.write(\"All models loaded successfully\\n\")\n","    sys.stdout.flush()\n","\n","    sys.stdout.write(\"\\nDebugging model dimensions...\\n\")\n","    sys.stdout.flush()\n","    debug_model_dimensions(client_models[0])\n","\n","    # Dynamically determine model parameters\n","    model_info = get_model_info(global_model)\n","    input_channels = model_info[\"input_channels\"] or 1\n","    num_classes = model_info[\"num_classes\"] or 4\n","    sys.stdout.write(f\"Detected model configuration: {input_channels} input channels, {num_classes} output classes\\n\")\n","    sys.stdout.flush()\n","\n","    # ----------------------------------------------------------------\n","    # Step 4: Set inversion parameters and scales\n","    # ----------------------------------------------------------------\n","    num_samples = 20  # Number of inversion samples per model per class\n","    target_classes = list(range(min(4, num_classes)))\n","    scales = [(1, input_channels, 112, 112),\n","              (1, input_channels, 224, 224),\n","              (1, input_channels, 299, 299)]\n","    attack_params = {\n","        'iterations_per_scale': 1000,\n","        'lr_initial': 0.05,\n","        'lr_final': 0.001,\n","        'scales': scales,\n","        'auto_schedule_hyperparams': True,\n","        'regularization_weights': {\n","            'tv': 2e-3,\n","            'l2': 5e-4,\n","            'color': 1e-4,\n","            'smooth': 3e-4,\n","            'natural': 5e-4,\n","            'fft': 5e-5,\n","            'feature': 1e-2\n","        },\n","        'init_strategy': 'mixed'\n","    }\n","\n","    # For ensemble attack: define weights and aggregate all models\n","    ensemble_weights = [0.5, 0.3, 0.2, 1.0]\n","    all_models = client_models + [global_model]\n","\n","    # Dictionary to store one sample per model type per class for later comparison\n","    inverted_images = {}\n","\n","    # ----------------------------------------------------------------\n","    # Step 5: Process each target class\n","    # ----------------------------------------------------------------\n","    for target_class in target_classes:\n","        # Create subfolder for this target class\n","        class_save_path = os.path.join(base_save_path, f\"class_{target_class}\")\n","        if not os.path.exists(class_save_path):\n","            os.makedirs(class_save_path)\n","        sys.stdout.write(f\"\\n=== Processing target class {target_class} ===\\n\")\n","        sys.stdout.write(f\"Images for class {target_class} will be saved in: {class_save_path}\\n\")\n","        sys.stdout.flush()\n","\n","        # ---- Inversions for Client Models ----\n","        for i, model in enumerate(client_models):\n","            client_save_path = os.path.join(class_save_path, f\"client_{i+1}\")\n","            if not os.path.exists(client_save_path):\n","                os.makedirs(client_save_path)\n","            sys.stdout.write(f\"\\nPerforming inversion attack on Client Model {i+1} for class {target_class}\\n\")\n","            sys.stdout.flush()\n","            first_sample_saved = False\n","            for sample in range(num_samples):\n","                inv_img = progressive_model_inversion_attack(\n","                    model,\n","                    target_class,\n","                    verbose=verbose,\n","                    log_interval=log_interval,\n","                    **attack_params\n","                )\n","                filename = os.path.join(\n","                    client_save_path,\n","                    f\"client_model_{i+1}_class_{target_class}_inv_{sample+1}.png\"\n","                )\n","                vutils.save_image(inv_img, filename)\n","                sys.stdout.write(f\"\\rClient Model {i+1} [Class {target_class}]: Sample {sample+1}/{num_samples} saved\")\n","                sys.stdout.flush()\n","                if not first_sample_saved:\n","                    inverted_images[f'client_model_{i+1}_class_{target_class}'] = inv_img\n","                    first_sample_saved = True\n","            sys.stdout.write(\"\\n\")\n","            sys.stdout.flush()\n","\n","        # ---- Inversions for Global Model ----\n","        global_save_path = os.path.join(class_save_path, \"global\")\n","        if not os.path.exists(global_save_path):\n","            os.makedirs(global_save_path)\n","        sys.stdout.write(f\"\\nPerforming inversion attack on Global Model for class {target_class}\\n\")\n","        sys.stdout.flush()\n","        first_sample_saved = False\n","        for sample in range(num_samples):\n","            inv_img_global = progressive_model_inversion_attack(\n","                global_model,\n","                target_class,\n","                verbose=verbose,\n","                log_interval=log_interval,\n","                **attack_params\n","            )\n","            filename = os.path.join(\n","                global_save_path,\n","                f\"global_model_class_{target_class}_inv_{sample+1}.png\"\n","            )\n","            vutils.save_image(inv_img_global, filename)\n","            sys.stdout.write(f\"\\rGlobal Model [Class {target_class}]: Sample {sample+1}/{num_samples} saved\")\n","            sys.stdout.flush()\n","            if not first_sample_saved:\n","                inverted_images[f'global_model_class_{target_class}'] = inv_img_global\n","                first_sample_saved = True\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","        # ---- Inversions for Ensemble Model ----\n","        ensemble_save_path = os.path.join(class_save_path, \"ensemble\")\n","        if not os.path.exists(ensemble_save_path):\n","            os.makedirs(ensemble_save_path)\n","        sys.stdout.write(f\"\\nPerforming advanced ensemble attack for class {target_class}\\n\")\n","        sys.stdout.flush()\n","        first_sample_saved = False\n","        for sample in range(num_samples):\n","            inv_img_ensemble = advanced_ensemble_attack(\n","                all_models,\n","                target_class,\n","                weights=ensemble_weights,\n","                verbose=verbose,\n","                log_interval=log_interval,\n","                **attack_params\n","            )\n","            filename = os.path.join(\n","                ensemble_save_path,\n","                f\"ensemble_model_class_{target_class}_inv_{sample+1}.png\"\n","            )\n","            vutils.save_image(inv_img_ensemble, filename)\n","            sys.stdout.write(f\"\\rEnsemble Model [Class {target_class}]: Sample {sample+1}/{num_samples} saved\")\n","            sys.stdout.flush()\n","            if not first_sample_saved:\n","                inverted_images[f'ensemble_model_class_{target_class}'] = inv_img_ensemble\n","                first_sample_saved = True\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","        # ---- Optional Comparative Analysis ----\n","        sys.stdout.write(f\"\\nRunning comparative analysis for class {target_class}...\\n\")\n","        sys.stdout.flush()\n","        compare_reconstructions(\n","            [\n","                inverted_images.get(f'client_model_1_class_{target_class}'),\n","                inverted_images.get(f'client_model_2_class_{target_class}'),\n","                inverted_images.get(f'client_model_3_class_{target_class}'),\n","                inverted_images.get(f'global_model_class_{target_class}'),\n","                inverted_images.get(f'ensemble_model_class_{target_class}')\n","            ],\n","            ['Client 1', 'Client 2', 'Client 3', 'Global Model', 'Ensemble Model'],\n","            os.path.join(class_save_path, f\"class_{target_class}_comparison.png\")\n","        )\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","    # ----------------------------------------------------------------\n","    # Step 6: Final display of results if needed\n","    # ----------------------------------------------------------------\n","    display_results(inverted_images)\n","\n","\n","\n","\n","def compare_reconstructions(images, labels, filename):\n","    \"\"\"\n","    Create a visual comparison of different reconstruction methods\n","    with evaluation metrics. Fixed to handle different tensor dimensions.\n","    \"\"\"\n","    fig, axs = plt.subplots(1, len(images), figsize=(4*len(images), 4))\n","\n","    # Handle the case of a single image (convert axs to array for consistent indexing)\n","    if len(images) == 1:\n","        axs = np.array([axs])\n","\n","    for i, (img, label) in enumerate(zip(images, labels)):\n","        # Display the image with proper dimension handling\n","        img_cpu = img.squeeze().cpu()\n","\n","        # Check tensor dimensions\n","        if len(img_cpu.shape) == 2:\n","            # Already a 2D grayscale image (H, W)\n","            img_np = img_cpu.numpy()\n","            axs[i].imshow(img_np, cmap='gray')\n","\n","        elif len(img_cpu.shape) == 3:\n","            if img_cpu.shape[0] == 1:\n","                # Single channel image (1, H, W)\n","                img_np = img_cpu[0].numpy()\n","                axs[i].imshow(img_np, cmap='gray')\n","            else:\n","                # Color image (3, H, W) -> (H, W, 3)\n","                img_np = img_cpu.permute(1, 2, 0).numpy()\n","                axs[i].imshow(img_np)\n","        else:\n","            # Unexpected format - display blank with error text\n","            axs[i].text(0.5, 0.5, f\"Invalid shape: {img_cpu.shape}\",\n","                      ha='center', va='center', transform=axs[i].transAxes)\n","\n","        # Try to calculate quality metrics\n","        try:\n","            if len(img_cpu.shape) == 3 and img_cpu.shape[0] == 3:\n","                # Color image\n","                sharpness = calculate_sharpness(img_np)\n","                colorfulness = calculate_colorfulness(img_np)\n","                axs[i].set_title(f\"{label}\\nSharp: {sharpness:.2f}, Color: {colorfulness:.2f}\")\n","            else:\n","                # Grayscale image - only calculate sharpness\n","                if len(img_cpu.shape) == 2:\n","                    sharpness = calculate_sharpness_gray(img_cpu.numpy())\n","                else:\n","                    sharpness = calculate_sharpness_gray(img_cpu[0].numpy())\n","                axs[i].set_title(f\"{label}\\nSharp: {sharpness:.2f}\")\n","        except Exception as e:\n","            # If metrics fail, just show the label\n","            print(f\"Error calculating metrics for {label}: {str(e)}\")\n","            axs[i].set_title(label)\n","\n","        axs[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig(filename, dpi=300)\n","    plt.close()\n","\n","def calculate_sharpness_gray(img):\n","    \"\"\"\n","    Calculate image sharpness using gradient magnitude for grayscale images.\n","    \"\"\"\n","    # Compute gradients\n","    if img.shape[0] > 1 and img.shape[1] > 1:\n","        gx = img[1:, :] - img[:-1, :]\n","        gy = img[:, 1:] - img[:, :-1]\n","\n","        # Use the valid region where both gradients are available\n","        if gx.shape[1] > 0 and gy.shape[0] > 0:\n","            gx_valid = gx[:, :-1]\n","            gy_valid = gy[:-1, :]\n","\n","            # Compute gradient magnitude\n","            grad_mag = np.sqrt(gx_valid**2 + gy_valid**2)\n","\n","            # Return mean gradient magnitude as sharpness measure\n","            return np.mean(grad_mag)\n","\n","    # Fallback\n","    return 0.0\n","\n","def calculate_sharpness(img):\n","    \"\"\"\n","    Calculate image sharpness using gradient magnitude for color images.\n","    \"\"\"\n","    # Convert to grayscale for gradient calculation\n","    gray = 0.2989 * img[:,:,0] + 0.5870 * img[:,:,1] + 0.1140 * img[:,:,2]\n","    return calculate_sharpness_gray(gray)\n","\n","def calculate_colorfulness(img):\n","    \"\"\"\n","    Calculate perceptual colorfulness metric.\n","    Based on Hasler and SÃ¼sstrunk (2003) metric.\n","    \"\"\"\n","    # Split image into channels\n","    r = img[:,:,0]\n","    g = img[:,:,1]\n","    b = img[:,:,2]\n","\n","    # Compute rg and yb components\n","    rg = r - g\n","    yb = 0.5 * (r + g) - b\n","\n","    # Compute mean and std of components\n","    rg_mean = np.mean(rg)\n","    rg_std = np.std(rg)\n","    yb_mean = np.mean(yb)\n","    yb_std = np.std(yb)\n","\n","    # Compute the colorfulness metric\n","    mean_rgyb = np.sqrt(rg_mean**2 + yb_mean**2)\n","    std_rgyb = np.sqrt(rg_std**2 + yb_std**2)\n","\n","    return mean_rgyb + std_rgyb\n","\n","# Also update display_results function\n","def display_results(inverted_images):\n","    \"\"\"\n","    Display all generated images grouped by target class with metrics.\n","    \"\"\"\n","    # Group by class\n","    class_groups = {}\n","    for key in inverted_images:\n","        if 'class' in key:\n","            class_id = key.split('class_')[1].split('_')[0]\n","            if class_id not in class_groups:\n","                class_groups[class_id] = []\n","            class_groups[class_id].append((key, inverted_images[key]))\n","\n","    # Plot each class separately with quality metrics\n","    for class_id, images in class_groups.items():\n","        # Handle differently based on number of images\n","        num_images = len(images)\n","        if num_images == 0:\n","            continue\n","\n","        fig, axs = plt.subplots(2, num_images, figsize=(15, 10))\n","        fig.suptitle(f\"Reconstructions for Class {class_id}\", fontsize=16)\n","\n","        # If only one image, axs needs to be reshaped for consistent indexing\n","        if num_images == 1:\n","            axs = np.array([[axs[0]], [axs[1]]])\n","\n","        for i, (key, img_tensor) in enumerate(images):\n","            # Convert tensor to numpy for display with dimension handling\n","            img = img_tensor.squeeze().cpu()\n","\n","            # Handle both color and grayscale images\n","            if len(img.shape) == 3 and img.shape[0] in [1, 3]:\n","                if img.shape[0] == 1:\n","                    # Grayscale image\n","                    img_display = img[0].numpy()\n","                    axs[0, i].imshow(img_display, cmap='gray')\n","                else:\n","                    # Color image - permute to HWC for plotting\n","                    img_display = img.permute(1, 2, 0).numpy()\n","                    axs[0, i].imshow(img_display)\n","            elif len(img.shape) == 2:\n","                # Direct 2D grayscale\n","                axs[0, i].imshow(img.numpy(), cmap='gray')\n","            else:\n","                # Unexpected format\n","                axs[0, i].text(0.5, 0.5, f\"Invalid shape: {img.shape}\",\n","                             ha='center', va='center', transform=axs[0, i].transAxes)\n","\n","            axs[0, i].set_title(key.split(f'_class_{class_id}')[0])\n","            axs[0, i].axis('off')\n","\n","            # Display frequency spectrum for visualization\n","            try:\n","                # Convert to grayscale for FFT if needed\n","                if len(img.shape) == 3 and img.shape[0] == 3:\n","                    gray = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]\n","                elif len(img.shape) == 3 and img.shape[0] == 1:\n","                    gray = img[0]\n","                else:\n","                    gray = img\n","\n","                f_transform = np.fft.fft2(gray.numpy())\n","                f_transform_shifted = np.fft.fftshift(f_transform)\n","                magnitude = np.log(np.abs(f_transform_shifted) + 1)\n","\n","                axs[1, i].imshow(magnitude, cmap='viridis')\n","                axs[1, i].set_title('Frequency Domain')\n","                axs[1, i].axis('off')\n","            except Exception as e:\n","                axs[1, i].text(0.5, 0.5, f\"FFT error: {str(e)[:20]}...\",\n","                             ha='center', va='center', transform=axs[1, i].transAxes)\n","\n","        plt.tight_layout()\n","        plt.subplots_adjust(top=0.9)\n","        plt.savefig(f'comparison_class_{class_id}.png')\n","        plt.close()"],"metadata":{"id":"38ojmohlzPPt"},"id":"38ojmohlzPPt","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"51a4fa17-0922-4b5e-9625-e80027447d39","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"id":"51a4fa17-0922-4b5e-9625-e80027447d39","outputId":"68937251-dd90-47ed-e8cc-ba3259b05f24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Base save path set to: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/bench\n","Loading client models...\n","Loading client model 3/3\n","Loading global model...\n","All models loaded successfully\n","\n","Debugging model dimensions...\n","Input shape: torch.Size([1, 1, 299, 299])\n","After features[0] Conv2d: torch.Size([1, 32, 299, 299])\n","After features[1] ReLU: torch.Size([1, 32, 299, 299])\n","After features[2] MaxPool2d: torch.Size([1, 32, 149, 149])\n","After features[3] BatchNorm2d: torch.Size([1, 32, 149, 149])\n","After features[4] Conv2d: torch.Size([1, 64, 149, 149])\n","After features[5] ReLU: torch.Size([1, 64, 149, 149])\n","After features[6] MaxPool2d: torch.Size([1, 64, 74, 74])\n","After features[7] BatchNorm2d: torch.Size([1, 64, 74, 74])\n","After features[8] Conv2d: torch.Size([1, 128, 74, 74])\n","After features[9] ReLU: torch.Size([1, 128, 74, 74])\n","After features[10] MaxPool2d: torch.Size([1, 128, 37, 37])\n","After features[11] BatchNorm2d: torch.Size([1, 128, 37, 37])\n","After features[12] Conv2d: torch.Size([1, 256, 37, 37])\n","After features[13] ReLU: torch.Size([1, 256, 37, 37])\n","After features[14] MaxPool2d: torch.Size([1, 256, 18, 18])\n","After features[15] BatchNorm2d: torch.Size([1, 256, 18, 18])\n","After features[16] Conv2d: torch.Size([1, 256, 18, 18])\n","After features[17] ReLU: torch.Size([1, 256, 18, 18])\n","After features[18] MaxPool2d: torch.Size([1, 256, 9, 9])\n","After features[19] BatchNorm2d: torch.Size([1, 256, 9, 9])\n","Flattened size: 20736, Expected: 20736\n","Final output shape: torch.Size([1, 4])\n","Detected model configuration: 1 input channels, 4 output classes\n","\n","=== Processing target class 0 ===\n","Images for class 0 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/bench/class_0\n","\n","Performing inversion attack on Client Model 1 for class 0\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0267, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0120, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0269, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0270, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0119, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0273, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0133, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0274, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0267, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0272, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0264, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0098, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0268, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0267, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0109, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0269, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0272, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0104, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0268, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0099, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0267, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0264, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0123, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0271, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0265, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0112, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0264, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0268, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0118, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0272, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0107, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 0]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 0\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0144, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0112, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0111, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0109, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0111, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0144, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0111, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0109, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0109, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0122, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0112, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 0]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 0\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0210, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0090, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0129, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0128, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0139, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0130, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0261, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0161, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0084, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0136, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0131, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0171, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0090, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0128, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0203, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0132, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0128, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0077, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0168, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0087, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0259, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0168, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0129, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0129, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0166, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0132, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0211, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0089, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 0]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 0\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0104, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0257, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0103, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0244, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0083, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0243, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0153, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0089, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0244, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0085, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0094, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0105, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0178, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0243, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0076, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0083, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0141, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0095, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0086, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 0]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 0\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 176.8092, CE: 176.7256\n","  Distillation iter 10/500, Loss: 31.1177, CE: 31.0416\n","  Distillation iter 20/500, Loss: 9.7882, CE: 9.7173\n","  Distillation iter 30/500, Loss: 2.2493, CE: 2.1818\n","  Distillation iter 40/500, Loss: 0.3486, CE: 0.2829\n","  Distillation iter 50/500, Loss: 0.0735, CE: 0.0087\n","  Distillation iter 60/500, Loss: 0.0645, CE: 0.0001\n","  Distillation iter 70/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0637, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 176.9312, CE: 176.8479\n","  Distillation iter 10/500, Loss: 35.0302, CE: 34.9544\n","  Distillation iter 20/500, Loss: 10.2952, CE: 10.2249\n","  Distillation iter 30/500, Loss: 1.7132, CE: 1.6464\n","  Distillation iter 40/500, Loss: 0.1772, CE: 0.1123\n","  Distillation iter 50/500, Loss: 0.0652, CE: 0.0011\n","  Distillation iter 60/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0631, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 180.5706, CE: 180.4875\n","  Distillation iter 10/500, Loss: 30.9283, CE: 30.8526\n","  Distillation iter 20/500, Loss: 6.3086, CE: 6.2382\n","  Distillation iter 30/500, Loss: 0.4946, CE: 0.4274\n","  Distillation iter 40/500, Loss: 0.0923, CE: 0.0265\n","  Distillation iter 50/500, Loss: 0.0656, CE: 0.0004\n","  Distillation iter 60/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0643, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 175.6077, CE: 175.5258\n","  Distillation iter 10/500, Loss: 30.2795, CE: 30.2054\n","  Distillation iter 20/500, Loss: 8.4611, CE: 8.3923\n","  Distillation iter 30/500, Loss: 1.0515, CE: 0.9861\n","  Distillation iter 40/500, Loss: 0.1005, CE: 0.0367\n","  Distillation iter 50/500, Loss: 0.0639, CE: 0.0007\n","  Distillation iter 60/500, Loss: 0.0629, CE: 0.0001\n","  Distillation iter 70/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0622, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 178.3029, CE: 178.2192\n","  Distillation iter 10/500, Loss: 29.9696, CE: 29.8932\n","  Distillation iter 20/500, Loss: 5.4072, CE: 5.3363\n","  Distillation iter 30/500, Loss: 0.5616, CE: 0.4939\n","  Distillation iter 40/500, Loss: 0.1064, CE: 0.0401\n","  Distillation iter 50/500, Loss: 0.0664, CE: 0.0006\n","  Distillation iter 60/500, Loss: 0.0655, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0654, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0653, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0653, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0653, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0653, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0653, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0647, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 184.5138, CE: 184.4306\n","  Distillation iter 10/500, Loss: 33.4970, CE: 33.4214\n","  Distillation iter 20/500, Loss: 9.4089, CE: 9.3387\n","  Distillation iter 30/500, Loss: 1.3760, CE: 1.3093\n","  Distillation iter 40/500, Loss: 0.1845, CE: 0.1195\n","  Distillation iter 50/500, Loss: 0.0658, CE: 0.0016\n","  Distillation iter 60/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0633, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 169.6460, CE: 169.5642\n","  Distillation iter 10/500, Loss: 28.6285, CE: 28.5541\n","  Distillation iter 20/500, Loss: 6.6486, CE: 6.5793\n","  Distillation iter 30/500, Loss: 0.2450, CE: 0.1790\n","  Distillation iter 40/500, Loss: 0.0658, CE: 0.0011\n","  Distillation iter 50/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0633, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 169.5131, CE: 169.4289\n","  Distillation iter 10/500, Loss: 33.7671, CE: 33.6906\n","  Distillation iter 20/500, Loss: 9.9859, CE: 9.9148\n","  Distillation iter 30/500, Loss: 3.1680, CE: 3.1003\n","  Distillation iter 40/500, Loss: 0.5553, CE: 0.4895\n","  Distillation iter 50/500, Loss: 0.0761, CE: 0.0112\n","  Distillation iter 60/500, Loss: 0.0646, CE: 0.0001\n","  Distillation iter 70/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0637, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 168.9087, CE: 168.8273\n","  Distillation iter 10/500, Loss: 29.4490, CE: 29.3753\n","  Distillation iter 20/500, Loss: 7.5500, CE: 7.4816\n","  Distillation iter 30/500, Loss: 0.8567, CE: 0.7915\n","  Distillation iter 40/500, Loss: 0.1578, CE: 0.0941\n","  Distillation iter 50/500, Loss: 0.0670, CE: 0.0039\n","  Distillation iter 60/500, Loss: 0.0631, CE: 0.0003\n","  Distillation iter 70/500, Loss: 0.0628, CE: 0.0001\n","  Distillation iter 80/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0621, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 169.6686, CE: 169.5857\n","  Distillation iter 10/500, Loss: 33.9389, CE: 33.8637\n","  Distillation iter 20/500, Loss: 8.4170, CE: 8.3474\n","  Distillation iter 30/500, Loss: 0.3236, CE: 0.2574\n","  Distillation iter 40/500, Loss: 0.0665, CE: 0.0017\n","  Distillation iter 50/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0633, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 187.7769, CE: 187.6958\n","  Distillation iter 10/500, Loss: 31.5223, CE: 31.4481\n","  Distillation iter 20/500, Loss: 2.5858, CE: 2.5165\n","  Distillation iter 30/500, Loss: 0.1525, CE: 0.0859\n","  Distillation iter 40/500, Loss: 0.0671, CE: 0.0015\n","  Distillation iter 50/500, Loss: 0.0653, CE: 0.0001\n","  Distillation iter 60/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0646, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0644, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 171.3486, CE: 171.2655\n","  Distillation iter 10/500, Loss: 35.0706, CE: 34.9948\n","  Distillation iter 20/500, Loss: 9.1746, CE: 9.1042\n","  Distillation iter 30/500, Loss: 1.6057, CE: 1.5387\n","  Distillation iter 40/500, Loss: 0.1856, CE: 0.1204\n","  Distillation iter 50/500, Loss: 0.0683, CE: 0.0039\n","  Distillation iter 60/500, Loss: 0.0643, CE: 0.0001\n","  Distillation iter 70/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0634, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 168.9773, CE: 168.8945\n","  Distillation iter 10/500, Loss: 32.8940, CE: 32.8188\n","  Distillation iter 20/500, Loss: 10.2118, CE: 10.1420\n","  Distillation iter 30/500, Loss: 2.3174, CE: 2.2509\n","  Distillation iter 40/500, Loss: 0.5133, CE: 0.4487\n","  Distillation iter 50/500, Loss: 0.1133, CE: 0.0495\n","  Distillation iter 60/500, Loss: 0.0643, CE: 0.0009\n","  Distillation iter 70/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0626, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 181.0275, CE: 180.9447\n","  Distillation iter 10/500, Loss: 32.2551, CE: 32.1797\n","  Distillation iter 20/500, Loss: 7.7877, CE: 7.7177\n","  Distillation iter 30/500, Loss: 0.6700, CE: 0.6032\n","  Distillation iter 40/500, Loss: 0.2272, CE: 0.1619\n","  Distillation iter 50/500, Loss: 0.0852, CE: 0.0206\n","  Distillation iter 60/500, Loss: 0.0670, CE: 0.0026\n","  Distillation iter 70/500, Loss: 0.0649, CE: 0.0007\n","  Distillation iter 80/500, Loss: 0.0645, CE: 0.0003\n","  Distillation iter 90/500, Loss: 0.0644, CE: 0.0002\n","  Distillation iter 100/500, Loss: 0.0643, CE: 0.0002\n","  Distillation iter 110/500, Loss: 0.0643, CE: 0.0002\n","  Distillation iter 120/500, Loss: 0.0642, CE: 0.0001\n","  Distillation iter 130/500, Loss: 0.0642, CE: 0.0001\n","  Distillation iter 140/500, Loss: 0.0642, CE: 0.0001\n","  Distillation iter 150/500, Loss: 0.0641, CE: 0.0001\n","  Distillation iter 160/500, Loss: 0.0641, CE: 0.0001\n","  Distillation iter 170/500, Loss: 0.0641, CE: 0.0001\n","  Distillation iter 180/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 190/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 200/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 210/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 220/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 230/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 240/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 250/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 260/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 270/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 280/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 290/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 300/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 310/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 320/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 330/500, Loss: 0.0638, CE: 0.0001\n","  Distillation iter 340/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 350/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 360/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 370/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 380/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 390/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 400/500, Loss: 0.0637, CE: 0.0001\n","  Distillation iter 410/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0636, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 177.8059, CE: 177.7240\n","  Distillation iter 10/500, Loss: 29.4902, CE: 29.4161\n","  Distillation iter 20/500, Loss: 6.2656, CE: 6.1969\n","  Distillation iter 30/500, Loss: 0.5673, CE: 0.5018\n","  Distillation iter 40/500, Loss: 0.0836, CE: 0.0195\n","  Distillation iter 50/500, Loss: 0.0638, CE: 0.0002\n","  Distillation iter 60/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0627, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 184.5639, CE: 184.4815\n","  Distillation iter 10/500, Loss: 35.4197, CE: 35.3444\n","  Distillation iter 20/500, Loss: 10.3598, CE: 10.2896\n","  Distillation iter 30/500, Loss: 2.0007, CE: 1.9338\n","  Distillation iter 40/500, Loss: 0.3764, CE: 0.3112\n","  Distillation iter 50/500, Loss: 0.1007, CE: 0.0363\n","  Distillation iter 60/500, Loss: 0.0670, CE: 0.0029\n","  Distillation iter 70/500, Loss: 0.0643, CE: 0.0003\n","  Distillation iter 80/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 90/500, Loss: 0.0640, CE: 0.0001\n","  Distillation iter 100/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 110/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 120/500, Loss: 0.0639, CE: 0.0001\n","  Distillation iter 130/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0634, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 172.1892, CE: 172.1064\n","  Distillation iter 10/500, Loss: 29.6705, CE: 29.5954\n","  Distillation iter 20/500, Loss: 8.1857, CE: 8.1160\n","  Distillation iter 30/500, Loss: 0.8191, CE: 0.7528\n","  Distillation iter 40/500, Loss: 0.1102, CE: 0.0454\n","  Distillation iter 50/500, Loss: 0.0646, CE: 0.0005\n","  Distillation iter 60/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0631, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 183.6878, CE: 183.6052\n","  Distillation iter 10/500, Loss: 31.9057, CE: 31.8307\n","  Distillation iter 20/500, Loss: 8.7230, CE: 8.6533\n","  Distillation iter 30/500, Loss: 1.3917, CE: 1.3253\n","  Distillation iter 40/500, Loss: 0.1528, CE: 0.0880\n","  Distillation iter 50/500, Loss: 0.0652, CE: 0.0011\n","  Distillation iter 60/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0631, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 185.3154, CE: 185.2333\n","  Distillation iter 10/500, Loss: 32.4308, CE: 32.3564\n","  Distillation iter 20/500, Loss: 8.1500, CE: 8.0808\n","  Distillation iter 30/500, Loss: 2.0834, CE: 2.0172\n","  Distillation iter 40/500, Loss: 0.3436, CE: 0.2792\n","  Distillation iter 50/500, Loss: 0.0892, CE: 0.0255\n","  Distillation iter 60/500, Loss: 0.0659, CE: 0.0026\n","  Distillation iter 70/500, Loss: 0.0638, CE: 0.0005\n","  Distillation iter 80/500, Loss: 0.0634, CE: 0.0002\n","  Distillation iter 90/500, Loss: 0.0633, CE: 0.0002\n","  Distillation iter 100/500, Loss: 0.0632, CE: 0.0001\n","  Distillation iter 110/500, Loss: 0.0632, CE: 0.0001\n","  Distillation iter 120/500, Loss: 0.0632, CE: 0.0001\n","  Distillation iter 130/500, Loss: 0.0632, CE: 0.0001\n","  Distillation iter 140/500, Loss: 0.0631, CE: 0.0001\n","  Distillation iter 150/500, Loss: 0.0631, CE: 0.0001\n","  Distillation iter 160/500, Loss: 0.0631, CE: 0.0001\n","  Distillation iter 170/500, Loss: 0.0631, CE: 0.0001\n","  Distillation iter 180/500, Loss: 0.0630, CE: 0.0001\n","  Distillation iter 190/500, Loss: 0.0630, CE: 0.0001\n","  Distillation iter 200/500, Loss: 0.0630, CE: 0.0001\n","  Distillation iter 210/500, Loss: 0.0630, CE: 0.0001\n","  Distillation iter 220/500, Loss: 0.0629, CE: 0.0001\n","  Distillation iter 230/500, Loss: 0.0629, CE: 0.0001\n","  Distillation iter 240/500, Loss: 0.0629, CE: 0.0001\n","  Distillation iter 250/500, Loss: 0.0629, CE: 0.0001\n","  Distillation iter 260/500, Loss: 0.0629, CE: 0.0001\n","  Distillation iter 270/500, Loss: 0.0628, CE: 0.0001\n","  Distillation iter 280/500, Loss: 0.0628, CE: 0.0001\n","  Distillation iter 290/500, Loss: 0.0628, CE: 0.0001\n","  Distillation iter 300/500, Loss: 0.0628, CE: 0.0001\n","  Distillation iter 310/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0626, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 175.3686, CE: 175.2862\n","  Distillation iter 10/500, Loss: 34.5808, CE: 34.5057\n","  Distillation iter 20/500, Loss: 10.6581, CE: 10.5883\n","  Distillation iter 30/500, Loss: 2.0558, CE: 1.9897\n","  Distillation iter 40/500, Loss: 0.2836, CE: 0.2193\n","  Distillation iter 50/500, Loss: 0.0819, CE: 0.0185\n","  Distillation iter 60/500, Loss: 0.0642, CE: 0.0011\n","  Distillation iter 70/500, Loss: 0.0632, CE: 0.0002\n","  Distillation iter 80/500, Loss: 0.0630, CE: 0.0001\n","  Distillation iter 90/500, Loss: 0.0630, CE: 0.0001\n","  Distillation iter 100/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0624, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 0]: Sample 20/20 saved\n","\n","Running comparative analysis for class 0...\n","\n","\n","=== Processing target class 1 ===\n","Images for class 1 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/bench/class_1\n","\n","Performing inversion attack on Client Model 1 for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0265, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0137, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0098, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0272, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0119, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0092, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0261, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0122, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0090, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0105, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0267, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0129, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0262, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0265, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0097, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0261, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0259, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0107, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0093, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0262, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0261, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0094, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0265, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0266, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0096, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0259, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0091, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0257, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0130, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0198, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0095, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0200, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0193, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0095, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0198, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0202, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0175, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0194, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0095, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0178, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0090, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0183, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0214, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0186, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0086, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0195, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0123, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0196, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0094, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0212, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0200, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0204, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0099, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0112, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0073, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0203, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0099, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0199, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0098, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0193, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0094, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0086, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0209, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0086, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0051, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0093, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0221, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0089, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0204, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0076, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0208, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0215, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0085, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0210, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0082, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0210, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0215, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0097, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0091, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0202, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0076, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0093, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0209, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0090, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0210, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0221, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0201, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0071, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0238, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0157, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0239, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0238, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0238, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0127, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0239, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0126, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0130, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0238, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0239, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 1\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 41.4186, CE: 41.3364\n","  Distillation iter 10/500, Loss: 0.0761, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0742, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0715, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 48.9786, CE: 48.8958\n","  Distillation iter 10/500, Loss: 0.0762, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0742, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0715, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 45.3302, CE: 45.2477\n","  Distillation iter 10/500, Loss: 0.0765, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0747, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0741, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0721, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 44.0881, CE: 44.0062\n","  Distillation iter 10/500, Loss: 0.0762, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0718, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 46.8719, CE: 46.7891\n","  Distillation iter 10/500, Loss: 0.0765, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0744, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0718, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 48.9800, CE: 48.8986\n","  Distillation iter 10/500, Loss: 0.0753, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0708, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 41.4359, CE: 41.3538\n","  Distillation iter 10/500, Loss: 0.0763, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0746, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0739, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0719, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 48.0737, CE: 47.9910\n","  Distillation iter 10/500, Loss: 0.0766, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0747, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0740, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0721, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 47.3708, CE: 47.2885\n","  Distillation iter 10/500, Loss: 0.0756, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0708, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 46.9053, CE: 46.8233\n","  Distillation iter 10/500, Loss: 0.0757, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0710, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 43.4461, CE: 43.3640\n","  Distillation iter 10/500, Loss: 0.0761, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0743, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0716, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 44.9887, CE: 44.9072\n","  Distillation iter 10/500, Loss: 0.0754, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0716, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0715, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0714, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0713, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0710, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0709, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 45.9869, CE: 45.9046\n","  Distillation iter 10/500, Loss: 0.0764, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0746, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0739, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0720, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 46.9939, CE: 46.9111\n","  Distillation iter 10/500, Loss: 0.0765, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0718, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 47.1385, CE: 47.0557\n","  Distillation iter 10/500, Loss: 0.0765, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0746, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0719, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 38.9242, CE: 38.8419\n","  Distillation iter 10/500, Loss: 0.0763, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0717, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 46.4098, CE: 46.3274\n","  Distillation iter 10/500, Loss: 0.0763, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0718, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 46.2949, CE: 46.2107\n","  Distillation iter 10/500, Loss: 0.0778, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0758, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0750, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0747, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0746, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0744, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0744, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0743, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0743, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0743, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0742, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0742, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0741, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0741, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0740, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0740, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0739, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0739, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0730, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 49.1240, CE: 49.0418\n","  Distillation iter 10/500, Loss: 0.0762, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0743, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0736, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0717, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 43.8302, CE: 43.7477\n","  Distillation iter 10/500, Loss: 0.0763, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0745, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0733, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0730, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0727, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0723, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0718, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 20/20 saved\n","\n","Running comparative analysis for class 1...\n","\n","\n","=== Processing target class 2 ===\n","Images for class 2 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/bench/class_2\n","\n","Performing inversion attack on Client Model 1 for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0226, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0084, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0220, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0085, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0227, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0226, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0069, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0223, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0218, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0107, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0125, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0059, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0059, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0115, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0059, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0111, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0227, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0103, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0107, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0123, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0123, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0059, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0156, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0230, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0141, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0230, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0111, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0107, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0120, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0149, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0069, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0158, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0071, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0141, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0121, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0157, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0072, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0159, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0071, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0146, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0069, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0165, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0073, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0121, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0165, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0073, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0124, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0115, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0247, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0112, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0247, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0162, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0073, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0121, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0158, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0072, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0071, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0074, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0239, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0239, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0244, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0071, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0244, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0239, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 2\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0840, CE: 0.0014\n","  Distillation iter 10/500, Loss: 0.0664, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0515, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0404, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0331, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0286, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0260, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0245, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0235, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0229, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0225, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0217, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0211, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.3124, CE: 0.2291\n","  Distillation iter 10/500, Loss: 0.0789, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0771, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0758, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0746, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0698, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0686, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0675, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0665, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0654, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0602, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0595, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0588, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0582, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0575, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0570, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0564, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0559, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0554, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0550, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0546, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0542, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0538, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0534, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0531, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0528, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0526, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0523, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0521, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0518, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0516, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0515, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0513, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0511, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0510, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0509, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0508, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0506, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0505, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0504, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0503, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0503, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0502, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0918, CE: 0.0100\n","  Distillation iter 10/500, Loss: 0.0740, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0670, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0603, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0543, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0493, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0452, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0418, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0368, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0350, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0335, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0322, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0311, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0301, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0293, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0286, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0280, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0275, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0270, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0266, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0262, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0259, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0256, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0254, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0251, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0249, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0247, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0244, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0243, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0241, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0240, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0239, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0238, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0238, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0237, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0236, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0235, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0235, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0234, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0234, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0234, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0233, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0233, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0233, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0231, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0231, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1085, CE: 0.0266\n","  Distillation iter 10/500, Loss: 0.0763, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0683, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0577, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0547, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0521, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0498, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0477, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0459, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0443, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0428, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0415, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0403, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0383, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0374, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0367, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0359, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0353, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0347, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0341, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0336, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0332, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0328, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0324, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0320, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0317, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0312, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0309, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0307, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0305, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0303, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0301, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0300, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0299, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0297, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0296, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0295, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0294, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0293, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0293, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0292, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0291, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0291, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0290, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0289, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0289, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.4732, CE: 0.3905\n","  Distillation iter 10/500, Loss: 0.0786, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0771, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0760, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0750, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0740, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0731, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0702, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0693, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0684, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0675, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0667, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0659, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0602, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0597, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0592, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0588, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0583, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0580, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0573, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0569, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0566, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0564, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0561, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0559, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0556, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0554, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0553, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0551, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0549, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0548, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0546, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0545, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0544, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0543, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0542, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0541, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0540, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0539, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0539, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.7876, CE: 0.7047\n","  Distillation iter 10/500, Loss: 0.0786, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0772, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0761, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0752, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0744, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0735, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0717, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0709, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0700, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0692, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0684, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0676, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0669, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0662, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0655, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0602, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0598, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0594, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0590, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0587, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0584, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0581, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0579, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0574, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0572, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0570, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0568, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0566, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0565, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0563, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0562, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0561, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0560, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0559, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0558, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0557, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0556, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0555, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0555, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1114, CE: 0.0284\n","  Distillation iter 10/500, Loss: 0.0772, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0732, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0694, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0657, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0589, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0560, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0534, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0511, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0491, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0472, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0456, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0441, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0428, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0416, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0405, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0395, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0379, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0371, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0365, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0359, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0353, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0348, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0343, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0339, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0335, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0332, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0329, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0326, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0323, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0321, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0318, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0316, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0313, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0311, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0310, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0308, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0307, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0306, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0305, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0304, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0304, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0303, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0302, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0301, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0301, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0300, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0300, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1529, CE: 0.0705\n","  Distillation iter 10/500, Loss: 0.0775, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0750, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0728, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0706, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0685, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0664, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0643, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0590, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0574, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0560, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0546, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0533, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0522, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0511, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0500, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0491, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0482, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0474, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0466, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0459, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0453, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0447, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0441, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0436, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0431, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0426, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0422, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0418, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0415, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0412, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0409, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0406, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0403, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0401, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0399, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0397, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0395, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0392, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0389, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0388, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0386, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0385, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0384, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0383, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0383, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.6454, CE: 0.5621\n","  Distillation iter 10/500, Loss: 0.0790, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0775, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0764, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0756, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0747, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0738, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0729, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0720, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0703, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0695, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0687, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0679, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0672, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0664, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0657, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0651, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0600, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0596, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0592, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0589, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0586, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0583, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0580, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0578, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0574, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0572, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0570, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0568, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0567, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0565, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0564, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0563, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0562, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0561, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0560, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0559, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0558, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0557, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0556, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1816, CE: 0.0984\n","  Distillation iter 10/500, Loss: 0.0783, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0760, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0740, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0721, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0702, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0683, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0666, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0648, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0602, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0589, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0564, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0553, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0543, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0533, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0524, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0516, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0508, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0500, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0493, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0487, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0481, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0475, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0470, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0465, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0461, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0456, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0453, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0449, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0446, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0443, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0440, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0437, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0435, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0433, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0431, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0429, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0427, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0426, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0424, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0423, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0422, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0421, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0420, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0419, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0418, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0417, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0416, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0844, CE: 0.0007\n","  Distillation iter 10/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0460, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0347, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0282, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0249, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0233, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0226, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0219, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0217, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0214, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1185, CE: 0.0352\n","  Distillation iter 10/500, Loss: 0.0776, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0739, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0704, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0670, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0580, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0555, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0532, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0512, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0493, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0477, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0462, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0449, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0436, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0425, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0415, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0406, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0390, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0383, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0377, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0371, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0365, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0360, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0356, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0352, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0348, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0345, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0341, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0338, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0336, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0333, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0331, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0329, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0327, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0326, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0324, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0323, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0321, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0320, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0319, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0318, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0317, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0317, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0316, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0315, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0313, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 1.4512, CE: 1.3683\n","  Distillation iter 10/500, Loss: 0.0789, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0775, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0765, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0758, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0750, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0742, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0726, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0718, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0703, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0696, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0688, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0681, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0675, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0668, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0662, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0656, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0603, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0600, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0598, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0595, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0593, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0590, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0588, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0586, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0585, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0583, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0582, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0580, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0579, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0578, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0577, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0575, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0574, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0573, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0572, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0572, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1307, CE: 0.0475\n","  Distillation iter 10/500, Loss: 0.0780, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0749, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0691, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0663, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0589, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0569, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0550, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0533, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0517, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0502, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0489, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0477, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0466, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0456, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0446, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0438, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0430, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0422, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0416, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0409, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0404, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0389, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0385, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0381, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0377, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0374, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0371, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0369, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0366, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0364, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0362, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0360, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0358, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0356, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0355, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0354, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0353, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0351, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0350, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0349, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0349, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0348, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0347, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0346, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0346, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1585, CE: 0.0756\n","  Distillation iter 10/500, Loss: 0.0781, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0756, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0734, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0712, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0690, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0669, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0595, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0580, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0565, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0552, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0539, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0528, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0517, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0507, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0497, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0489, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0481, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0473, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0466, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0460, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0454, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0448, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0443, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0438, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0434, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0429, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0426, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0422, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0419, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0416, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0413, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0411, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0408, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0406, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0404, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0402, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0401, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0399, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0397, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0395, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0394, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0392, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0390, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.2376, CE: 0.1547\n","  Distillation iter 10/500, Loss: 0.0786, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0767, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0752, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0722, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0708, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0694, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0680, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0667, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0654, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0599, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0589, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0581, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0572, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0564, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0557, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0550, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0544, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0537, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0532, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0526, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0521, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0516, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0512, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0508, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0504, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0501, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0497, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0494, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0491, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0489, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0486, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0484, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0482, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0480, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0479, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0477, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0476, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0474, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0473, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0472, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0471, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0470, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0469, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0468, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0467, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0870, CE: 0.0055\n","  Distillation iter 10/500, Loss: 0.0725, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0554, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0486, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0432, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0360, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0336, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0317, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0302, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0290, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0280, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0265, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0259, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0255, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0250, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0247, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0244, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0241, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0239, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0236, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0235, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0233, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0230, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0229, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0227, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0226, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0226, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0225, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0223, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0223, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0223, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0220, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0832, CE: 0.0012\n","  Distillation iter 10/500, Loss: 0.0652, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0498, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0386, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0249, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0236, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0223, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0217, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0209, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1394, CE: 0.0571\n","  Distillation iter 10/500, Loss: 0.0772, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0744, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0719, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0694, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0670, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0586, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0568, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0552, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0537, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0524, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0511, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0499, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0488, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0478, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0469, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0461, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0453, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0445, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0438, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0432, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0426, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0421, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0416, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0411, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0407, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0403, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0399, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0390, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0385, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0383, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0381, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0379, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0377, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0376, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0374, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0373, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0372, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0371, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0370, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0369, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0368, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0367, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0366, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0366, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.2811, CE: 0.1981\n","  Distillation iter 10/500, Loss: 0.0783, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0764, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0750, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0737, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0724, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0711, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0698, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0685, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0673, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0661, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0650, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0601, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0593, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0585, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0578, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0571, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0564, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0558, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0552, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0546, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0541, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0536, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0531, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0527, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0523, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0519, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0516, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0513, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0510, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0507, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0504, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0502, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0500, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0498, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0496, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0494, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0493, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0491, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0490, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0489, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0488, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0487, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0486, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0485, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0484, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0483, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 20/20 saved\n","\n","Running comparative analysis for class 2...\n","\n","\n","=== Processing target class 3 ===\n","Images for class 3 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/bench/class_3\n","\n","Performing inversion attack on Client Model 1 for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0275, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0099, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0271, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0097, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0269, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0091, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0276, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0276, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0103, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0274, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0274, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0104, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0276, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0099, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0278, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0269, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0092, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0276, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0104, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0267, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0090, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0274, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0273, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0270, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0096, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0270, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0269, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0270, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0107, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0269, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0093, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0272, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0104, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0227, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0147, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0225, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0110, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0225, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0096, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0227, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0109, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0226, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0112, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0097, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0227, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0230, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0228, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0126, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0143, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0100, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0102, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0226, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0099, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0096, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0115, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0132, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0069, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0115, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0122, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0118, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0116, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0118, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0067, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0117, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0068, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0114, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0261, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0277, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0089, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0263, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0263, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0085, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0083, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0257, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0076, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0262, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0260, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0259, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0077, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0262, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0087, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 3\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 153.0087, CE: 152.9279\n","  Distillation iter 10/500, Loss: 12.2815, CE: 12.2100\n","  Distillation iter 20/500, Loss: 0.0657, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0615, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 171.9602, CE: 171.8791\n","  Distillation iter 10/500, Loss: 24.3851, CE: 24.3130\n","  Distillation iter 20/500, Loss: 0.0667, CE: 0.0006\n","  Distillation iter 30/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0618, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 167.5378, CE: 167.4545\n","  Distillation iter 10/500, Loss: 25.5426, CE: 25.4685\n","  Distillation iter 20/500, Loss: 0.0704, CE: 0.0024\n","  Distillation iter 30/500, Loss: 0.0654, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0645, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0639, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0633, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 157.5612, CE: 157.4813\n","  Distillation iter 10/500, Loss: 25.6745, CE: 25.6032\n","  Distillation iter 20/500, Loss: 0.3990, CE: 0.3338\n","  Distillation iter 30/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0604, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 149.0559, CE: 148.9750\n","  Distillation iter 10/500, Loss: 29.5938, CE: 29.5221\n","  Distillation iter 20/500, Loss: 0.0938, CE: 0.0286\n","  Distillation iter 30/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0605, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0604, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0604, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 160.8680, CE: 160.7860\n","  Distillation iter 10/500, Loss: 29.9134, CE: 29.8405\n","  Distillation iter 20/500, Loss: 0.0667, CE: 0.0001\n","  Distillation iter 30/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0622, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 159.7239, CE: 159.6418\n","  Distillation iter 10/500, Loss: 27.4306, CE: 27.3578\n","  Distillation iter 20/500, Loss: 0.2917, CE: 0.2252\n","  Distillation iter 30/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0616, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 166.8595, CE: 166.7771\n","  Distillation iter 10/500, Loss: 24.0988, CE: 24.0253\n","  Distillation iter 20/500, Loss: 0.4711, CE: 0.4036\n","  Distillation iter 30/500, Loss: 0.0647, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0637, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0626, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 163.1434, CE: 163.0611\n","  Distillation iter 10/500, Loss: 31.9151, CE: 31.8417\n","  Distillation iter 20/500, Loss: 2.6777, CE: 2.6106\n","  Distillation iter 30/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0618, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 163.1869, CE: 163.1045\n","  Distillation iter 10/500, Loss: 15.9053, CE: 15.8318\n","  Distillation iter 20/500, Loss: 0.0749, CE: 0.0073\n","  Distillation iter 30/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0635, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0628, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 160.1427, CE: 160.0622\n","  Distillation iter 10/500, Loss: 21.8607, CE: 21.7890\n","  Distillation iter 20/500, Loss: 0.0656, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0612, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 162.1836, CE: 162.1020\n","  Distillation iter 10/500, Loss: 25.1806, CE: 25.1078\n","  Distillation iter 20/500, Loss: 0.0667, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0641, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0621, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 159.6280, CE: 159.5448\n","  Distillation iter 10/500, Loss: 31.9423, CE: 31.8686\n","  Distillation iter 20/500, Loss: 0.2149, CE: 0.1479\n","  Distillation iter 30/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0620, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 168.2311, CE: 168.1503\n","  Distillation iter 10/500, Loss: 22.8242, CE: 22.7519\n","  Distillation iter 20/500, Loss: 0.0770, CE: 0.0105\n","  Distillation iter 30/500, Loss: 0.0640, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0621, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 169.5599, CE: 169.4767\n","  Distillation iter 10/500, Loss: 28.5316, CE: 28.4574\n","  Distillation iter 20/500, Loss: 1.8385, CE: 1.7706\n","  Distillation iter 30/500, Loss: 0.0649, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0638, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0627, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 180.0800, CE: 179.9976\n","  Distillation iter 10/500, Loss: 37.3736, CE: 37.2998\n","  Distillation iter 20/500, Loss: 2.8160, CE: 2.7487\n","  Distillation iter 30/500, Loss: 0.0642, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0618, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 165.1919, CE: 165.1085\n","  Distillation iter 10/500, Loss: 31.8114, CE: 31.7372\n","  Distillation iter 20/500, Loss: 1.9774, CE: 1.9099\n","  Distillation iter 30/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0633, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0623, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0622, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0621, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0621, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 163.2634, CE: 163.1816\n","  Distillation iter 10/500, Loss: 15.6492, CE: 15.5765\n","  Distillation iter 20/500, Loss: 0.0686, CE: 0.0017\n","  Distillation iter 30/500, Loss: 0.0644, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0636, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0632, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0631, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0630, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0629, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0627, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0626, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0625, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0624, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 162.1754, CE: 162.0938\n","  Distillation iter 10/500, Loss: 29.1196, CE: 29.0471\n","  Distillation iter 20/500, Loss: 0.1180, CE: 0.0518\n","  Distillation iter 30/500, Loss: 0.0634, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0624, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0620, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0619, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0618, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0616, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0615, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0614, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0613, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 159.8920, CE: 159.8108\n","  Distillation iter 10/500, Loss: 27.5757, CE: 27.5037\n","  Distillation iter 20/500, Loss: 1.2276, CE: 1.1620\n","  Distillation iter 30/500, Loss: 0.0628, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0617, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0613, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0612, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0611, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0611, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0611, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0610, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0609, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0608, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0607, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0606, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0606, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 20/20 saved\n","\n","Running comparative analysis for class 3...\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"environment":{"kernel":"pytorch_fl","name":"workbench-notebooks.m129","type":"gcloud","uri":"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"},"kernelspec":{"display_name":"Pytorch_fl (Local)","language":"python","name":"pytorch_fl"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}