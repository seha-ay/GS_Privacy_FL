{"cells":[{"cell_type":"markdown","source":["##README\n","\n","## Caution!\n","This notebook will take significantly long time to complete and generate inverted images **(20-30 hours with GPU)**!\n","\n","You are welcome to run this if you are okay for the execution times. We have used following system for this notebook:\n","\n","- Google Console Platform - Vertex AI Jupyter Notebook\n","  - Machine type: n1-standard-16 (16 vCPUs, 60 GB RAM)\n","  - GPU: NVIDIA Tesla P100 x 1\n","  - Environment: TensorFlow Enterprise 2.16 (IntelÂ® MKL-DNN/MKL)\n","  - CUDA Version 12\n","\n","### Also, please make sure you have adjust your dependencies as in the ```pytorch_fl_env.yaml``` file.\n","\n","\n","## Alternative\n","We have already run and obtain inverted image samples. This can be found at the ```inverted_images``` folder under ```/main/model_inversion/blakcbox/inverted_images``` path in the github page."],"metadata":{"id":"uomAzQF-ntdH"},"id":"uomAzQF-ntdH"},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"VKDSWQtCns-1"},"id":"VKDSWQtCns-1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"yqynSllunsgc"},"id":"yqynSllunsgc"},{"cell_type":"code","execution_count":null,"id":"7cf26470-91bb-44a2-8e8e-c87048e1cfe4","metadata":{"tags":[],"id":"7cf26470-91bb-44a2-8e8e-c87048e1cfe4"},"outputs":[],"source":["import os\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.utils as vutils\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n","import torch.nn.functional as F\n","from functools import partial\n","import traceback\n","from PIL import Image\n","\n","class BrainMRIClassifier(nn.Module):\n","    \"\"\"CNN classifier for brain MRI images with 4 output classes\"\"\"\n","    def __init__(self):\n","        super(BrainMRIClassifier, self).__init__()\n","        self.features = nn.Sequential(\n","            # First block\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(32),\n","            # Second block\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(64),\n","            # Third block\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(128),\n","            # Fourth block\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256),\n","            # Fifth block\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.BatchNorm2d(256),\n","        )\n","        # With input size 299x299 and 5 pooling layers, spatial dimensions ~9x9\n","        self.flat_features = 256 * 9 * 9\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Dropout(0.5),\n","            nn.Linear(self.flat_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, 4)  # 4 classes\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","    def get_feature_maps(self, x):\n","        # Helper method to extract intermediate feature maps for inversion guidance\n","        feature_maps = []\n","\n","        # Extract feature maps from each block\n","        x1 = self.features[0:4](x)  # First block\n","        feature_maps.append(x1)\n","\n","        x2 = self.features[4:8](x1)  # Second block\n","        feature_maps.append(x2)\n","\n","        x3 = self.features[8:12](x2)  # Third block\n","        feature_maps.append(x3)\n","\n","        x4 = self.features[12:16](x3)  # Fourth block\n","        feature_maps.append(x4)\n","\n","        x5 = self.features[16:20](x4)  # Fifth block\n","        feature_maps.append(x5)\n","\n","        return feature_maps\n","\n","\n","\n","\n","# -------------------------------------------------------------\n","# 1. Model Loading and Feature Extraction\n","# -------------------------------------------------------------\n","def load_model(model_path, model_class=None):\n","    \"\"\"\n","    Loads a pre-trained model from the given file path.\n","    If the loaded object is a state dictionary, a model_class must be provided\n","    to instantiate the model and load the state.\n","    \"\"\"\n","    checkpoint = torch.load(model_path)\n","    # Check if checkpoint is a state dict (OrderedDict) and not a full model\n","    if isinstance(checkpoint, dict) and not hasattr(checkpoint, 'eval'):\n","        if model_class is None:\n","            raise ValueError(\"The checkpoint is a state_dict but no model_class was provided. \"\n","                             \"Please supply the model_class argument.\")\n","        model = model_class()  # Instantiate your model architecture\n","        model.load_state_dict(checkpoint)\n","    else:\n","        model = checkpoint\n","    model.eval()  # set to evaluation mode\n","    return model\n","\n","\n","def extract_model_layers(model):\n","    \"\"\"\n","    Analyzes a model and extracts information about its layers\n","    to help determine optimal feature extraction points.\n","    Enhanced to handle EnsembleModel.\n","    \"\"\"\n","    layers_info = []\n","\n","    # Special handling for EnsembleModel\n","    if isinstance(model, EnsembleModel):\n","        # Try to extract layers from the first wrapped model\n","        if model.models and len(model.models) > 0:\n","            return extract_model_layers(model.models[0])\n","        return []  # Empty list if no models\n","\n","    # Standard layer extraction\n","    for name, module in model.named_modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            layers_info.append({\n","                'name': name,\n","                'type': type(module).__name__,\n","                'params': sum(p.numel() for p in module.parameters())\n","            })\n","    return layers_info\n","\n","def get_optimal_hook_layers(model):\n","    \"\"\"\n","    Automatically determines optimal layers for feature extraction\n","    based on model architecture. Fixed to handle custom models like EnsembleModel.\n","    \"\"\"\n","    layers_info = extract_model_layers(model)\n","\n","    # Check if we found any layers\n","    if not layers_info:\n","        # For custom models like EnsembleModel, return a default layer name\n","        # that will be handled specially in the main function\n","        return [\"default_hook\"]\n","\n","    if len(layers_info) <= 2:\n","        return [layers_info[-1]['name']]\n","\n","    # For ResNet-like models, target specific blocks\n","    all_names = [layer['name'] for layer in layers_info]\n","    candidates = []\n","\n","    # Look for common layer naming patterns\n","    for name in all_names:\n","        if any(pattern in name for pattern in ['layer2', 'layer3', 'layer4', 'block', 'features']):\n","            if 'conv' in name.lower() and 'weight' not in name:\n","                candidates.append(name)\n","\n","    # Select a diverse set of layers (early, middle, late)\n","    if len(candidates) >= 3:\n","        idx1 = len(candidates) // 4\n","        idx2 = len(candidates) // 2\n","        idx3 = (3 * len(candidates)) // 4\n","        return [candidates[idx1], candidates[idx2], candidates[idx3]]\n","    elif len(candidates) > 0:\n","        return candidates\n","    else:\n","        # Fallback to selecting first, middle and last conv layers\n","        conv_layers = [l['name'] for l in layers_info if 'Conv' in l['type']]\n","        if len(conv_layers) >= 3:\n","            return [conv_layers[0], conv_layers[len(conv_layers)//2], conv_layers[-1]]\n","        elif len(conv_layers) > 0:\n","            return [conv_layers[-1]]\n","        else:\n","            # Final fallback - use the last layer of any type\n","            return [layers_info[-1]['name']] if layers_info else [\"default_hook\"]\n","\n","\n","# -------------------------------------------------------------\n","# 2. Enhanced Regularization Functions with Self-tuning\n","# -------------------------------------------------------------\n","def total_variation_loss(img, tv_weight):\n","    \"\"\"\n","    Enhanced total variation loss with improved normalization.\n","    \"\"\"\n","    batch_size = img.size()[0]\n","    h_x = img.size()[2]\n","    w_x = img.size()[3]\n","    count_h = (h_x - 1) * w_x\n","    count_w = h_x * (w_x - 1)\n","\n","    # Use higher-order differences for better smoothness\n","    h_tv = torch.pow(img[:, :, 1:, :] - img[:, :, :h_x-1, :], 2).sum()\n","    w_tv = torch.pow(img[:, :, :, 1:] - img[:, :, :, :w_x-1], 2).sum()\n","\n","    # Add second-order differences for capturing textures better\n","    if h_x > 2:\n","        h_tv2 = torch.pow(img[:, :, 2:, :] - 2*img[:, :, 1:-1, :] + img[:, :, :-2, :], 2).sum()\n","        h_tv = h_tv + 0.5 * h_tv2\n","\n","    if w_x > 2:\n","        w_tv2 = torch.pow(img[:, :, :, 2:] - 2*img[:, :, :, 1:-1] + img[:, :, :, :-2], 2).sum()\n","        w_tv = w_tv + 0.5 * w_tv2\n","\n","    return tv_weight * (h_tv / count_h + w_tv / count_w) / batch_size\n","\n","def color_distribution_loss(img, color_weight):\n","    \"\"\"\n","    Enhanced color loss that encourages natural color distributions.\n","    Penalizes both gray images and unnatural color distributions.\n","    For grayscale images, returns zero loss.\n","    \"\"\"\n","    # If the image has less than 3 channels, bypass color loss.\n","    if img.shape[1] < 3:\n","        return torch.tensor(0.0, device=img.device)\n","\n","    # Calculate mean and std across spatial dimensions for each channel\n","    mean_rgb = torch.mean(img, dim=[2, 3])\n","    std_rgb = torch.std(img, dim=[2, 3])\n","\n","    # Split channels\n","    mr, mg, mb = torch.split(mean_rgb, 1, dim=1)\n","    sr, sg, sb = torch.split(std_rgb, 1, dim=1)\n","\n","    # 1. Channel diversity loss - penalize when channels are too similar (gray image)\n","    diversity_loss = -torch.mean(torch.abs(mr - mg) + torch.abs(mr - mb) + torch.abs(mg - mb))\n","\n","    # 2. Natural distribution loss - RGB channels typically have correlations\n","    # Encourage typical RGB relationships: G typically higher than R and B\n","    natural_mean_loss = torch.mean(torch.relu(mr - mg)) + torch.mean(torch.relu(mb - mg))\n","\n","    # 3. Natural variance loss - encourage reasonable variance in each channel\n","    target_std = torch.tensor([0.2, 0.2, 0.2], device=img.device).view(1, 3)\n","    variance_loss = F.mse_loss(std_rgb, target_std)\n","\n","    return color_weight * (diversity_loss + 0.5 * natural_mean_loss + variance_loss)\n","\n","\n","def perceptual_smoothness_loss(img, smooth_weight):\n","    \"\"\"\n","    Multi-scale perceptual smoothness that better preserves edges.\n","    Fixed to handle padding correctly.\n","    \"\"\"\n","    loss = 0.0\n","\n","    # Multiple kernel sizes capture different levels of detail\n","    for kernel_size in [3, 5, 7]:\n","        # Create Gaussian-like kernel (approximation)\n","        sigma = kernel_size / 3\n","        grid_x = torch.arange(kernel_size, device=img.device) - (kernel_size - 1) / 2\n","        grid_y = grid_x.view(-1, 1)\n","        kernel_2d = torch.exp(-(grid_x.pow(2) + grid_y.pow(2)) / (2 * sigma**2))\n","        kernel_2d = kernel_2d / kernel_2d.sum()\n","\n","        # Expand to 4D kernel\n","        channels = img.shape[1]\n","        kernel = kernel_2d.expand(channels, 1, kernel_size, kernel_size)\n","\n","        # Apply smoothing\n","        padding = (kernel_size - 1) // 2\n","        smoothed = F.conv2d(img, kernel, padding=padding, groups=channels)\n","\n","        # Compute image gradients using finite differences\n","        grad_x = torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1])\n","        grad_y = torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :])\n","\n","        # Manually handle padding instead of using F.pad with 'replicate'\n","        # For grad_x: pad the last column by repeating the last valid column\n","        last_col = grad_x[:, :, :, -1:]\n","        grad_x = torch.cat([grad_x, last_col], dim=3)\n","\n","        # For grad_y: pad the last row by repeating the last valid row\n","        last_row = grad_y[:, :, -1:, :]\n","        grad_y = torch.cat([grad_y, last_row], dim=2)\n","\n","        # Compute edge-preserving weights: lower weight near edges, higher in smooth regions\n","        edge_weights = torch.exp(-50 * (grad_x.pow(2) + grad_y.pow(2)))\n","\n","        # Calculate weighted difference\n","        weighted_diff = edge_weights * (img - smoothed).pow(2)\n","        loss += weighted_diff.mean()\n","\n","    return smooth_weight * (loss / 3)  # Average across scales\n","\n","def naturalness_prior_loss(img, natural_weight):\n","    \"\"\"\n","    Naturalness prior encouraging realistic image statistics.\n","    Based on natural image priors in the gradient domain.\n","    \"\"\"\n","    # Gradient in x and y directions\n","    grad_x = img[:, :, :, 1:] - img[:, :, :, :-1]\n","    grad_y = img[:, :, 1:, :] - img[:, :, :-1, :]\n","\n","    # Natural images follow a heavy-tailed distribution in gradient domain\n","    # We can approximate this with a combination of L1 and log penalties\n","    l1_grad = torch.mean(torch.abs(grad_x)) + torch.mean(torch.abs(grad_y))\n","\n","    # Log penalty encourages sparse but strong gradients (edges)\n","    eps = 1e-5\n","    log_grad = torch.mean(torch.log(torch.abs(grad_x) + eps)) + torch.mean(torch.log(torch.abs(grad_y) + eps))\n","\n","    return natural_weight * (l1_grad - 0.1 * log_grad)\n","\n","def compute_fft_loss(img, fft_weight):\n","    \"\"\"\n","    Spectral loss operating in the frequency domain to encourage\n","    natural frequency distributions found in real images.\n","    Fixed to handle cuFFT size requirements.\n","    \"\"\"\n","    # Convert to grayscale for frequency analysis\n","    if img.shape[1] >= 3:\n","        # RGB to grayscale\n","        gray = 0.299 * img[:, 0:1] + 0.587 * img[:, 1:2] + 0.114 * img[:, 2:3]\n","    else:\n","        # Already grayscale\n","        gray = img\n","\n","    # Ensure dimensions are compatible with cuFFT (powers of 2, 3, 5, 7)\n","    # A simple approach is to pad to the next power of 2\n","    h, w = gray.shape[2], gray.shape[3]\n","    padded_h = 2**int(np.ceil(np.log2(h)))\n","    padded_w = 2**int(np.ceil(np.log2(w)))\n","\n","    if h != padded_h or w != padded_w:\n","        # Pad to power of 2 dimensions\n","        padding_h = padded_h - h\n","        padding_w = padded_w - w\n","        pad_h1, pad_h2 = padding_h // 2, padding_h - (padding_h // 2)\n","        pad_w1, pad_w2 = padding_w // 2, padding_w - (padding_w // 2)\n","\n","        # Use zero padding\n","        gray = F.pad(gray, (pad_w1, pad_w2, pad_h1, pad_h2), mode='constant', value=0)\n","\n","    try:\n","        # Compute 2D FFT\n","        fft = torch.fft.fft2(gray)\n","        fft_mag = torch.abs(fft)\n","\n","        # Shift to center low frequencies\n","        fft_mag = torch.fft.fftshift(fft_mag)\n","\n","        # Create a reference power spectrum that follows 1/f distribution\n","        h, w = fft_mag.shape[-2:]\n","        cy, cx = h // 2, w // 2\n","        y_grid, x_grid = torch.meshgrid(torch.arange(h, device=img.device),\n","                                        torch.arange(w, device=img.device),\n","                                        indexing='ij')\n","        y_grid = y_grid - cy\n","        x_grid = x_grid - cx\n","        dist = torch.sqrt(x_grid.pow(2) + y_grid.pow(2)) + 1e-5\n","        target_spectrum = 1 / dist\n","\n","        # Normalize target and actual spectrum\n","        target_spectrum = target_spectrum / target_spectrum.sum()\n","        actual_spectrum = fft_mag / (fft_mag.sum() + 1e-8)\n","\n","        # Compute KL divergence as a measure of distribution difference\n","        eps = 1e-8\n","        kl_div = target_spectrum * torch.log((target_spectrum + eps) / (actual_spectrum + eps))\n","\n","        return fft_weight * kl_div.sum()\n","\n","    except RuntimeError:\n","        # Fallback if FFT still fails: return a small constant loss\n","        print(\"Warning: FFT computation failed, using fallback loss\")\n","        return fft_weight * torch.tensor(0.1, device=img.device)\n","\n","# -------------------------------------------------------------\n","# 3. Advanced Initialization Strategies\n","# -------------------------------------------------------------\n","def get_initial_image(strategy='mixed', size=(1, 3, 224, 224), device='cpu', target_class=None, channels=None):\n","    \"\"\"\n","    Enhanced initialization strategies for faster convergence.\n","    Accepts a 'channels' parameter. If not provided, it defaults to size[1].\n","    \"\"\"\n","    if channels is None:\n","        channels = size[1]\n","\n","    if strategy == 'mean':\n","        if channels == 1:\n","            mean = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","        else:\n","            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n","        img = mean.repeat(size[0], 1, size[2], size[3])\n","    elif strategy == 'gaussian':\n","        if channels == 1:\n","            mean = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","            std = torch.tensor([0.25]).view(1, 1, 1, 1).to(device)\n","        else:\n","            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n","            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n","        img = torch.randn(size, device=device) * std + mean\n","    elif strategy == 'pca':\n","        img = torch.zeros(size, device=device)\n","        for c in range(channels):\n","            freq_representation = torch.zeros((size[2], size[3]), dtype=torch.complex64, device=device)\n","            r_max = min(size[2], size[3]) // 8\n","            cy, cx = size[2] // 2, size[3] // 2\n","            for ky in range(size[2]):\n","                for kx in range(size[3]):\n","                    y_rel = (ky - cy) / r_max\n","                    x_rel = (kx - cx) / r_max\n","                    dist = torch.sqrt(y_rel**2 + x_rel**2)\n","                    if dist < 1.0:\n","                        phase = torch.rand(1, device=device) * 2 * np.pi\n","                        amplitude = torch.exp(-3.0 * dist)\n","                        freq_representation[ky, kx] = amplitude * torch.exp(1j * phase)\n","            channel_data = torch.fft.ifft2(freq_representation).real\n","            channel_data = (channel_data - channel_data.min()) / (channel_data.max() - channel_data.min() + 1e-8)\n","            img[0, c] = channel_data\n","    elif strategy == 'mixed':\n","        if channels == 1:\n","            mean = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","        else:\n","            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n","        base = mean.repeat(size[0], 1, size[2], size[3])\n","        freq_img = torch.zeros(size, device=device)\n","        for octave in range(3):\n","            scale_factor = 2 ** octave\n","            noise_size = (size[2] // scale_factor, size[3] // scale_factor)\n","            noise = torch.randn((1, channels, *noise_size), device=device)\n","            noise = F.interpolate(noise, size=(size[2], size[3]), mode='bilinear', align_corners=False)\n","            freq_img += noise * (0.5 ** octave)\n","        freq_img = (freq_img - freq_img.min()) / (freq_img.max() - freq_img.min() + 1e-8) * 0.2\n","        img = base + freq_img\n","    elif strategy == 'class_prior' and target_class is not None:\n","        hue_shift = (target_class % 10) / 10.0\n","        if channels == 1:\n","            base_color = torch.tensor([0.5]).view(1, 1, 1, 1).to(device)\n","        else:\n","            r = 0.5 + 0.4 * torch.cos(2 * np.pi * hue_shift)\n","            g = 0.5 + 0.4 * torch.cos(2 * np.pi * (hue_shift + 1/3))\n","            b = 0.5 + 0.4 * torch.cos(2 * np.pi * (hue_shift + 2/3))\n","            base_color = torch.tensor([r, g, b]).view(1, 3, 1, 1).to(device)\n","        img = base_color.repeat(size[0], 1, size[2], size[3])\n","        noise = torch.randn((1, channels, size[2]//4, size[3]//4), device=device)\n","        noise = F.interpolate(noise, size=(size[2], size[3]), mode='bilinear', align_corners=False)\n","        noise = (noise - noise.min()) / (noise.max() - noise.min() + 1e-8) * 0.15\n","        img = img + noise\n","    else:\n","        img = torch.randn(size, device=device)\n","\n","    img = torch.clamp(img, 0, 1)\n","    img.requires_grad = True\n","    return img\n","\n","# -------------------------------------------------------------\n","# 4. Improved Feature Matching with Adaptive Statistics\n","# -------------------------------------------------------------\n","class FeatureHook:\n","    \"\"\"\n","    Advanced feature hook with statistics tracking capabilities.\n","    \"\"\"\n","    def __init__(self, layer_name, adaptive_stats=True):\n","        self.layer_name = layer_name\n","        self.features = None\n","        self.adaptive_stats = adaptive_stats\n","        self.running_mean = None\n","        self.running_std = None\n","        self.momentum = 0.9\n","\n","    def hook_fn(self, module, input, output):\n","        self.features = output\n","\n","        # Update running statistics for adaptive matching\n","        if self.adaptive_stats:\n","            current_mean = output.mean(dim=[0, 2, 3]).detach()\n","            current_std = output.std(dim=[0, 2, 3]).detach()\n","\n","            if self.running_mean is None:\n","                self.running_mean = current_mean\n","                self.running_std = current_std\n","            else:\n","                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * current_mean\n","                self.running_std = self.momentum * self.running_std + (1 - self.momentum) * current_std\n","\n","    def get_target_stats(self):\n","        \"\"\"\n","        Returns target statistics for feature matching.\n","        Adapts to the evolving feature distribution during optimization.\n","        \"\"\"\n","        if self.adaptive_stats and self.running_mean is not None:\n","            return {\n","                'mean': self.running_mean,\n","                'std': self.running_std\n","            }\n","        else:\n","            # Fallback to reasonable defaults\n","            if self.features is not None:\n","                num_channels = self.features.shape[1]\n","                device = self.features.device\n","                return {\n","                    'mean': torch.zeros(num_channels, device=device),\n","                    'std': torch.ones(num_channels, device=device)\n","                }\n","            return None\n","\n","    def compute_feature_loss(self, target_stats=None):\n","        \"\"\"\n","        Compute feature distribution matching loss.\n","        \"\"\"\n","        if self.features is None:\n","            return torch.tensor(0.0, device='cpu')\n","\n","        if target_stats is None:\n","            target_stats = self.get_target_stats()\n","            if target_stats is None:\n","                return torch.tensor(0.0, device=self.features.device)\n","\n","        current_mean = self.features.mean(dim=[0, 2, 3])\n","        current_std = self.features.std(dim=[0, 2, 3])\n","\n","        # Mean and std matching with additional correlation structure\n","        mean_loss = F.mse_loss(current_mean, target_stats['mean'])\n","        std_loss = F.mse_loss(current_std, target_stats['std'])\n","\n","        # Optionally add correlation structure matching\n","        # (omitted for simplicity but could be added here)\n","\n","        return mean_loss + std_loss\n","\n","# -------------------------------------------------------------\n","# 5. Advanced Multi-scale and Multi-resolution Optimization\n","# -------------------------------------------------------------\n","def progressive_model_inversion_attack(\n","        model,\n","        target_class,\n","        scales=None,\n","        iterations_per_scale=500,\n","        lr_initial=0.1,\n","        lr_final=0.001,\n","        auto_schedule_hyperparams=True,\n","        regularization_weights={\n","            'tv': 5e-4,\n","            'l2': 1e-4,\n","            'color': 5e-5,\n","            'smooth': 1e-4,\n","            'natural': 2e-4,\n","            'fft': 1e-5,\n","            'feature': 5e-3\n","        },\n","        init_strategy='mixed',\n","        hook_layers=None,\n","        verbose=False,\n","        log_interval=50,\n","        init_img=None):\n","\n","    import sys  # for dynamic printing\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    # Get model information\n","    model_info = get_model_info(model)\n","    input_channels = model_info[\"input_channels\"] or 1\n","\n","    # For BrainMRIClassifier, we need input size 299x299\n","    if scales is None:\n","        scales = [(1, input_channels, 75, 75),\n","                  (1, input_channels, 150, 150),\n","                  (1, input_channels, 299, 299)]\n","\n","    # Force final scale to be 299x299\n","    if scales[-1][2:] != (299, 299):\n","        scales[-1] = (1, input_channels, 299, 299)\n","\n","    print(f\"Using scales: {scales} with {input_channels} input channels\")\n","\n","    # Get optimal hook layers if not specified\n","    if hook_layers is None:\n","        hook_layers = get_optimal_hook_layers(model)\n","\n","    # Initialize feature hooks\n","    hooks = []\n","    feature_extractors = {}\n","\n","    # Special handling for ensemble model\n","    if \"default_hook\" in hook_layers:\n","        print(\"Using default hooks for custom model\")\n","    else:\n","        for layer_name in hook_layers:\n","            for name, module in model.named_modules():\n","                if layer_name in name:\n","                    extractor = FeatureHook(layer_name)\n","                    feature_extractors[layer_name] = extractor\n","                    hooks.append(module.register_forward_hook(extractor.hook_fn))\n","                    break\n","\n","    # Initialize with the coarsest scale or provided init_img\n","    if init_img is None:\n","        current_img = get_initial_image(init_strategy, size=scales[0], device=device,\n","                                       target_class=target_class, channels=input_channels)\n","    else:\n","        current_img = init_img.to(device)\n","\n","    current_img.requires_grad = True\n","\n","    # Training losses for visualization\n","    losses_history = {\n","        'total': [], 'ce': [], 'tv': [], 'l2': [], 'color': [],\n","        'smooth': [], 'natural': [], 'fft': [], 'feature': []\n","    }\n","\n","    # Cross-entropy loss function\n","    ce_loss_fn = nn.CrossEntropyLoss()\n","    target = torch.tensor([target_class], device=device)\n","\n","    # Track best image and score\n","    best_img = current_img.clone().detach()\n","    best_score = float('inf')\n","\n","    # Process each scale\n","    for scale_idx, scale in enumerate(scales):\n","        # Print phase information on its own line.\n","        print(f\"\\nOptimizing at scale {scale[2]}x{scale[3]}\")\n","\n","        # Resize image if needed\n","        if current_img.shape != scale:\n","            with torch.no_grad():\n","                resized_img = F.interpolate(current_img.detach(), size=scale[2:],\n","                                           mode='bilinear', align_corners=False)\n","            current_img = resized_img.clone()\n","            current_img.requires_grad = True\n","\n","        # Determine iterations for this scale\n","        scale_factor = scale[2] / scales[0][2]\n","        iterations = int(iterations_per_scale * min(scale_factor, 2.0))\n","\n","        # Initialize optimizer and scheduler\n","        lr = lr_initial * (lr_final / lr_initial) ** (scale_idx / len(scales))\n","        optimizer = optim.AdamW([current_img], lr=lr, weight_decay=0.01)\n","        scheduler = OneCycleLR(optimizer, max_lr=lr, total_steps=iterations,\n","                             pct_start=0.3, anneal_strategy='cos')\n","\n","        # Define scheduler for hyperparameters\n","        if auto_schedule_hyperparams:\n","            def schedule_weight(base_weight, iteration, iterations, warmup=0.2):\n","                if iteration < iterations * warmup:\n","                    factor = iteration / (iterations * warmup)\n","                else:\n","                    factor = 1.0\n","                return base_weight * factor\n","        else:\n","            schedule_weight = lambda w, i, t, **kwargs: w\n","\n","        # Optimization loop for this scale\n","        for iteration in range(iterations):\n","            optimizer.zero_grad()\n","\n","            # Forward pass - ALWAYS resize to 299x299 for model output\n","            resized_img = F.interpolate(current_img, size=(299, 299),\n","                                       mode='bilinear', align_corners=False)\n","            output = model(resized_img)\n","\n","            # Calculate losses\n","            ce_loss = ce_loss_fn(output, target)\n","            tv_loss = total_variation_loss(current_img,\n","                                         schedule_weight(regularization_weights['tv'], iteration, iterations))\n","            l2_loss = schedule_weight(regularization_weights['l2'], iteration, iterations) * torch.norm(current_img, 2)\n","            color_loss = color_distribution_loss(current_img,\n","                                               schedule_weight(regularization_weights['color'], iteration, iterations))\n","            smooth_loss = perceptual_smoothness_loss(current_img,\n","                                                   schedule_weight(regularization_weights['smooth'], iteration, iterations))\n","            natural_loss = naturalness_prior_loss(current_img,\n","                                                schedule_weight(regularization_weights['natural'], iteration, iterations, warmup=0.4))\n","            fft_loss = compute_fft_loss(current_img,\n","                                       schedule_weight(regularization_weights['fft'], iteration, iterations, warmup=0.5))\n","\n","            # Feature matching losses\n","            feature_loss = 0.0\n","            if feature_extractors:\n","                feature_w = schedule_weight(regularization_weights['feature'] * scale_factor,\n","                                         iteration, iterations, warmup=0.3)\n","                for layer_name, extractor in feature_extractors.items():\n","                    feature_loss += extractor.compute_feature_loss() * feature_w\n","\n","            # Combine all losses\n","            total_loss = ce_loss + tv_loss + l2_loss + color_loss + smooth_loss + natural_loss + fft_loss + feature_loss\n","\n","            # Record losses occasionally for visualization\n","            if iteration % log_interval == 0 or iteration == iterations - 1:\n","                losses_history['total'].append(total_loss.item())\n","                losses_history['ce'].append(ce_loss.item())\n","                losses_history['tv'].append(tv_loss.item())\n","                losses_history['l2'].append(l2_loss.item())\n","                losses_history['color'].append(color_loss.item())\n","                losses_history['smooth'].append(smooth_loss.item())\n","                losses_history['natural'].append(natural_loss.item())\n","                losses_history['fft'].append(fft_loss.item())\n","                losses_history['feature'].append(feature_loss.item())\n","\n","            # Backward pass and update\n","            total_loss.backward()\n","            torch.nn.utils.clip_grad_norm_([current_img], max_norm=1.0)\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Ensure image remains in valid range\n","            current_img.data = torch.clamp(current_img.data, 0, 1)\n","\n","            # Save best result\n","            score = ce_loss.item() + 0.1 * (tv_loss.item() + smooth_loss.item() + natural_loss.item())\n","            if score < best_score:\n","                best_score = score\n","                best_img = current_img.clone().detach()\n","\n","            # Dynamic update of progress on the same line (if verbose)\n","            if verbose and (iteration % log_interval == 0 or iteration == iterations - 1):\n","                sys.stdout.write(f\"\\rScale {scale[2]}x{scale[3]} Iter {iteration}/{iterations} - Loss: {total_loss.item():.4f}, CE: {ce_loss.item():.4f}\")\n","                sys.stdout.flush()\n","        # Move to next line after finishing iterations for this scale\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","    # Clean up hooks\n","    for hook in hooks:\n","        hook.remove()\n","\n","    # Apply post-processing to the best image\n","    try:\n","        final_img = advanced_post_process(best_img)\n","        print(\"Post-processing completed successfully\")\n","    except Exception as e:\n","        print(f\"Error in post-processing: {str(e)}\")\n","        final_img = best_img  # Use best image without post-processing if there's an error\n","\n","    return final_img\n","\n","\n","def plot_losses(losses_history, filename):\n","    \"\"\"\n","    Create a detailed loss curve plot.\n","    \"\"\"\n","    plt.figure(figsize=(12, 8))\n","    # Main plot with overall loss\n","    plt.subplot(2, 1, 1)\n","    plt.plot(losses_history['total'], 'k-', label='Total Loss')\n","    plt.plot(losses_history['ce'], 'r-', label='CE Loss')\n","    plt.title('Overall Loss Progress')\n","    plt.yscale('log')\n","    plt.legend()\n","\n","    # Subplot with individual regularization terms\n","    plt.subplot(2, 1, 2)\n","    for key in ['tv', 'l2', 'color', 'smooth', 'natural', 'fft', 'feature']:\n","        if losses_history[key]:\n","            plt.plot(losses_history[key], label=f'{key} Loss')\n","    plt.title('Regularization Losses')\n","    plt.yscale('log')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(filename)\n","    plt.close()\n","\n","# -------------------------------------------------------------\n","# 6. Advanced Post-Processing Pipeline\n","# -------------------------------------------------------------\n","def advanced_post_process(img):\n","    \"\"\"\n","    Advanced post-processing pipeline for improved visual quality.\n","    Fixed to handle both grayscale and RGB images.\n","    \"\"\"\n","    # Convert tensor to numpy, handling both color and grayscale images\n","    img_cpu = img.squeeze().cpu()\n","\n","    # Check if grayscale (1 channel) or color (3 channels)\n","    if len(img_cpu.shape) == 2:\n","        # Grayscale image (H, W)\n","        img_np = img_cpu.numpy()\n","        is_grayscale = True\n","    elif len(img_cpu.shape) == 3 and img_cpu.shape[0] == 1:\n","        # Single channel image in format (1, H, W)\n","        img_np = img_cpu.numpy()[0]\n","        is_grayscale = True\n","    else:\n","        # Color image (C, H, W) -> (H, W, C)\n","        img_np = img_cpu.numpy().transpose(1, 2, 0)\n","        is_grayscale = False\n","\n","    # 1. Contrast stretching with percentile-based normalization\n","    p2, p98 = np.percentile(img_np, (2, 98))\n","    img_np = np.clip((img_np - p2) / (p98 - p2 + 1e-8), 0, 1)\n","\n","    # For grayscale images, skip color-specific enhancements\n","    if not is_grayscale:\n","        # 2. Local contrast enhancement (CLAHE-like)\n","        img_np = local_contrast_enhance(img_np)\n","\n","        # 3. Histogram equalization with color preservation\n","        img_np = histogram_equalization_with_color(img_np)\n","\n","        # 5. Color balancing\n","        img_np = color_balance(img_np)\n","    else:\n","        # Apply grayscale-specific enhancements\n","        hist, bins = np.histogram(img_np.flatten(), 256, [0, 1])\n","        cdf = hist.cumsum()\n","        cdf = cdf / (cdf[-1] + 1e-8)  # Normalize\n","        img_np = np.interp(img_np.flatten(), bins[:-1], cdf).reshape(img_np.shape)\n","\n","    # 4. Detail enhancement with edge preservation (works for both color and grayscale)\n","    if is_grayscale:\n","        # For grayscale\n","        img_np = edge_preserving_sharpen_gray(img_np)\n","\n","        # Convert back to tensor (add channel dimension)\n","        img_t = torch.from_numpy(img_np).unsqueeze(0).unsqueeze(0).float()\n","    else:\n","        # For color images\n","        img_np = edge_preserving_sharpen(img_np)\n","\n","        # Convert back to tensor\n","        img_t = torch.from_numpy(img_np.transpose(2, 0, 1)).unsqueeze(0).float()\n","\n","    return img_t\n","\n","def edge_preserving_sharpen_gray(img, sigma=0.5, amount=1.0):\n","    \"\"\"\n","    Apply edge-preserving sharpening to grayscale images.\n","    \"\"\"\n","    # Create a Gaussian kernel for edge detection\n","    kernel_size = max(3, int(2 * sigma) * 2 + 1)\n","    kernel_1d = np.exp(-np.arange(-(kernel_size//2), kernel_size//2 + 1)**2 / (2 * sigma**2))\n","    kernel_1d = kernel_1d / kernel_1d.sum()\n","    kernel_2d = np.outer(kernel_1d, kernel_1d)\n","\n","    # Apply Gaussian blur\n","    blurred = convolve2d(img, kernel_2d, mode='same', boundary='symm')\n","\n","    # Calculate edge mask\n","    gx = convolve2d(img, np.array([[-1, 0, 1]]), mode='same', boundary='symm')\n","    gy = convolve2d(img, np.array([[-1], [0], [1]]), mode='same', boundary='symm')\n","    gradient_mag = np.sqrt(gx**2 + gy**2)\n","\n","    # Normalize and invert to give less weight to edges\n","    edge_mask = 1 - np.clip(gradient_mag / (gradient_mag.max() + 1e-8), 0, 1)**2\n","\n","    # Apply sharpening with edge preservation\n","    high_freq = img - blurred\n","    sharpened = img + amount * high_freq * edge_mask\n","\n","    return np.clip(sharpened, 0, 1)\n","\n","def local_contrast_enhance(img, tile_size=16, clip_limit=3.0):\n","    \"\"\"\n","    Simplified CLAHE-like local contrast enhancement.\n","    \"\"\"\n","    result = np.zeros_like(img)\n","\n","    # Process each channel\n","    for c in range(img.shape[2]):\n","        channel = img[:, :, c]\n","        height, width = channel.shape\n","\n","        # Process each tile\n","        for y in range(0, height, tile_size):\n","            for x in range(0, width, tile_size):\n","                # Get the tile\n","                y_end = min(y + tile_size, height)\n","                x_end = min(x + tile_size, width)\n","                tile = channel[y:y_end, x:x_end]\n","\n","                # Skip empty tiles\n","                if tile.size == 0:\n","                    continue\n","\n","                # Compute histogram\n","                hist, bins = np.histogram(tile.flatten(), 256, [0, 1])\n","\n","                # Clip histogram\n","                if clip_limit > 0:\n","                    clip = clip_limit * tile.size / 256\n","                    hist_sum = 0\n","                    for i in range(len(hist)):\n","                        if hist[i] > clip:\n","                            hist_sum += hist[i] - clip\n","                            hist[i] = clip\n","\n","                    # Redistribute clipped pixels\n","                    redistr = hist_sum / 256\n","                    for i in range(len(hist)):\n","                        hist[i] += redistr\n","\n","                # Calculate CDF\n","                cdf = hist.cumsum()\n","                cdf = cdf / cdf[-1]  # Normalize\n","\n","                # Apply histogram equalization to the tile\n","                tile_result = np.interp(tile.flatten(), bins[:-1], cdf)\n","                result[y:y_end, x:x_end, c] = tile_result.reshape(tile.shape)\n","\n","    return result\n","\n","def histogram_equalization_with_color(img):\n","    \"\"\"\n","    Performs histogram equalization while preserving color relationships.\n","    Works in YCbCr color space to maintain color while enhancing contrast.\n","    \"\"\"\n","    # Convert to YCbCr-like space (simple approximation)\n","    y = 0.299 * img[:,:,0] + 0.587 * img[:,:,1] + 0.114 * img[:,:,2]\n","    cb = -0.1687 * img[:,:,0] - 0.3313 * img[:,:,1] + 0.5 * img[:,:,2] + 0.5\n","    cr = 0.5 * img[:,:,0] - 0.4187 * img[:,:,1] - 0.0813 * img[:,:,2] + 0.5\n","\n","    # Apply histogram equalization to Y channel only\n","    hist, bins = np.histogram(y.flatten(), 256, [0, 1])\n","    cdf = hist.cumsum()\n","    cdf = cdf / cdf[-1]  # Normalize\n","    y_eq = np.interp(y.flatten(), bins[:-1], cdf).reshape(y.shape)\n","\n","    # Convert back to RGB\n","    r = y_eq + 1.402 * (cr - 0.5)\n","    g = y_eq - 0.344136 * (cb - 0.5) - 0.714136 * (cr - 0.5)\n","    b = y_eq + 1.772 * (cb - 0.5)\n","\n","    # Combine and clip\n","    result = np.stack([r, g, b], axis=2)\n","    return np.clip(result, 0, 1)\n","\n","def edge_preserving_sharpen(img, sigma=0.5, amount=1.0):\n","    \"\"\"\n","    Apply edge-preserving sharpening using a bilateral filter approximation.\n","    \"\"\"\n","    # Create a Gaussian kernel for edge detection\n","    kernel_size = max(3, int(2 * sigma) * 2 + 1)\n","    kernel_1d = np.exp(-np.arange(-(kernel_size//2), kernel_size//2 + 1)**2 / (2 * sigma**2))\n","    kernel_1d = kernel_1d / kernel_1d.sum()\n","    kernel_2d = np.outer(kernel_1d, kernel_1d)\n","\n","    # Apply filtering\n","    blurred = np.zeros_like(img)\n","    for c in range(img.shape[2]):\n","        # Apply Gaussian blur for each channel\n","        blurred[:,:,c] = convolve2d(img[:,:,c], kernel_2d, mode='same', boundary='symm')\n","\n","    # Calculate edge mask\n","    edge_mask = np.ones_like(img)\n","    for c in range(img.shape[2]):\n","        # Create edge mask using gradient magnitude\n","        gx = convolve2d(img[:,:,c], np.array([[-1, 0, 1]]), mode='same', boundary='symm')\n","        gy = convolve2d(img[:,:,c], np.array([[-1], [0], [1]]), mode='same', boundary='symm')\n","        gradient_mag = np.sqrt(gx**2 + gy**2)\n","\n","        # Normalize and invert to give less weight to edges\n","        edge_mask[:,:,c] = 1 - np.clip(gradient_mag / gradient_mag.max(), 0, 1)**2\n","\n","    # Apply sharpening with edge preservation\n","    high_freq = img - blurred\n","    sharpened = img + amount * high_freq * edge_mask\n","\n","    return np.clip(sharpened, 0, 1)\n","\n","def convolve2d(img, kernel, mode='same', boundary='symm'):\n","    \"\"\"\n","    Simple 2D convolution implementation to avoid scipy dependency.\n","    \"\"\"\n","    k_h, k_w = kernel.shape\n","    i_h, i_w = img.shape\n","\n","    # Pad the image based on boundary mode\n","    if boundary == 'symm':\n","        pad_h = k_h // 2\n","        pad_w = k_w // 2\n","        padded = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='symmetric')\n","    else:\n","        pad_h = k_h // 2\n","        pad_w = k_w // 2\n","        padded = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n","\n","    # Output array\n","    out = np.zeros_like(img)\n","\n","    # Apply convolution\n","    for i in range(i_h):\n","        for j in range(i_w):\n","            out[i, j] = np.sum(padded[i:i+k_h, j:j+k_w] * kernel)\n","\n","    return out\n","\n","def color_balance(img, clip_percent=1):\n","    \"\"\"\n","    Automatically balance colors by applying separate contrast stretching to each channel.\n","    \"\"\"\n","    result = np.zeros_like(img)\n","\n","    for c in range(img.shape[2]):\n","        channel = img[:,:,c]\n","        # Calculate percentile values\n","        low = np.percentile(channel, clip_percent)\n","        high = np.percentile(channel, 100 - clip_percent)\n","\n","        # Apply contrast stretching\n","        result[:,:,c] = np.clip((channel - low) / (high - low), 0, 1)\n","\n","    return result\n","\n","# -------------------------------------------------------------\n","# 7. Highly Advanced Ensemble Attack with Knowledge Distillation\n","# -------------------------------------------------------------\n","def advanced_ensemble_attack(\n","        models,\n","        target_class,\n","        weights=None,\n","        distill_iterations=500,\n","        scales=None,\n","        verbose=True,\n","        log_interval=10,\n","        **attack_params):\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Move all models to the same device\n","    for i in range(len(models)):\n","        models[i] = models[i].to(device)\n","\n","    # Normalize weights if not provided\n","    if weights is None:\n","        weights = [1.0/len(models)] * len(models)\n","\n","    # Get model info\n","    model_info = get_model_info(models[0])\n","    input_channels = model_info[\"input_channels\"] or 1\n","\n","    # Set scales with 299x299 final size\n","    if scales is None:\n","        scales = [(1, input_channels, 112, 112),\n","                 (1, input_channels, 224, 224),\n","                 (1, input_channels, 299, 299)]\n","\n","    # Force 299x299 final scale\n","    if scales[-1][2:] != (299, 299):\n","        if verbose:\n","            print(f\"Adjusting final scale to (1, {input_channels}, 299, 299)\")\n","        scales[-1] = (1, input_channels, 299, 299)\n","\n","    # Phase 1: Individual model inversions\n","    print(\"\\nPhase 1: Individual model inversions\")\n","    individual_images = []\n","\n","    indiv_attack_params = attack_params.copy()\n","    if 'iterations_per_scale' in indiv_attack_params:\n","        indiv_attack_params['iterations_per_scale'] = indiv_attack_params['iterations_per_scale'] // 2\n","\n","    for i, model in enumerate(models):\n","        print(f\"  Inverting model {i+1}/{len(models)}\")\n","        img = progressive_model_inversion_attack(model, target_class, scales=scales[:2], **indiv_attack_params)\n","        if img is not None:  # Ensure we have a valid image\n","            individual_images.append(img)\n","        else:\n","            print(f\"  WARNING: Model {i+1} produced no image\")\n","            # Create a default random noise image\n","            default_img = torch.rand((1, input_channels, 224, 224), device=device)\n","            individual_images.append(default_img)\n","\n","    # Verify we have at least one valid image\n","    if not individual_images:\n","        print(\"ERROR: No valid images produced. Returning random noise.\")\n","        return torch.rand((1, input_channels, 299, 299), device=device)\n","\n","    # Phase 2: Distillation\n","    print(\"\\nPhase 2: Knowledge distillation from individual reconstructions\")\n","\n","    # Resize all images to the same dimensions before averaging\n","    standard_size = (1, input_channels, 224, 224)  # Use a standard size for distillation\n","    resized_images = []\n","\n","    print(\"  Standardizing image dimensions...\")\n","    for i, img in enumerate(individual_images):\n","        print(f\"  Image {i+1} shape before resize: {img.shape}\")\n","        resized = F.interpolate(img.to(device), size=standard_size[2:], mode='bilinear', align_corners=False)\n","        print(f\"  Image {i+1} shape after resize: {resized.shape}\")\n","        resized_images.append(resized)\n","\n","    # Initialize ensemble seed with weighted average of RESIZED images\n","    ensemble_seed = torch.zeros(standard_size, device=device)\n","    for img, weight in zip(resized_images, weights):\n","        ensemble_seed += weight * img\n","\n","    # Ensure ensemble_seed is valid and on the correct device\n","    ensemble_seed = torch.clamp(ensemble_seed, 0, 1)  # Ensure valid range\n","    ensemble_seed.requires_grad = True\n","\n","    # Run distillation\n","    optimizer = optim.Adam([ensemble_seed], lr=0.01)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=distill_iterations, eta_min=0.001)\n","\n","    # Debug print\n","    print(f\"  Ensemble seed shape: {ensemble_seed.shape}, device: {ensemble_seed.device}\")\n","    print(f\"  Model devices: {[next(m.parameters()).device for m in models]}\")\n","\n","    feature_hook = FeatureHook(\"features\", adaptive_stats=False)\n","    hooks = []\n","\n","    for iter in range(distill_iterations):\n","        optimizer.zero_grad()\n","\n","        # Resize to 299x299 for model\n","        resized_seed = F.interpolate(ensemble_seed, size=(299, 299),\n","                                   mode='bilinear', align_corners=False)\n","\n","        # Ensemble classification loss\n","        ce_loss = 0\n","        for model, weight in zip(models, weights):\n","            output = model(resized_seed)\n","            target = torch.tensor([target_class], device=device)\n","            ce_loss += weight * F.cross_entropy(output, target)\n","\n","        # Feature consistency with individual reconstructions\n","        feature_loss = 0\n","        for i, (model, indiv_img) in enumerate(zip(models, individual_images)):\n","            # Register temporary hooks\n","            for name, module in model.named_modules():\n","                if isinstance(module, nn.Conv2d) and 'features' in name:\n","                    hook = module.register_forward_hook(feature_hook.hook_fn)\n","                    hooks.append(hook)\n","                    break\n","\n","            # Get individual image features\n","            with torch.no_grad():\n","                indiv_img_resized = F.interpolate(indiv_img.to(device), size=(299, 299),\n","                                                mode='bilinear', align_corners=False)\n","                _ = model(indiv_img_resized)\n","                target_features = feature_hook.features.detach() if feature_hook.features is not None else None\n","\n","            # Skip if we couldn't extract features\n","            if target_features is None:\n","                continue\n","\n","            # Get current seed features\n","            _ = model(resized_seed)\n","            current_features = feature_hook.features\n","\n","            # Compute feature consistency loss if we have both feature sets\n","            if current_features is not None and target_features is not None:\n","                feature_loss += weights[i] * F.mse_loss(current_features, target_features)\n","\n","            # Clear hooks\n","            for hook in hooks:\n","                hook.remove()\n","            hooks = []\n","\n","        # Regularization\n","        tv_loss = total_variation_loss(ensemble_seed, 1e-4)\n","\n","        # Total loss\n","        total_loss = ce_loss + 0.5 * feature_loss + tv_loss\n","\n","        # Update\n","        total_loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Ensure valid range\n","        ensemble_seed.data = torch.clamp(ensemble_seed.data, 0, 1)\n","\n","        if verbose and (iter % log_interval == 0 or iter == distill_iterations - 1):\n","            print(f\"  Distillation iter {iter}/{distill_iterations}, \"\n","                  f\"Loss: {total_loss.item():.4f}, CE: {ce_loss.item():.4f}\")\n","\n","    # Phase 3: Final ensemble optimization\n","    print(\"\\nPhase 3: Final ensemble optimization\")\n","\n","    # Create ensemble model wrapper\n","    ensemble_model = EnsembleModel(models, weights).to(device)\n","\n","    # Ensure we have a valid seed image\n","    final_seed = ensemble_seed.detach().to(device)\n","    if torch.isnan(final_seed).any() or torch.isinf(final_seed).any():\n","        print(\"WARNING: NaN/Inf values detected in distilled image. Using random initialization.\")\n","        final_seed = torch.rand((1, input_channels, 224, 224), device=device)\n","\n","    # Final inversion with ensemble model\n","    final_img = progressive_model_inversion_attack(\n","        ensemble_model,\n","        target_class,\n","        scales=scales,\n","        init_img=final_seed,\n","        **attack_params\n","    )\n","\n","    # Final sanity check\n","    if final_img is None or torch.isnan(final_img).any() or torch.isinf(final_img).any():\n","        print(\"ERROR: Invalid final image. Returning the distilled seed instead.\")\n","        return final_seed\n","\n","    return final_img\n","\n","class EnsembleModel(nn.Module):\n","    def __init__(self, models, weights):\n","        super().__init__()\n","        self.models = nn.ModuleList(models)  # Use ModuleList for proper device handling\n","        self.weights = weights\n","\n","        # For feature extraction capability\n","        # Get the first model's features module if available\n","        self.features = None\n","        for model in models:\n","            if hasattr(model, 'features'):\n","                self.features = model.features\n","                break\n","\n","    def forward(self, x):\n","        outputs = []\n","        for model in self.models:\n","            outputs.append(model(x))\n","\n","        # Weighted average of logits\n","        result = torch.zeros_like(outputs[0])\n","        for output, weight in zip(outputs, self.weights):\n","            result += weight * output\n","        return result\n","\n","    # Add direct feature extraction method to simplify hook registration\n","    def get_feature_maps(self, x):\n","        if hasattr(self.models[0], 'get_feature_maps'):\n","            return self.models[0].get_feature_maps(x)\n","        return []  # Return empty list if not available\n","\n","# -------------------------------------------------------------\n","# 8. Main Routine: State-of-the-Art Federated MI Attack Framework\n","# -------------------------------------------------------------\n","def get_model_info(model):\n","    \"\"\"Extract input channels, expected input size, and classes from model architecture\"\"\"\n","    input_channels = None\n","    num_classes = None\n","    expected_input_size = None\n","\n","    # Find first conv layer for input channels\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            input_channels = module.in_channels\n","            break\n","\n","    # Try to determine expected input size from model\n","    if hasattr(model, 'flat_features'):\n","        expected_input_size = model.flat_features\n","    else:\n","        # Default to standard size for BrainMRIClassifier\n","        expected_input_size = 256 * 9 * 9  # As defined in BrainMRIClassifier\n","\n","    # Find output classes from last linear layer\n","    last_linear = None\n","    for module in model.modules():\n","        if isinstance(module, nn.Linear):\n","            last_linear = module\n","    if last_linear:\n","        num_classes = last_linear.out_features\n","\n","    return {\n","        \"input_channels\": input_channels,\n","        \"num_classes\": num_classes,\n","        \"expected_input_size\": expected_input_size\n","    }\n","\n","# Add this function to debug model dimensions:\n","def debug_model_dimensions(model, input_size=(1, 1, 299, 299)):\n","    \"\"\"Print the output shape at each layer to diagnose dimension issues\"\"\"\n","    device = next(model.parameters()).device\n","    x = torch.randn(input_size).to(device)\n","\n","    # Track feature dimensions\n","    print(f\"Input shape: {x.shape}\")\n","\n","    # Features\n","    for i, layer in enumerate(model.features):\n","        x = layer(x)\n","        print(f\"After features[{i}] {type(layer).__name__}: {x.shape}\")\n","\n","    # Check final flattened size\n","    flat_size = x.view(x.size(0), -1).shape[1]\n","    print(f\"Flattened size: {flat_size}, Expected: {model.flat_features}\")\n","\n","    # Try classifier\n","    try:\n","        output = model.classifier(x)\n","        print(f\"Final output shape: {output.shape}\")\n","    except Exception as e:\n","        print(f\"Error in classifier: {str(e)}\")\n","\n","\n","# Helper function to scan for existing inversion images.\n","def get_existing_inversions(base_path):\n","    \"\"\"\n","    Recursively scans base_path for any PNG files and returns a set\n","    of their full file paths.\n","    \"\"\"\n","    existing_files = set()\n","    for root, dirs, files in os.walk(base_path):\n","        for file in files:\n","            if file.endswith('.png'):\n","                existing_files.add(os.path.join(root, file))\n","    return existing_files\n","\n","# Helper function to load an image from file and convert it to a tensor.\n","def load_inversion_image(filename, device):\n","    \"\"\"\n","    Loads an image from filename, converts it to a tensor, and adds a batch dimension.\n","    The tensor values are normalized to [0, 1].\n","    \"\"\"\n","    img = Image.open(filename).convert(\"RGB\")\n","    transform = transforms.ToTensor()  # Converts image to tensor with values in [0, 1]\n","    img_tensor = transform(img).to(device)\n","    # Add batch dimension if missing\n","    if len(img_tensor.shape) == 3:\n","        img_tensor = img_tensor.unsqueeze(0)\n","    return img_tensor\n","\n","def main():\n","    # ----------------------------------------------------------------\n","    # Step 1: Create the base save directory dynamically\n","    # ----------------------------------------------------------------\n","    base_save_path = os.path.join(os.getcwd(), \"inverted_images\", \"gs20p\")\n","    if not os.path.exists(base_save_path):\n","        os.makedirs(base_save_path)\n","    sys.stdout.write(f\"Base save path set to: {base_save_path}\\n\")\n","    sys.stdout.flush()\n","\n","    # Scan for existing inversion files\n","    existing_inversions = get_existing_inversions(base_save_path)\n","    sys.stdout.write(f\"Found {len(existing_inversions)} existing inversion files.\\n\")\n","    sys.stdout.flush()\n","\n","    # ----------------------------------------------------------------\n","    # Step 2: Set up file paths for pre-trained models\n","    # ----------------------------------------------------------------\n","    MODEL_PATHS = {\n","        'global': '/home/jupyter/notebooks/federated/model_inversion_fl/saved_models/global_model_gs20p.pth',\n","        'clients': [\n","            '/home/jupyter/notebooks/federated/model_inversion_fl/saved_models/client_0_model_gs20p.pth',\n","            '/home/jupyter/notebooks/federated/model_inversion_fl/saved_models/client_1_model_gs20p.pth',\n","            '/home/jupyter/notebooks/federated/model_inversion_fl/saved_models/client_2_model_gs20p.pth'\n","        ]\n","    }\n","    client_model_paths = MODEL_PATHS['clients']\n","    global_model_path = MODEL_PATHS['global']\n","\n","    # ----------------------------------------------------------------\n","    # Step 3: Load models with dynamic progress updates\n","    # ----------------------------------------------------------------\n","    verbose = True\n","    log_interval = 10  # Log every 10 iterations\n","\n","    sys.stdout.write(\"Loading client models...\\n\")\n","    sys.stdout.flush()\n","    client_models = []\n","    for i, path in enumerate(client_model_paths):\n","        sys.stdout.write(f\"\\rLoading client model {i+1}/{len(client_model_paths)}\")\n","        sys.stdout.flush()\n","        client_models.append(load_model(path, model_class=BrainMRIClassifier))\n","    sys.stdout.write(\"\\n\")\n","\n","    sys.stdout.write(\"Loading global model...\\n\")\n","    sys.stdout.flush()\n","    global_model = load_model(global_model_path, model_class=BrainMRIClassifier)\n","    sys.stdout.write(\"All models loaded successfully\\n\")\n","    sys.stdout.flush()\n","\n","    sys.stdout.write(\"\\nDebugging model dimensions...\\n\")\n","    sys.stdout.flush()\n","    debug_model_dimensions(client_models[0])\n","\n","    # Dynamically determine model parameters\n","    model_info = get_model_info(global_model)\n","    input_channels = model_info[\"input_channels\"] or 1\n","    num_classes = model_info[\"num_classes\"] or 4\n","    sys.stdout.write(f\"Detected model configuration: {input_channels} input channels, {num_classes} output classes\\n\")\n","    sys.stdout.flush()\n","\n","    # ----------------------------------------------------------------\n","    # Step 4: Set inversion parameters and scales\n","    # ----------------------------------------------------------------\n","    num_samples = 20  # Number of inversion samples per model per class\n","    target_classes = list(range(min(4, num_classes)))\n","    scales = [(1, input_channels, 112, 112),\n","              (1, input_channels, 224, 224),\n","              (1, input_channels, 299, 299)]\n","    attack_params = {\n","        'iterations_per_scale': 1000,\n","        'lr_initial': 0.05,\n","        'lr_final': 0.001,\n","        'scales': scales,\n","        'auto_schedule_hyperparams': True,\n","        'regularization_weights': {\n","            'tv': 2e-3,\n","            'l2': 5e-4,\n","            'color': 1e-4,\n","            'smooth': 3e-4,\n","            'natural': 5e-4,\n","            'fft': 5e-5,\n","            'feature': 1e-2\n","        },\n","        'init_strategy': 'mixed'\n","    }\n","\n","    # For ensemble attack: define weights and aggregate all models\n","    ensemble_weights = [0.5, 0.3, 0.2, 1.0]\n","    all_models = client_models + [global_model]\n","\n","    # Dictionary to store one sample per model type per class for later comparison\n","    inverted_images = {}\n","\n","    # ----------------------------------------------------------------\n","    # Step 5: Process each target class\n","    # ----------------------------------------------------------------\n","    for target_class in target_classes:\n","        # Create subfolder for this target class\n","        class_save_path = os.path.join(base_save_path, f\"class_{target_class}\")\n","        if not os.path.exists(class_save_path):\n","            os.makedirs(class_save_path)\n","        sys.stdout.write(f\"\\n=== Processing target class {target_class} ===\\n\")\n","        sys.stdout.write(f\"Images for class {target_class} will be saved in: {class_save_path}\\n\")\n","        sys.stdout.flush()\n","\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # ---- Inversions for Client Models ----\n","        for i, model in enumerate(client_models):\n","            client_save_path = os.path.join(class_save_path, f\"client_{i+1}\")\n","            if not os.path.exists(client_save_path):\n","                os.makedirs(client_save_path)\n","            sys.stdout.write(f\"\\nPerforming inversion attack on Client Model {i+1} for class {target_class}\\n\")\n","            sys.stdout.flush()\n","            first_sample_saved = False\n","            for sample in range(num_samples):\n","                filename = os.path.join(\n","                    client_save_path,\n","                    f\"client_model_{i+1}_class_{target_class}_inv_{sample+1}.png\"\n","                )\n","                if filename in existing_inversions:\n","                    sys.stdout.write(f\"\\rClient Model {i+1} [Class {target_class}]: Sample {sample+1} already exists. Skipping...\\n\")\n","                    sys.stdout.flush()\n","                    # Load the existing image for dictionary storage if not already stored\n","                    if not first_sample_saved:\n","                        inv_img = load_inversion_image(filename, device)\n","                        inverted_images[f'client_model_{i+1}_class_{target_class}'] = inv_img\n","                        first_sample_saved = True\n","                    continue\n","                inv_img = progressive_model_inversion_attack(\n","                    model,\n","                    target_class,\n","                    verbose=verbose,\n","                    log_interval=log_interval,\n","                    **attack_params\n","                )\n","                vutils.save_image(inv_img, filename)\n","                sys.stdout.write(f\"\\rClient Model {i+1} [Class {target_class}]: Sample {sample+1}/{num_samples} saved\")\n","                sys.stdout.flush()\n","                if not first_sample_saved:\n","                    inverted_images[f'client_model_{i+1}_class_{target_class}'] = inv_img\n","                    first_sample_saved = True\n","            sys.stdout.write(\"\\n\")\n","            sys.stdout.flush()\n","\n","        # ---- Inversions for Global Model ----\n","        global_save_path = os.path.join(class_save_path, \"global\")\n","        if not os.path.exists(global_save_path):\n","            os.makedirs(global_save_path)\n","        sys.stdout.write(f\"\\nPerforming inversion attack on Global Model for class {target_class}\\n\")\n","        sys.stdout.flush()\n","        first_sample_saved = False\n","        for sample in range(num_samples):\n","            filename = os.path.join(\n","                global_save_path,\n","                f\"global_model_class_{target_class}_inv_{sample+1}.png\"\n","            )\n","            if filename in existing_inversions:\n","                sys.stdout.write(f\"\\rGlobal Model [Class {target_class}]: Sample {sample+1} already exists. Skipping...\\n\")\n","                sys.stdout.flush()\n","                if not first_sample_saved:\n","                    inv_img_global = load_inversion_image(filename, device)\n","                    inverted_images[f'global_model_class_{target_class}'] = inv_img_global\n","                    first_sample_saved = True\n","                continue\n","            inv_img_global = progressive_model_inversion_attack(\n","                global_model,\n","                target_class,\n","                verbose=verbose,\n","                log_interval=log_interval,\n","                **attack_params\n","            )\n","            vutils.save_image(inv_img_global, filename)\n","            sys.stdout.write(f\"\\rGlobal Model [Class {target_class}]: Sample {sample+1}/{num_samples} saved\")\n","            sys.stdout.flush()\n","            if not first_sample_saved:\n","                inverted_images[f'global_model_class_{target_class}'] = inv_img_global\n","                first_sample_saved = True\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","        # ---- Inversions for Ensemble Model ----\n","        ensemble_save_path = os.path.join(class_save_path, \"ensemble\")\n","        if not os.path.exists(ensemble_save_path):\n","            os.makedirs(ensemble_save_path)\n","        sys.stdout.write(f\"\\nPerforming advanced ensemble attack for class {target_class}\\n\")\n","        sys.stdout.flush()\n","        first_sample_saved = False\n","        for sample in range(num_samples):\n","            filename = os.path.join(\n","                ensemble_save_path,\n","                f\"ensemble_model_class_{target_class}_inv_{sample+1}.png\"\n","            )\n","            if filename in existing_inversions:\n","                sys.stdout.write(f\"\\rEnsemble Model [Class {target_class}]: Sample {sample+1} already exists. Skipping...\\n\")\n","                sys.stdout.flush()\n","                if not first_sample_saved:\n","                    inv_img_ensemble = load_inversion_image(filename, device)\n","                    inverted_images[f'ensemble_model_class_{target_class}'] = inv_img_ensemble\n","                    first_sample_saved = True\n","                continue\n","            inv_img_ensemble = advanced_ensemble_attack(\n","                all_models,\n","                target_class,\n","                weights=ensemble_weights,\n","                verbose=verbose,\n","                log_interval=log_interval,\n","                **attack_params\n","            )\n","            vutils.save_image(inv_img_ensemble, filename)\n","            sys.stdout.write(f\"\\rEnsemble Model [Class {target_class}]: Sample {sample+1}/{num_samples} saved\")\n","            sys.stdout.flush()\n","            if not first_sample_saved:\n","                inverted_images[f'ensemble_model_class_{target_class}'] = inv_img_ensemble\n","                first_sample_saved = True\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","        # ---- Optional Comparative Analysis ----\n","        sys.stdout.write(f\"\\nRunning comparative analysis for class {target_class}...\\n\")\n","        sys.stdout.flush()\n","        compare_reconstructions(\n","            [\n","                inverted_images.get(f'client_model_1_class_{target_class}'),\n","                inverted_images.get(f'client_model_2_class_{target_class}'),\n","                inverted_images.get(f'client_model_3_class_{target_class}'),\n","                inverted_images.get(f'global_model_class_{target_class}'),\n","                inverted_images.get(f'ensemble_model_class_{target_class}')\n","            ],\n","            ['Client 1', 'Client 2', 'Client 3', 'Global Model', 'Ensemble Model'],\n","            os.path.join(class_save_path, f\"class_{target_class}_comparison.png\")\n","        )\n","        sys.stdout.write(\"\\n\")\n","        sys.stdout.flush()\n","\n","    # ----------------------------------------------------------------\n","    # Step 6: Final display of results if needed\n","    # ----------------------------------------------------------------\n","    display_results(inverted_images)\n","\n","\n","\n","\n","def compare_reconstructions(images, labels, filename):\n","    \"\"\"\n","    Create a visual comparison of different reconstruction methods\n","    with evaluation metrics. Fixed to handle different tensor dimensions.\n","    \"\"\"\n","    fig, axs = plt.subplots(1, len(images), figsize=(4*len(images), 4))\n","\n","    # Handle the case of a single image (convert axs to array for consistent indexing)\n","    if len(images) == 1:\n","        axs = np.array([axs])\n","\n","    for i, (img, label) in enumerate(zip(images, labels)):\n","        # Display the image with proper dimension handling\n","        img_cpu = img.squeeze().cpu()\n","\n","        # Check tensor dimensions\n","        if len(img_cpu.shape) == 2:\n","            # Already a 2D grayscale image (H, W)\n","            img_np = img_cpu.numpy()\n","            axs[i].imshow(img_np, cmap='gray')\n","\n","        elif len(img_cpu.shape) == 3:\n","            if img_cpu.shape[0] == 1:\n","                # Single channel image (1, H, W)\n","                img_np = img_cpu[0].numpy()\n","                axs[i].imshow(img_np, cmap='gray')\n","            else:\n","                # Color image (3, H, W) -> (H, W, 3)\n","                img_np = img_cpu.permute(1, 2, 0).numpy()\n","                axs[i].imshow(img_np)\n","        else:\n","            # Unexpected format - display blank with error text\n","            axs[i].text(0.5, 0.5, f\"Invalid shape: {img_cpu.shape}\",\n","                      ha='center', va='center', transform=axs[i].transAxes)\n","\n","        # Try to calculate quality metrics\n","        try:\n","            if len(img_cpu.shape) == 3 and img_cpu.shape[0] == 3:\n","                # Color image\n","                sharpness = calculate_sharpness(img_np)\n","                colorfulness = calculate_colorfulness(img_np)\n","                axs[i].set_title(f\"{label}\\nSharp: {sharpness:.2f}, Color: {colorfulness:.2f}\")\n","            else:\n","                # Grayscale image - only calculate sharpness\n","                if len(img_cpu.shape) == 2:\n","                    sharpness = calculate_sharpness_gray(img_cpu.numpy())\n","                else:\n","                    sharpness = calculate_sharpness_gray(img_cpu[0].numpy())\n","                axs[i].set_title(f\"{label}\\nSharp: {sharpness:.2f}\")\n","        except Exception as e:\n","            # If metrics fail, just show the label\n","            print(f\"Error calculating metrics for {label}: {str(e)}\")\n","            axs[i].set_title(label)\n","\n","        axs[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig(filename, dpi=300)\n","    plt.close()\n","\n","def calculate_sharpness_gray(img):\n","    \"\"\"\n","    Calculate image sharpness using gradient magnitude for grayscale images.\n","    \"\"\"\n","    # Compute gradients\n","    if img.shape[0] > 1 and img.shape[1] > 1:\n","        gx = img[1:, :] - img[:-1, :]\n","        gy = img[:, 1:] - img[:, :-1]\n","\n","        # Use the valid region where both gradients are available\n","        if gx.shape[1] > 0 and gy.shape[0] > 0:\n","            gx_valid = gx[:, :-1]\n","            gy_valid = gy[:-1, :]\n","\n","            # Compute gradient magnitude\n","            grad_mag = np.sqrt(gx_valid**2 + gy_valid**2)\n","\n","            # Return mean gradient magnitude as sharpness measure\n","            return np.mean(grad_mag)\n","\n","    # Fallback\n","    return 0.0\n","\n","def calculate_sharpness(img):\n","    \"\"\"\n","    Calculate image sharpness using gradient magnitude for color images.\n","    \"\"\"\n","    # Convert to grayscale for gradient calculation\n","    gray = 0.2989 * img[:,:,0] + 0.5870 * img[:,:,1] + 0.1140 * img[:,:,2]\n","    return calculate_sharpness_gray(gray)\n","\n","def calculate_colorfulness(img):\n","    \"\"\"\n","    Calculate perceptual colorfulness metric.\n","    Based on Hasler and SÃ¼sstrunk (2003) metric.\n","    \"\"\"\n","    # Split image into channels\n","    r = img[:,:,0]\n","    g = img[:,:,1]\n","    b = img[:,:,2]\n","\n","    # Compute rg and yb components\n","    rg = r - g\n","    yb = 0.5 * (r + g) - b\n","\n","    # Compute mean and std of components\n","    rg_mean = np.mean(rg)\n","    rg_std = np.std(rg)\n","    yb_mean = np.mean(yb)\n","    yb_std = np.std(yb)\n","\n","    # Compute the colorfulness metric\n","    mean_rgyb = np.sqrt(rg_mean**2 + yb_mean**2)\n","    std_rgyb = np.sqrt(rg_std**2 + yb_std**2)\n","\n","    return mean_rgyb + std_rgyb\n","\n","# Also update display_results function\n","def display_results(inverted_images):\n","    \"\"\"\n","    Display all generated images grouped by target class with metrics.\n","    \"\"\"\n","    # Group by class\n","    class_groups = {}\n","    for key in inverted_images:\n","        if 'class' in key:\n","            class_id = key.split('class_')[1].split('_')[0]\n","            if class_id not in class_groups:\n","                class_groups[class_id] = []\n","            class_groups[class_id].append((key, inverted_images[key]))\n","\n","    # Plot each class separately with quality metrics\n","    for class_id, images in class_groups.items():\n","        # Handle differently based on number of images\n","        num_images = len(images)\n","        if num_images == 0:\n","            continue\n","\n","        fig, axs = plt.subplots(2, num_images, figsize=(15, 10))\n","        fig.suptitle(f\"Reconstructions for Class {class_id}\", fontsize=16)\n","\n","        # If only one image, axs needs to be reshaped for consistent indexing\n","        if num_images == 1:\n","            axs = np.array([[axs[0]], [axs[1]]])\n","\n","        for i, (key, img_tensor) in enumerate(images):\n","            # Convert tensor to numpy for display with dimension handling\n","            img = img_tensor.squeeze().cpu()\n","\n","            # Handle both color and grayscale images\n","            if len(img.shape) == 3 and img.shape[0] in [1, 3]:\n","                if img.shape[0] == 1:\n","                    # Grayscale image\n","                    img_display = img[0].numpy()\n","                    axs[0, i].imshow(img_display, cmap='gray')\n","                else:\n","                    # Color image - permute to HWC for plotting\n","                    img_display = img.permute(1, 2, 0).numpy()\n","                    axs[0, i].imshow(img_display)\n","            elif len(img.shape) == 2:\n","                # Direct 2D grayscale\n","                axs[0, i].imshow(img.numpy(), cmap='gray')\n","            else:\n","                # Unexpected format\n","                axs[0, i].text(0.5, 0.5, f\"Invalid shape: {img.shape}\",\n","                             ha='center', va='center', transform=axs[0, i].transAxes)\n","\n","            axs[0, i].set_title(key.split(f'_class_{class_id}')[0])\n","            axs[0, i].axis('off')\n","\n","            # Display frequency spectrum for visualization\n","            try:\n","                # Convert to grayscale for FFT if needed\n","                if len(img.shape) == 3 and img.shape[0] == 3:\n","                    gray = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]\n","                elif len(img.shape) == 3 and img.shape[0] == 1:\n","                    gray = img[0]\n","                else:\n","                    gray = img\n","\n","                f_transform = np.fft.fft2(gray.numpy())\n","                f_transform_shifted = np.fft.fftshift(f_transform)\n","                magnitude = np.log(np.abs(f_transform_shifted) + 1)\n","\n","                axs[1, i].imshow(magnitude, cmap='viridis')\n","                axs[1, i].set_title('Frequency Domain')\n","                axs[1, i].axis('off')\n","            except Exception as e:\n","                axs[1, i].text(0.5, 0.5, f\"FFT error: {str(e)[:20]}...\",\n","                             ha='center', va='center', transform=axs[1, i].transAxes)\n","\n","        plt.tight_layout()\n","        plt.subplots_adjust(top=0.9)\n","        plt.savefig(f'comparison_class_{class_id}.png')\n","        plt.close()\n","\n","def get_existing_inversions(base_path):\n","    \"\"\"\n","    Recursively scans base_path for any PNG files (i.e. previously saved inversions)\n","    and returns a set of their full file paths.\n","\n","    If for some reason, the framework stops or breaks, this function will trace the base_path\n","    and let you continue from where you left of instead of starting from the beginning!\n","\n","    If you want a fresh start, please delete all inverted images on your base_path, or simply\n","    change your base_path location to an empty folder!\n","    \"\"\"\n","    existing_files = set()\n","    for root, dirs, files in os.walk(base_path):\n","        for file in files:\n","            if file.endswith('.png'):\n","                existing_files.add(os.path.join(root, file))\n","    return existing_files\n"]},{"cell_type":"code","execution_count":null,"id":"51a4fa17-0922-4b5e-9625-e80027447d39","metadata":{"tags":[],"id":"51a4fa17-0922-4b5e-9625-e80027447d39","outputId":"fd7ce94a-fc1d-4708-f553-cbda74b067b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Base save path set to: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/gs20p\n","Found 119 existing inversion files.\n","Loading client models...\n","Loading client model 3/3\n","Loading global model...\n","All models loaded successfully\n","\n","Debugging model dimensions...\n","Input shape: torch.Size([1, 1, 299, 299])\n","After features[0] Conv2d: torch.Size([1, 32, 299, 299])\n","After features[1] ReLU: torch.Size([1, 32, 299, 299])\n","After features[2] MaxPool2d: torch.Size([1, 32, 149, 149])\n","After features[3] BatchNorm2d: torch.Size([1, 32, 149, 149])\n","After features[4] Conv2d: torch.Size([1, 64, 149, 149])\n","After features[5] ReLU: torch.Size([1, 64, 149, 149])\n","After features[6] MaxPool2d: torch.Size([1, 64, 74, 74])\n","After features[7] BatchNorm2d: torch.Size([1, 64, 74, 74])\n","After features[8] Conv2d: torch.Size([1, 128, 74, 74])\n","After features[9] ReLU: torch.Size([1, 128, 74, 74])\n","After features[10] MaxPool2d: torch.Size([1, 128, 37, 37])\n","After features[11] BatchNorm2d: torch.Size([1, 128, 37, 37])\n","After features[12] Conv2d: torch.Size([1, 256, 37, 37])\n","After features[13] ReLU: torch.Size([1, 256, 37, 37])\n","After features[14] MaxPool2d: torch.Size([1, 256, 18, 18])\n","After features[15] BatchNorm2d: torch.Size([1, 256, 18, 18])\n","After features[16] Conv2d: torch.Size([1, 256, 18, 18])\n","After features[17] ReLU: torch.Size([1, 256, 18, 18])\n","After features[18] MaxPool2d: torch.Size([1, 256, 9, 9])\n","After features[19] BatchNorm2d: torch.Size([1, 256, 9, 9])\n","Flattened size: 20736, Expected: 20736\n","Final output shape: torch.Size([1, 4])\n","Detected model configuration: 1 input channels, 4 output classes\n","\n","=== Processing target class 0 ===\n","Images for class 0 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/gs20p/class_0\n","\n","Performing inversion attack on Client Model 1 for class 0\n","Client Model 1 [Class 0]: Sample 1 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 2 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 3 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 4 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 5 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 6 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 7 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 8 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 9 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 10 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 11 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 12 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 13 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 14 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 15 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 16 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 17 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 18 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 19 already exists. Skipping...\n","Client Model 1 [Class 0]: Sample 20 already exists. Skipping...\n","\n","\n","Performing inversion attack on Client Model 2 for class 0\n","Client Model 2 [Class 0]: Sample 1 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 2 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 3 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 4 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 5 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 6 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 7 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 8 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 9 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 10 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 11 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 12 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 13 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 14 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 15 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 16 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 17 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 18 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 19 already exists. Skipping...\n","Client Model 2 [Class 0]: Sample 20 already exists. Skipping...\n","\n","\n","Performing inversion attack on Client Model 3 for class 0\n","Client Model 3 [Class 0]: Sample 1 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 2 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 3 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 4 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 5 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 6 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 7 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 8 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 9 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 10 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 11 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 12 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 13 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 14 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 15 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 16 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 17 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 18 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 19 already exists. Skipping...\n","Client Model 3 [Class 0]: Sample 20 already exists. Skipping...\n","\n","\n","Performing inversion attack on Global Model for class 0\n","Global Model [Class 0]: Sample 1 already exists. Skipping...\n","Global Model [Class 0]: Sample 2 already exists. Skipping...\n","Global Model [Class 0]: Sample 3 already exists. Skipping...\n","Global Model [Class 0]: Sample 4 already exists. Skipping...\n","Global Model [Class 0]: Sample 5 already exists. Skipping...\n","Global Model [Class 0]: Sample 6 already exists. Skipping...\n","Global Model [Class 0]: Sample 7 already exists. Skipping...\n","Global Model [Class 0]: Sample 8 already exists. Skipping...\n","Global Model [Class 0]: Sample 9 already exists. Skipping...\n","Global Model [Class 0]: Sample 10 already exists. Skipping...\n","Global Model [Class 0]: Sample 11 already exists. Skipping...\n","Global Model [Class 0]: Sample 12 already exists. Skipping...\n","Global Model [Class 0]: Sample 13 already exists. Skipping...\n","Global Model [Class 0]: Sample 14 already exists. Skipping...\n","Global Model [Class 0]: Sample 15 already exists. Skipping...\n","Global Model [Class 0]: Sample 16 already exists. Skipping...\n","Global Model [Class 0]: Sample 17 already exists. Skipping...\n","Global Model [Class 0]: Sample 18 already exists. Skipping...\n","Global Model [Class 0]: Sample 19 already exists. Skipping...\n","Global Model [Class 0]: Sample 20 already exists. Skipping...\n","\n","\n","Performing advanced ensemble attack for class 0\n","Ensemble Model [Class 0]: Sample 1 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 2 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 3 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 4 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 5 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 6 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 7 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 8 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 9 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 10 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 11 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 12 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 13 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 14 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 15 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 16 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 17 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 18 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 19 already exists. Skipping...\n","Ensemble Model [Class 0]: Sample 20 already exists. Skipping...\n","\n","\n","Running comparative analysis for class 0...\n","\n","\n","=== Processing target class 1 ===\n","Images for class 1 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/gs20p/class_1\n","\n","Performing inversion attack on Client Model 1 for class 1\n","Client Model 1 [Class 1]: Sample 1 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 2 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 3 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 4 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 5 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 6 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 7 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 8 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 9 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 10 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 11 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 12 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 13 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 14 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 15 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 16 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 17 already exists. Skipping...\n","Client Model 1 [Class 1]: Sample 18 already exists. Skipping...\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0031, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0031, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0073, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 1]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0118, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0051, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0072, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0096, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0121, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0098, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0101, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0103, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0087, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0091, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0113, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0098, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0091, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0104, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0077, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0090, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0092, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0123, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0111, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0076, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0105, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 1]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0085, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0073, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0088, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0090, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0137, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0076, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0097, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0095, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0073, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0128, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0108, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0051, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0083, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0080, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0098, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0106, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0169, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0073, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0091, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0125, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0084, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 1]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 1\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0092, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0032, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0067, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0085, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0082, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0033, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0066, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0067, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0072, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0082, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0081, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0065, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0073, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0072, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0070, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0079, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0032, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0074, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0067, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 1]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 1\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0729, CE: 0.0091\n","  Distillation iter 10/500, Loss: 0.0543, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0446, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0367, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0310, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0230, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0181, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0663, CE: 0.0024\n","  Distillation iter 10/500, Loss: 0.0494, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0368, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0284, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0234, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0180, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0681, CE: 0.0032\n","  Distillation iter 10/500, Loss: 0.0517, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0312, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0260, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0230, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0189, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0750, CE: 0.0106\n","  Distillation iter 10/500, Loss: 0.0557, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0468, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0338, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0298, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0271, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0252, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0239, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0229, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0208, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0190, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0677, CE: 0.0036\n","  Distillation iter 10/500, Loss: 0.0511, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0309, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0258, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0186, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0665, CE: 0.0019\n","  Distillation iter 10/500, Loss: 0.0495, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0368, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0285, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0238, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0190, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0693, CE: 0.0052\n","  Distillation iter 10/500, Loss: 0.0529, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0420, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0339, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0284, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0250, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0230, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0217, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0187, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0682, CE: 0.0038\n","  Distillation iter 10/500, Loss: 0.0516, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0262, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0188, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0695, CE: 0.0051\n","  Distillation iter 10/500, Loss: 0.0531, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0421, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0338, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0283, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0249, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0208, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0187, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0707, CE: 0.0064\n","  Distillation iter 10/500, Loss: 0.0533, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0425, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0343, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0288, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0252, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0231, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0185, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0728, CE: 0.0098\n","  Distillation iter 10/500, Loss: 0.0539, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0446, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0370, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0314, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0275, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0249, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0219, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0178, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0696, CE: 0.0058\n","  Distillation iter 10/500, Loss: 0.0528, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0420, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0339, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0284, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0249, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0184, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0676, CE: 0.0042\n","  Distillation iter 10/500, Loss: 0.0510, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0309, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0256, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0176, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0679, CE: 0.0044\n","  Distillation iter 10/500, Loss: 0.0513, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0315, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0261, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0229, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0177, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0682, CE: 0.0034\n","  Distillation iter 10/500, Loss: 0.0516, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0311, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0259, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0229, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0189, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0668, CE: 0.0032\n","  Distillation iter 10/500, Loss: 0.0501, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0380, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0295, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0243, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0175, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0719, CE: 0.0081\n","  Distillation iter 10/500, Loss: 0.0542, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0445, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0368, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0313, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0276, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0251, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0235, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0188, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0732, CE: 0.0081\n","  Distillation iter 10/500, Loss: 0.0552, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0453, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0373, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0317, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0279, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0255, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0239, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0193, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0695, CE: 0.0052\n","  Distillation iter 10/500, Loss: 0.0528, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0417, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0334, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0279, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0226, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0187, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 112, 112])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0713, CE: 0.0075\n","  Distillation iter 10/500, Loss: 0.0539, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0438, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0360, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0305, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0269, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0231, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0190, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 1]: Sample 20/20 saved\n","\n","Running comparative analysis for class 1...\n","\n","\n","=== Processing target class 2 ===\n","Images for class 2 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/gs20p/class_2\n","\n","Performing inversion attack on Client Model 1 for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0209, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0206, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0030, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0199, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0203, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0027, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0200, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0030, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0206, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0030, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0206, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0200, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0210, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0027, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0209, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0026, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0205, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0031, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0204, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0027, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0027, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0207, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0031, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0205, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0033, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0203, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0026, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0030, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0204, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0030, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0030, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0204, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0206, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0033, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0203, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0028, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0203, CE: 0.0000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0033, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0029, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 2]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0243, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0250, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0247, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0248, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0244, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 2]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0215, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0078, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0220, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0060, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0214, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0061, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0214, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0214, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0070, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0212, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0214, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0062, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0212, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0063, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0222, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0214, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0051, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0212, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 2]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 2\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0222, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0058, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0220, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0224, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0218, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0221, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0047, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0221, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0218, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0220, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0221, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0059, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0221, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0223, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0054, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0038, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0217, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0216, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0220, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0220, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0037, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0219, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 2]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 2\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0568, CE: 0.0003\n","  Distillation iter 10/500, Loss: 0.0407, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0293, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0226, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0166, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0569, CE: 0.0006\n","  Distillation iter 10/500, Loss: 0.0426, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0316, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0170, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0166, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0550, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0374, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0262, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0202, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0162, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0161, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0160, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0160, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0556, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0379, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0266, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0163, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0767, CE: 0.0198\n","  Distillation iter 10/500, Loss: 0.0527, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0488, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0451, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0417, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0362, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0341, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0323, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0308, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0295, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0283, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0274, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0265, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0258, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0251, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0241, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0236, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0232, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0228, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0225, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0217, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0202, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0201, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0199, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0197, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0197, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0558, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0380, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0267, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0164, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0559, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0381, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0268, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0163, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0163, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0559, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0381, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0268, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0164, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0164, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0563, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0384, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0270, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0166, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0561, CE: 0.0001\n","  Distillation iter 10/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0277, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0165, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0165, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0667, CE: 0.0103\n","  Distillation iter 10/500, Loss: 0.0511, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0460, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0412, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0372, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0339, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0313, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0293, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0276, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0263, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0252, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0243, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0235, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0229, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0223, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0202, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0198, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0182, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0565, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0386, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0167, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.1437, CE: 0.0863\n","  Distillation iter 10/500, Loss: 0.0542, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0523, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0504, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0486, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0468, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0452, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0436, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0422, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0409, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0377, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0367, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0359, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0351, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0344, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0337, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0331, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0325, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0320, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0315, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0311, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0306, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0303, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0299, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0296, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0293, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0290, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0287, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0285, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0283, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0281, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0279, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0277, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0276, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0274, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0273, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0271, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0270, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0269, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0268, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0267, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0266, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0266, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0265, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0265, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0264, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0264, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0263, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0566, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0167, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0565, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0386, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0167, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0567, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0390, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0275, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0167, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.6293, CE: 0.5714\n","  Distillation iter 10/500, Loss: 0.0554, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0545, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0537, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0530, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0523, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0517, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0510, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0503, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0497, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0491, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0485, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0479, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0473, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0468, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0463, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0458, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0454, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0449, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0445, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0441, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0437, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0433, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0430, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0427, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0424, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0421, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0418, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0415, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0413, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0411, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0409, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0407, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0405, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0403, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0402, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0400, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0399, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0395, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0394, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0394, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0392, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0391, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0390, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0389, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0389, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0388, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0565, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0386, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0272, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0166, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0567, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0387, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0273, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0169, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0168, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0561, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0383, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0270, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0168, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0167, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0166, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0166, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 2]: Sample 20/20 saved\n","\n","Running comparative analysis for class 2...\n","\n","\n","=== Processing target class 3 ===\n","Images for class 3 will be saved in: /home/jupyter/notebooks/federated/model_inversion_fl/no_ref_multi_inversion/inverted_images/gs20p/class_3\n","\n","Performing inversion attack on Client Model 1 for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0230, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0230, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0233, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0235, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0231, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0229, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0036, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0237, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0236, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0035, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0232, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0034, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 1 [Class 3]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 2 for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0048, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0052, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0051, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0249, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0051, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0258, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0057, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0056, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0053, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0257, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0050, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0055, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 2 [Class 3]: Sample 20/20 saved\n","\n","Performing inversion attack on Client Model 3 for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0252, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0049, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0247, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0254, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0259, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0253, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0064, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0251, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0255, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0247, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0257, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0256, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0042, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","Post-processing completed successfully\n","Client Model 3 [Class 3]: Sample 20/20 saved\n","\n","Performing inversion attack on Global Model for class 3\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 1/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.00006\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 2/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 3/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0243, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 4/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0246, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 5/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0245, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 6/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0243, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 7/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0238, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0044, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 8/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 9/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0244, CE: 0.00004\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0039, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 10/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 11/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.00005\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 12/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0242, CE: 0.00008\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 13/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.00002\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 14/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0243, CE: 0.00009\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0046, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 15/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0045, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 16/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.00007\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 17/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0241, CE: 0.00000\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0041, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 18/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0234, CE: 0.00001\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 19/20 savedUsing scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","Scale 112x112 Iter 999/1000 - Loss: 0.0240, CE: 0.00003\n","\n","Optimizing at scale 224x224\n","Scale 224x224 Iter 1999/2000 - Loss: 0.0043, CE: 0.0000\n","\n","Optimizing at scale 299x299\n","Scale 299x299 Iter 1999/2000 - Loss: 0.0040, CE: 0.0000\n","Post-processing completed successfully\n","Global Model [Class 3]: Sample 20/20 saved\n","\n","Performing advanced ensemble attack for class 3\n","\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0568, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0389, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0276, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0216, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0173, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 1/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0395, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0281, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0219, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0176, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 2/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0573, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0393, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0279, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0175, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 3/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0574, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0394, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0279, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0174, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0174, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 4/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0582, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0400, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0284, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0177, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 5/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0567, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0388, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0275, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0173, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0172, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 6/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0601, CE: 0.0015\n","  Distillation iter 10/500, Loss: 0.0473, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0370, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0296, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0250, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0200, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0189, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0188, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0184, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0181, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 7/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0745, CE: 0.0150\n","  Distillation iter 10/500, Loss: 0.0544, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0496, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0451, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0412, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0379, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0352, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0330, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0312, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0297, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0285, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0275, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0266, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0258, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0252, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0246, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0241, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0237, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0233, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0230, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0227, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0217, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0215, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0212, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0211, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0210, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0209, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0208, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0208, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0207, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0206, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0205, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0204, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0203, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0203, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 8/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0586, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0403, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0287, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0224, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0196, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0180, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 9/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0580, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0399, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0283, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0177, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 10/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0584, CE: 0.0006\n","  Distillation iter 10/500, Loss: 0.0434, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0321, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0251, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0213, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0187, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0177, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 11/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0576, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0397, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0282, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0176, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 12/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0582, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0400, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0285, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0223, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0195, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0185, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0180, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 13/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0582, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0401, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0285, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0222, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0194, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0183, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0178, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 14/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0577, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0281, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0176, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 15/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0579, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0398, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0283, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0221, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0193, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0182, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0179, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0177, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 16/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0573, CE: 0.0001\n","  Distillation iter 10/500, Loss: 0.0396, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0281, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0219, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0191, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0175, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 17/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0568, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0388, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0275, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0214, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0186, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0172, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0171, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0171, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 18/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0575, CE: 0.0001\n","  Distillation iter 10/500, Loss: 0.0397, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0283, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0220, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0192, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0181, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0178, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0176, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 19/20 saved\n","Phase 1: Individual model inversions\n","  Inverting model 1/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 2/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 3/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","  Inverting model 4/4\n","Using scales: [(1, 1, 112, 112), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","\n","Phase 2: Knowledge distillation from individual reconstructions\n","  Standardizing image dimensions...\n","  Image 1 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 1 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 2 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 2 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 3 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 3 shape after resize: torch.Size([1, 1, 224, 224])\n","  Image 4 shape before resize: torch.Size([1, 1, 299, 299])\n","  Image 4 shape after resize: torch.Size([1, 1, 224, 224])\n","  Ensemble seed shape: torch.Size([1, 1, 224, 224]), device: cuda:0\n","  Model devices: [device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n","  Distillation iter 0/500, Loss: 0.0574, CE: 0.0000\n","  Distillation iter 10/500, Loss: 0.0394, CE: 0.0000\n","  Distillation iter 20/500, Loss: 0.0279, CE: 0.0000\n","  Distillation iter 30/500, Loss: 0.0218, CE: 0.0000\n","  Distillation iter 40/500, Loss: 0.0190, CE: 0.0000\n","  Distillation iter 50/500, Loss: 0.0180, CE: 0.0000\n","  Distillation iter 60/500, Loss: 0.0177, CE: 0.0000\n","  Distillation iter 70/500, Loss: 0.0176, CE: 0.0000\n","  Distillation iter 80/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 90/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 100/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 110/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 120/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 130/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 140/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 150/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 160/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 170/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 180/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 190/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 200/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 210/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 220/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 230/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 240/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 250/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 260/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 270/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 280/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 290/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 300/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 310/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 320/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 330/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 340/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 350/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 360/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 370/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 380/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 390/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 400/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 410/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 420/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 430/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 440/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 450/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 460/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 470/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 480/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 490/500, Loss: 0.0175, CE: 0.0000\n","  Distillation iter 499/500, Loss: 0.0175, CE: 0.0000\n","\n","Phase 3: Final ensemble optimization\n","Using scales: [(1, 1, 112, 112), (1, 1, 224, 224), (1, 1, 299, 299)] with 1 input channels\n","\n","Optimizing at scale 112x112\n","\n","\n","Optimizing at scale 224x224\n","\n","\n","Optimizing at scale 299x299\n","\n","Post-processing completed successfully\n","Ensemble Model [Class 3]: Sample 20/20 saved\n","\n","Running comparative analysis for class 3...\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"id":"51bbb4fe-c74e-4de6-b695-34d6cfa5c43b","metadata":{"id":"51bbb4fe-c74e-4de6-b695-34d6cfa5c43b","outputId":"f240a3cd-cd9f-44cf-ea1d-59553a9f4f9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["print('done')"]},{"cell_type":"code","execution_count":null,"id":"57c41157-7ac3-4585-9229-1e4fead07d9a","metadata":{"id":"57c41157-7ac3-4585-9229-1e4fead07d9a"},"outputs":[],"source":[]}],"metadata":{"environment":{"kernel":"pytorch_fl","name":"tf2-gpu.2-16.m127","type":"gcloud","uri":"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-16:m127"},"kernelspec":{"display_name":"Python (pytorch_fl)","language":"python","name":"pytorch_fl"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}